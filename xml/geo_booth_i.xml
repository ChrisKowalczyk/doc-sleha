<!DOCTYPE sect1 PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

 <sect1 id="sec.ha.geo.booth">
  <title>Setting Up the Booth Services</title>
  <para>The default booth configuration is &booth.conf;. This file must be 
    the same on all sites of your &geo; cluster, including the
    arbitrator or arbitrators. To keep the booth configuration synchronous 
    across all sites and arbitrators, use &csync;, as described in <xref
     linkend="sec.ha.geo.booth.sync"/>. </para>
  
   
  <para>&booth-multi-tenancy; For details on how to configure booth for
   multiple &geo; clusters, refer to <xref linkend="sec.ha.geo.booth.multi"
   />. </para>
  
  <sect2 id="sec.ha.geo.booth.default">
   <title>Default Booth Setup</title>
   <para>To configure all parameters needed for booth, either edit the booth
    configuration files manually or by using the &yast; <guimenu>Geo
     Cluster</guimenu> module. To access the &yast; module, start it from
    command line with <command>yast2
     geo-cluster</command> (or start
    &yast; and select <menuchoice>
     <guimenu>High Availability</guimenu>
     <guimenu>Geo Cluster</guimenu>
    </menuchoice>). </para>
   <example id="ex.ha.booth.conf.default">
    <title>A Booth Configuration File</title>
      <!--taroth 2014-08-21: not sure if it makes sense that all tickets
      configured here should have similar options and values or if we should
      rather show different options and values for individual tickets - dejan,
      please check! - dejan (bnc#896673): It makes sense for the network parameters 
      to be shared between tickets as the parties communicating are the same 
      (parameters 6-9)-->
   <para>
    <remark>taroth 2014-12-11: todo - check IP addresses for sites and arbitrator used here and make
     consistent with DRBD section!</remark>
   </para>
<screen>transport = UDP <co id="co.ha.geo.booth.config.transport"/>
port = 9929 <co id="co.ha.geo.booth.config.port"/>
arbitrator = 147.2.207.14 <co id="co.ha.geo.booth.config.arbitrator"/>
site =  192.168.201.151 <co id="co.ha.geo.booth.config.site"/>
site =  192.168.202.151 <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket = "ticket-nfs" <co id="co.ha.geo.booth.config.ticket"/>
     expire = 600 <co id="co.ha.geo.booth.config.expiry"/>
     timeout = 10 <co id="co.ha.geo.booth.config.timeout"/>
     retries = 5 <co id="co.ha.geo.booth.config.retries"/>
     renewal-freq = 30 <co id="co.ha.geo.booth.config.renewal"/>
     before-acquire-handler<co id="co.ha.geo.booth.config.handler"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<co 
     id="co.ha.geo.booth.config.script"/>&nbsp;ms_drbd_nfs<co  id="co.ha.geo.booth.config.rsc"/>
     acquire-after = 60 <co id="co.ha.geo.booth.config.acquire-after"/>
ticket = "&ticket1;" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
 expire = 600 <xref linkend="co.ha.geo.booth.config.expiry" xrefstyle="select:label nopage"/>
 timeout = 10 <xref linkend="co.ha.geo.booth.config.timeout" xrefstyle="select:label nopage"/>
 retries = 5 <xref linkend="co.ha.geo.booth.config.retries" xrefstyle="select:label nopage"/>
 renewal-freq = 30 <xref linkend="co.ha.geo.booth.config.renewal" xrefstyle="select:label nopage"/>
 before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
  xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
   linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;db-1 <xref
    linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/>
 acquire-after = 60 <xref linkend="co.ha.geo.booth.config.acquire-after" xrefstyle="select:label nopage"/>
ticket = "&ticket2;" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
     expire = 600 <xref linkend="co.ha.geo.booth.config.expiry" xrefstyle="select:label nopage"/>
     timeout = 10 <xref linkend="co.ha.geo.booth.config.timeout" xrefstyle="select:label nopage"/>
     retries = 5 <xref linkend="co.ha.geo.booth.config.retries" xrefstyle="select:label nopage"/>
     renewal-freq = 30 <xref linkend="co.ha.geo.booth.config.renewal" xrefstyle="select:label nopage"/>
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
     xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
     linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;db-8 <xref
     linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/>
     acquire-after = 60 <xref linkend="co.ha.geo.booth.config.acquire-after" xrefstyle="select:label nopage"/>
    </screen>
    <calloutlist>
     <callout arearefs="co.ha.geo.booth.config.transport">
      <para>&booth-transport; Currently, this parameter can therefore be omitted.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.port">
      <para> &booth-port; When not using the default port (<literal>9929</literal>), choose a port that is not 
       already used for different services. Make sure to open the port in the
       nodes&apos; and arbitrators&apos; firewalls. The booth clients use TCP to communicate with the 
       &boothd;. Booth will always bind and listen to both UDP and TCP ports.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.arbitrator">
      <para>&booth-arbitrator; Add an entry for each 
       arbitrator you use in your &geo; cluster setup.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.site">
      <para> &booth-site; Add an entry for each site you use in your
       &geo; cluster setup. Make sure to insert the correct virtual IP
       addresses (<systemitem>IPaddr2</systemitem>) for each site, otherwise the
       booth mechanism will not work correctly. Booth works with both IPv4 and
       IPv6 addresses.
       <!--taroth 2014-08-21: https://fate.suse.com/316122: booth should support IPv6 in full 
        (prio: important--></para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.ticket">
     <para>&booth-ticket; For each ticket, add a <literal>ticket</literal> entry. In this
      example, the ticket <literal>ticket-nfs</literal> will be used for failover of NFS and DRBD in the
      following. See <xref linkend="sec.ha.geo.drbd"/> for details.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.expiry">
      <para> Optional parameter. Defines the ticket&apos;s expiry time in seconds. A site that
       has been granted a ticket will renew the ticket regularly. If booth does not receive any
       information about renewal of the ticket within the defined expiry time, the ticket will be
       revoked and granted to another site. If no expiry time is specified, the ticket will expire
       after <literal>600</literal> seconds by default. The parameter should not be set to a value
       less than 120 seconds.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.timeout">
      <para> Optional parameter. Defines a timeout period in seconds. After that time, booth will
       resend packets if it did not receive a reply within this period. The timeout defined should
       be long enough to allow packets to reach other booth members (all arbitrators and
       sites).</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.retries">
      <para> Optional parameter. Defines how many times booth retries sending packets before giving
       up waiting for confirmation by other sites. Values smaller than <literal>3</literal> are
       invalid and will prevent booth from starting.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.renewal">
      <para> Optional parameter. Sets the ticket renewal frequency period. Ticket renewal occurs
       every half expiry time by default. If the network reliability is often reduced over prolonged
       periods, it is advisable to renew more often. Before every renewal the
        <literal>before-acquire-handler</literal> is run. </para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.handler">
      <para> Optional parameter. If set, the specified command will be called before &boothd;
       tries to acquire or renew a ticket. On exit code other than <literal>0</literal>,
       &boothd; relinquishes the ticket.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.script">
      <para>The <filename>service-runnable</filename> script referenced here is included in the
       product as an example. It is a simple script based on <command>crm_simulate</command>. It can
       be used to test if a particular cluster resource <emphasis>can</emphasis> be run on the
       current cluster site. That means, it checks if the cluster is healthy enough to run the
       resource (all resource dependencies are fulfilled, the cluster partition has quorum, no dirty nodes,
       etc.). For example, if a service in the dependency-chain has a failcount of
        <literal>INFINITY</literal> on all available nodes, the service cannot be run on that site.
       In that case, it is of no use to claim the ticket.</para>
      </callout>
    </calloutlist>
    <calloutlist>
     <callout arearefs="co.ha.geo.booth.config.rsc">
      <para>The resource to be tested by the <literal>before-acquire-handler</literal> (in this
       case, by the <filename>service-runnable</filename> script). You need to reference the
       resource which is protected by the respective ticket. In this example, resource
        <literal>db-1</literal> is protected by <literal>&ticket1;</literal> whereas
        <literal>db-8</literal> is protected by <literal>&ticket2;</literal>. The resource for DRBD
       (<literal>ms_drbd_nfs</literal>) is protected by the ticket <literal>ticket-nfs</literal>.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.acquire-after">
      <para>Optional parameter. After a ticket is lost, booth will wait this time in addition before
       acquiring the ticket. This is to allow for the site that lost the ticket to relinquish the
       resources, by either stopping them or fencing a node. A typical delay might be
        <literal>60</literal> seconds, but ultimately it depends on the protected resources and the
       fencing configuration. The default value is <literal>0</literal>.</para>
      <para>If you are unsure how long stopping or demoting the resources or fencing a node may take
       (depending on the <literal>loss-policy</literal>), use this parameter to prevent resources
       from running on two sites at the same time.</para>
     </callout>
    </calloutlist>
    
   </example>
   
   <procedure id="pro.ha.geo.setup.booth.config.edit">
    <title>Manually Editing The Booth Configuration File</title>
    <step>
     <para> Log in to a cluster node as &rootuser; or equivalent. </para>
    </step>
    <step>
     <para> Copy the example booth configuration file
       <filename>/etc/booth/booth.conf.example</filename> to
      &booth.conf;.</para>
    </step>
    <step>
     <para>Edit &booth.conf; according to <xref
       linkend="ex.ha.booth.conf.default"/>.</para>
    </step>
    <step>
     <para>Verify your changes and save the file. </para>
    </step>
    <step>
     <para>On all cluster nodes and arbitrators, open the port in the firewall
      that you have configured for booth. See <xref
       linkend="ex.ha.booth.conf.default"/>, position <xref
       linkend="co.ha.geo.booth.config.port"/>. </para>
    </step>
   </procedure>


   <procedure id="pro.ha.geo.setup.booth.yast">
    <title>Setting Up Booth with &yast;</title>
    <step>
     <para> Log in to a cluster node as &rootuser; or equivalent. </para>
    </step>
    <step>
     <para>Start the &yast; <guimenu>Geo Cluster</guimenu> module. </para>
    </step>
    <step>
     <para>Choose to <guimenu>Edit</guimenu> an existing booth configuration
      file or click <guimenu>Add</guimenu> to create a new booth configuration
      file:</para>
     <substeps>

      <step id="step.ha.booth.conf.params">
       <para>In the screen that appears configure the following parameters:</para>
       <itemizedlist>
        <listitem>
         <formalpara>
          <title>Configuration File</title>
          <para>A name for the booth configuration file. &yast; suggests
            <literal>booth</literal> by default. This results in the booth configuration being
           written to &booth.conf;. Only change this value if you need to set up multiple booth
           instances for different &geo; clusters as described in <xref
            linkend="sec.ha.geo.booth.multi"/>.</para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara>
          <title>Transport</title>
          <para>&booth-transport; See also <xref linkend="ex.ha.booth.conf.default"/>, position
            <xref linkend="co.ha.geo.booth.config.transport"/>.</para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara>
          <title>Port</title>
          <para>&booth-port; See also <xref linkend="ex.ha.booth.conf.default"/>, position <xref
            linkend="co.ha.geo.booth.config.port"/>. </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara>
          <title>Arbitrator</title>
          <para>&booth-arbitrator; See also <xref linkend="ex.ha.booth.conf.default"/>, position
            <xref linkend="co.ha.geo.booth.config.arbitrator"/>.</para>
         </formalpara>
         <para>To specify an <guimenu>Arbitrator</guimenu>, click <guimenu>Add</guimenu>. In the
          dialog that opens, enter the IP address of your arbitrator and click
          <guimenu>OK</guimenu>. </para>
        </listitem>
        <listitem>
         <formalpara>
          <title>Site</title>
          <para>&booth-site; See also <xref linkend="ex.ha.booth.conf.default"/>, position <xref
            linkend="co.ha.geo.booth.config.site"/>.</para>
         </formalpara>
         <para>To specify a <guimenu>Site</guimenu> of your &geo; cluster, click
           <guimenu>Add</guimenu>. In the dialog that opens, enter the IP address of one site and
          click <guimenu>OK</guimenu>.</para>
        </listitem>
        <listitem>
         <formalpara>
          <title>Ticket</title>
          <para>&booth-ticket; See also <xref linkend="ex.ha.booth.conf.default"/>, position
            <xref linkend="co.ha.geo.booth.config.ticket"/>.</para>
         </formalpara>
         <para>To specify a <guimenu>Ticket</guimenu>, click <guimenu>Add</guimenu>. In the dialog
          that opens, enter a unique <guimenu>Ticket</guimenu> name. If you need to define multiple
          tickets with the same parameters and values, save configuration effort by creating
          a <quote>ticket template</quote> that specifies the defaults parameters and values for
          all tickets. To do so, use <literal>__default__</literal> as <guimenu>Ticket</guimenu>
          name.</para>
         <para>Additionally, you can specify
          optional parameters for your ticket. For an overview, see <xref
           linkend="ex.ha.booth.conf.default"/>, positions <xref
           linkend="co.ha.geo.booth.config.expiry"/> to <xref
           linkend="co.ha.geo.booth.config.acquire-after"/>.</para>
         <para>Click <guimenu>OK</guimenu> to confirm
          your changes.</para>
        </listitem>
       </itemizedlist>
       <figure id="fig.yast2.ha.geo.booth">
        <title>Example Ticket Dependency</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="yast2_geo_cluster_booth.png" width="80%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="yast2_geo_cluster_booth.png" width="50%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
      <step>
       <para>Click <guimenu>OK</guimenu> to close the current booth
        configuration screen. &yast; shows the name of the booth
        configuration file that you have defined. </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>Before closing the &yast; module, switch to the <guimenu>Firewall
       Configuration</guimenu> category.</para>
    </step>
    <step>
     <para>
     To open the port you have configured for booth, enable <guimenu>Open
      Port in Firewall</guimenu>. </para>
     <important>
      <title>Firewall Setting for Local Machine Only</title>
      <para>The firewall setting is only applied to the current machine. It will open the UDP/TCP
       ports for all ports that have been specified in &booth.conf; or any other booth
       configuration files (see <xref linkend="sec.ha.geo.booth.multi"/>).</para>
      <para>Make sure to open the respective ports on all other cluster nodes and arbitrators of
       your &geo; cluster setup, too. Do so either manually or by synchronizing the following
       files with &csync;:</para>
     <itemizedlist>
       <listitem>
        <para>
         <filename>/etc/sysconfig/SuSEfirewall2</filename>
        </para>
       </listitem>
      <listitem>
       <para>
        <filename>/etc/sysconfig/SuSEfirewall2.d/services/booth</filename>
       </para>
      </listitem>
     </itemizedlist>
     </important>
    </step>
    <step>
     <para>Click <guimenu>Finish</guimenu> to confirm all settings and close the
      &yast; module. Depending on the <replaceable>NAME</replaceable> of the
       <guimenu>Configuration File </guimenu> specified in <xref
       linkend="step.ha.booth.conf.params"/>, the configuration is written to
        <filename>/etc/booth/<replaceable>NAME</replaceable>.conf</filename>.</para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.geo.booth.multi">
   <title>Booth Setup for Multiple Tenants</title>
   <!--taroth 2014-08-07:  https://fate.suse.com/316123:
    Multi-tenancy for booth (prio: important)-->
   <para>&booth-multi-tenancy;</para>
   <para>
    Let us assume you have two
    &geo; clusters, one in EMEA (Europe, the Middle East and Africa), and
    one in the Asia-Pacific region (APAC). </para>
   <para>To use the same arbitrator for both &geo; clusters, create two
    configuration files in the <filename>/etc/booth</filename> directory:
     <filename>/etc/booth/emea.conf</filename> and
     <filename>/etc/booth/apac.conf</filename>. Both must minimally differ in
    the following parameters:</para>
   <itemizedlist>
    <listitem>
     <para>The port used for the communication of the booth instances.</para>
    </listitem>
    <listitem>
     <para>The sites belonging to the different &geo; clusters that the
      arbitrator is used for.</para>
    </listitem>
   </itemizedlist>

   <example id="ex.ha.conf.booth.multi-1">
    <title>
     <filename>/etc/booth/apac.conf</filename>
    </title>
    <para><remark>taroth 2014-08-26: DEVs, can someone provide better IP
     addresses for the cluster sites below? I just noticed that the entities we have
    defined for our usual network examples in the docs are too similar for different cluster
    sites...</remark></para>
    <screen><?dbsuse-fo font-size="0.75em"?>port = 9133 <xref linkend="co.ha.geo.booth.config.port" xrefstyle="select:label nopage"/>
arbitrator = 147.2.207.14 <xref linkend="co.ha.geo.booth.config.arbitrator" xrefstyle="select:label nopage"/>
site= &slpip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
site= &proxyip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket="tkt-db-apac-intern" <xref linkend="co.ha.geo.booth.config.ticket"/>
     timeout = 10 
     retries = 5 
     renewal-freq = 60 
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
      xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
       linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;db-apac-intern <xref
        linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/> 
ticket="tkt-db-apac-cust" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
     timeout = 10 
     retries = 5 
     renewal-freq = 60 
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
      xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
       linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;db-apac-cust <xref
        linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/></screen>
   </example>

   <example id="ex.ha.conf.booth.multi-2">
    <title>
     <filename>/etc/booth/emea.conf</filename>
    </title>
    <para><remark>taroth 2014-08-26: DEVs, can someone provide better IP
     addresses for the cluster sites below? I just noticed that the entities we have
     defined for our usual network examples in the docs are too similar for different cluster
     sites...</remark></para>
    <screen><?dbsuse-fo font-size="0.75em"?>port = 9150 <xref linkend="co.ha.geo.booth.config.port" xrefstyle="select:label nopage"/>
arbitrator = 147.2.207.14 <xref linkend="co.ha.geo.booth.config.arbitrator" xrefstyle="select:label nopage"/>
site= 192.168.4.113 <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
site=  192.168.6.113<xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket="tkt-sap-crm" <xref linkend="co.ha.geo.booth.config.ticket"/>
     expire = 900 
     renewal-freq = 60 
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
      xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
       linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;sap-crm <xref
        linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/>
ticket="tkt-sap-prod" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
     expire = 600 
     renewal-freq = 60 
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
      xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
       linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;sap-prod <xref
        linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/></screen>
   </example>

   <calloutlist>
    <callout arearefs="co.ha.geo.booth.config.port">
     <para>&booth-port; The configuration files use different ports to allow
      for start of multiple booth instances on the same arbitrator.</para>
    </callout>
    <callout arearefs="co.ha.geo.booth.config.arbitrator">
     <para>&booth-arbitrator; In the examples above, we use the same
      arbitrator for different &geo; clusters.</para>
    </callout>
    <callout arearefs="co.ha.geo.booth.config.site">
     <para>&booth-site; The sites defined in both booth configuration files
      are different, because they belong to two different &geo; clusters.
     </para>
    </callout>
    <callout arearefs="co.ha.geo.booth.config.ticket">
     <para>&booth-ticket; Theoretically the same ticket names can be defined
      in different booth configuration files&mdash;the tickets will not
      interfere because they are part of different &geo; clusters that are
      managed by different booth instances. However, (for better overview), we
      advise to use distinct ticket names for each &geo; cluster as shown in
      the examples above.</para>
    </callout>
   </calloutlist>

   <procedure>
    <title>Using the Same Arbitrator for Different &geo; Clusters</title>
    <step>
     <para>Create different booth configuration files in
       <filename>/etc/booth</filename> as shown in <xref
       linkend="ex.ha.conf.booth.multi-1"/> and <xref
       linkend="ex.ha.conf.booth.multi-2"/>. Do so either manually or with
      &yast;, as outlined in <xref linkend="pro.ha.geo.setup.booth.yast"/>.
     </para>
    </step>
    <step>
     <para>On the arbitrator, open the ports that are defined in any of the
      booth configuration files in <filename>/etc/booth</filename>.</para>
    </step>
    <step>
     <para>On the nodes belonging to the individual &geo; clusters that the
      arbitrator is used for, open the port that is used for the respective
      booth instance.</para>
    </step>
    <step>
     <para>Synchronize the respective booth configuration files across all
      cluster nodes and arbitrators that use the same booth configuration. For
      details, see <xref linkend="sec.ha.geo.booth.sync"/>.</para>
    </step>
    <step>
     <para>On the arbitrator, start the individual booth instances as described
      in <xref linkend="vle.ha.geo.setup.booth.service.arbitrator"/> for
      multi-tenancy setups.</para>
    </step>
    <step>
     <para>On the individual &geo; clusters, start the booth service as
      described in <xref linkend="vle.ha.geo.setup.booth.service.sites"
      />.</para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.geo.booth.sync">
   <title>Synchronizing the Booth Configuration Across All Sites and
    Arbitrators</title>
   <para>To make booth work correctly, all cluster nodes and arbitrators within
    one &geo; cluster must use the same booth configuration. In case of any
    booth configuration changes, make sure to update the configuration files accordingly on
    all parties and to restart the booth services as described in <xref
    linkend="sec.ha.geo.setup.booth.reconfig"/>. </para>
   
   <note>
    <title>Synchronize Booth Configuration to All Sites and Arbitrators</title>
  <para>  
     All cluster nodes and arbitrators within the &geo; cluster must use the same booth
     configuration. You can use &csync;  to synchronize the booth configuration across
     the arbitrator and the cluster nodes on all sites. For details, see 
   <xref linkend="sec.ha.geo.booth.sync.csync2.setup"/> and 
   <xref linkend="sec.ha.geo.booth.sync.csync2.start"/>.</para>
   </note>
   
   <!--taroth 2014-08-11: https://fate.suse.com/316223: [docu] sync and change config files (prio: mandatory)-->
   <sect3 id="sec.ha.geo.booth.sync.csync2.setup">
    <title>&csync; Setup for &geo; Clusters</title>
    
    <para><!--taroth 2014-09-25: the following is copied from ha_installation.xml, 
     varlistentry vle.csync2-->
     &def-csync2; &csync; can handle any number of hosts, sorted into
     synchronization groups. Each synchronization group has its own list of
     member hosts and its include/exclude patterns that define which ﬁles
     should be synchronized in the synchronization group. The groups, the
     host names belonging to each group, and the include/exclude rules for
     each group are specified in the &csync; configuration file,
     <filename>/etc/csync2/csync2.cfg</filename>.
    </para>
    <para>
     For authentication, &csync; uses the IP addresses and pre-shared keys
     within a synchronization group. You need to generate one key file for
     each synchronization group and copy it to all group members.
    </para>
    <para>
     For detailed information about &csync;, refer to
     <ulink
      url="http://oss.linbit.com/csync2/paper.pdf"/>
    </para>
   
    <para>&csync; will contact other servers via a TCP port (per default <literal>6556</literal>), 
     and uses xinetd to start remote &csync; instances. </para>
    
    <para>How to set up &csync; for individual clusters with &yast; is explained in the
      <citetitle>&haguide;</citetitle> for &productname;, chapter <citetitle>Installation
      and Basic Setup</citetitle>, section <citetitle>Transferring the Configuration to All
      Nodes</citetitle>. However, &yast; cannot handle more complex &csync; setups, like those that
     are needed for &geo; clusters. For the following setup, configure &csync; manually by editing
     the configuration files.</para>
     
    <example>
     <title>&csync; Setup for &geo; Clusters</title>


     <para>To adjust &csync; for synchronizing files not only within local clusters but also
      across &geo.dispersed; sites, you need define two synchronization groups in the
      &csync; configuration:</para>
     <itemizedlist>
      <listitem>
       <para>A global group <literal>ha_global</literal> (for the files that need to be synchronized
        globally, across all sites and arbitrators belonging to a &geo; cluster).</para>
      </listitem>
      <listitem>
       <para>A group for the local cluster site <literal>ha_local</literal> (for the files that need
        to be synchronized within the local cluster).</para>
      </listitem>
     </itemizedlist>

     <para>For an overview of the multiple &csync; configuration files for the two
      synchronization groups, see <xref linkend="fig.ha.geo.csync.config"/>.</para>
   </example>
    
   <figure id="fig.ha.geo.csync.config">
    <title>Example &csync; Setup for &geo; Clusters</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="multi-site-csync2.svg" width="100%"
       format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="multi-site-csync2.png" width="100%"
       format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>Authentication key files and their references are displayed in red. The names of
    &csync; configuration files are displayed in blue, their references are displayed in green. For
    details, refer to  <xref linkend="vl.fig.ha.geo.csync.config.files"/>.</para>
    
    <variablelist id="vl.fig.ha.geo.csync.config.files" >
     <title>Example &csync; Setup: Configuration Files</title>
     <varlistentry id="vle.ha.geo.csync.csync2.cfg">
      <term><filename>/etc/csync2/csync2.cfg</filename></term>
      <listitem>
       <para>The main &csync; configuration file. It is kept short and simple on purpose and
        only contains the following:</para>
       <itemizedlist>
        <listitem>
         <para>The definition of the synchronization group <literal>ha_local</literal>. The group
          consists of two nodes (<literal>this-site-host-1</literal> and
           <literal>this-site-host-2</literal>) and uses
           <filename>/etc/csync2/ha_local.key</filename> for authentication. A list of files to be
          synchronized for this group only is defined in another &csync; configuration file,
           <filename>/etc/csync2/ha_local.cfg</filename>. It is included with the
           <literal>config</literal> statement.</para>
        </listitem>
        <listitem>
         <para>A reference to another &csync; configuration file,
           <filename>/etc/csync2.cfg/ha_global.cfg</filename>, included with the
           <literal>config</literal> statement.</para>
        </listitem>
       </itemizedlist>
      </listitem>
     </varlistentry>
     <varlistentry id="vle.ha.geo.csync.ha_local.cfg">
      <term><filename>/etc/csync2/ha_local.cfg</filename></term>
      <listitem>
       <para>This file concerns only the local cluster. It specifies a list of files to be
        synchronized only within the <literal>ha_local</literal> synchronization group, as this
        files are specific per cluster. The most important ones are the following:</para>
       <itemizedlist>
        <listitem>
         <para><filename>/etc/csync2/csync2.cfg</filename>, as this file contains the list of the
          local cluster nodes.</para>
        </listitem>
        <listitem>
         <para>
          <filename>/etc/csync2/ha_local.key</filename>, the authentication key to be used for
          &csync; synchronization within the local cluster.</para>
        </listitem>
        <listitem>
         <para>&corosync.conf;, as this file defines the communication
          channels between the local cluster nodes. </para>
        </listitem>
        <listitem>
         <para><filename>/etc/corosync/authkey</filename>, the &corosync; authentication
          key.</para>
        </listitem>
       </itemizedlist>
       <para>The rest of the file list depends on your specific cluster setup. The files listed in
         <xref linkend="fig.ha.geo.csync.config"/> are only examples. If you also want to
        synchronize files for any site-specific applications, include them in
         <filename>ha_local.cfg</filename>, too. Even though <filename>ha_local.cfg</filename> is
        targeted at the nodes belonging to one site of your &geo; cluster, the content may be
        identical on all sites. If you need different sets of hosts or different keys, adding extra
        groups may be necessary. </para>
      </listitem>
     </varlistentry>
     <varlistentry id="vle.ha.geo.csync.ha_global.cfg">
      <term> <filename>/etc/csync2.cfg/ha_global.cfg</filename></term>
      <listitem>
       <para>This files defines the &csync; synchronization group <literal>ha_global</literal>.
        The group spans <emphasis>all</emphasis> cluster nodes across multiple sites, including the
        arbitrator. As it is recommended to use a separate key for each &csync; synchronization group, this group
        uses <filename>/etc/csync2/ha_global.key</filename> for authentication. The
         <literal>include</literal> statements define the list of files to be synchronized within
        the <literal>ha_global</literal> synchronization group. The most important ones are the
        following:</para>
       <itemizedlist>
        <listitem>
         <para><filename>/etc/csync2/ha_global.cfg</filename> and
           <filename>/etc/csync2/ha_global.key</filename> (the configuration file for the
           <literal>ha_global</literal>synchronization group and the authentication key used for
          synchronization within the group)</para>
        </listitem>
        <listitem>
         <para>
          <filename>/etc/booth/booth.conf</filename>, the default booth configuration file. In case
          you are using a booth setup for multiple tenants, replace this file with the different
          booth configuration files that you have created. See <xref
           linkend="sec.ha.geo.booth.multi"/>for details.</para>
        </listitem>
        <listitem>
         <para><filename>/etc/drbd.conf</filename> and <filename>/etc/drbd.d</filename> (if you are
          using DRBD within your cluster setup). The DRBD configuration can be globally
          synchronized, as it derives the configuration by the host names contained in the resource
          configuration file.</para>
        </listitem>
        <listitem>
         <para><filename>/etc/zypp/repos.de</filename>. The package repositories are likely to be
          the same on all cluster nodes.</para>
        </listitem>
       </itemizedlist>
       <para>The other files shown (<filename>/etc/root/<replaceable>*</replaceable></filename>) are
        examples that may be included for convenience reasons (to make a cluster administrator's
        life easier).</para>
      </listitem>
     </varlistentry>
    </variablelist>
    <note>
     <para>The files <filename>csync2.cfg</filename> and <filename>ha_local.key</filename> are
      site-specific, which means you need to create different ones for each cluster site. The files are
      identical on the nodes belonging to the same cluster but different on another cluster. Each
       <filename>csync2.cfg</filename> file needs to contain a lists of hosts (cluster nodes)
       belonging to the site, plus a site-specific authentication key.</para>
     <para>The arbitrator needs a <filename>csync2.cfg</filename> file, too. It only needs to
      reference <filename>ha_global.cfg</filename> though.</para>
    </note>
   </sect3>
   <sect3 id="sec.ha.geo.booth.sync.csync2.start">
    <title>Synchronizing Changes with &csync;</title>
  
    <para>
     To successfully synchronize the files with &csync;, the following prerequisites must be met:</para>
    <itemizedlist>
     <listitem>
      <para>
       The same &csync; configuration is available on all machines that belong to the same
       synchronization group. 
      </para>
     </listitem>
     <listitem>
      <para>The &csync; authentication key for each synchronization group must be available on
       all members of that group.</para>
     </listitem>
     <listitem>
      <para> Both &csync; and <systemitem class="daemon">xinetd</systemitem> must be running on
        <emphasis>all</emphasis> nodes and the arbitrator. </para>
     </listitem>
    </itemizedlist>
    
    <para>
    Before the first &csync; run, you therefore need to make the following preparations:</para>
    
    <procedure>
     <step>
      <para>Log in to one machine per synchronization group and generate an authentication key for
       the respective group:</para>
      <screen>csync2 -k <replaceable>NAME_OF_KEYFILE</replaceable></screen>
      <para>However, do <emphasis>not</emphasis> regenerate the key file on any other member of the
       same group.</para>
      <para>With regard to <xref linkend="fig.ha.geo.csync.config"/>, this would result in the
       following key files: <filename>/etc/csync2/ha_global.key</filename> and one local key
       (<filename>/etc/csync2/ha_local.key</filename>) per
       site. </para>
     </step>
     <step>
      <para>Copy each key file to <emphasis>all</emphasis> members of the respective synchronization
       group. With regard to <xref linkend="fig.ha.geo.csync.config"/>:</para>
      <substeps>
       <step>
        <para>Copy <filename>/etc/csync2/ha_global.key</filename> to 
         <emphasis>all</emphasis> parties (the arbitrator
         and all cluster nodes on all sites of your &geo; cluster). The key file needs to be
         available on all hosts listed within the <literal>ha_global</literal> group that is defined
         in <filename>ha_global.cfg</filename>.</para>
       </step>
       <step>
        <para>Copy the local key file for each site (<filename>/etc/csync2/ha_local.key</filename>)
         to all cluster nodes belonging to the respective site of your &geo; cluster. </para>
       </step>
      </substeps>
     </step>
     <step>
      <para>Copy the site-specific <filename>/etc/csync2/csync2.cfg</filename> configuration file 
       to all cluster nodes belonging to the respective site of your &geo; cluster and to the
       arbitrator. </para>
     </step>
     <step>
      <para> Execute the following commands on all nodes and the arbitrator 
        to make both xinetd and csync2 services start automatically at boot time: </para>
      <screen>&prompt.root;<command>systemctl</command> enable csync2.socket
&prompt.root;<command>systemctl</command> enable xinetd.service</screen>
     </step>
     <step>
      <para>Execute the following commands on all nodes and the arbitrator to start both services
       now: </para>
      <screen>&prompt.root;<command>systemctl</command> start csync2.socket
&prompt.root;<command>systemctl</command> start xinetd.service</screen>
     </step>
    </procedure>
    
    <procedure id="pro.ha.geo.setup.csync2.start">
     <title>Synchronizing Files with &csync;</title>
    <step>
     <para>To initially synchronize all files once, execute the following command 
      on the machine that you want to copy the configuration
      <emphasis>from</emphasis>:
     </para>
     <screen>&prompt.root;<command>csync2</command> <option>-xv</option></screen>
     <para>
      This will synchronize all the files once by pushing them to the other members of the
      synchronization groups. If all files are synchronized successfully, &csync; will finish
      with no errors.
     </para>
     <para>
      If one or several files that are to be synchronized have been modified
      on other machines (not only on the current one), &csync; will report a
      conflict. You will get an output similar to the one below:
     </para>
     <screen>While syncing file /etc/corosync/corosync.conf:
ERROR from peer site-2-host-1: File is also marked dirty here!
Finished with 1 errors.</screen>
    </step>
    <step>
     <para>
      If you are sure that the file version on the current machine is the
      <quote>best</quote> one, you can resolve the conflict by forcing this
      file and resynchronizing:
     </para>
     <screen>&prompt.root;<command>csync2</command> <option>-f</option> /etc/corosync/corosync.conf
&prompt.root;<command>csync2</command> <option>-x</option></screen>
    </step>
    </procedure>
    <para>
     For more information on the &csync; options, run
     <command>csync2&nbsp;<option>-help</option></command>.
    </para>
    <note>
     <title>Pushing Synchronization After Any Changes</title>
     <para>
      &csync; only pushes changes. It does <emphasis>not</emphasis>
      continuously synchronize files between the machines.
     </para>
     <para>
      Each time you update files that need to be synchronized, you need to
      push the changes to the other machines of the same synchronization group: Run
      <command>csync2&nbsp;<option>-xv</option></command> on the machine where
      you did the changes. If you run the command on any of the other machines
      with unchanged files, nothing will happen.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 id="sec.ha.geo.setup.booth.service">
   <title>Enabling and Starting the Booth Services</title>
  
   <variablelist>
    <varlistentry id="vle.ha.geo.setup.booth.service.sites">
     <term>Starting the Booth Services on Cluster Sites</term>
     <listitem>
      <para>The booth service for each cluster site is managed by the booth
       resource group configured in <xref linkend="pro.ha.geo.setup.rsc.boothd"
       />.
       <!--taroth 2014-08-25: FIXME: check and add link to Hawk config, too -->
       To start one instance of the booth service per site, start the respective
       booth resource group on each cluster site.</para>
     </listitem>
    </varlistentry>
    <varlistentry id="vle.ha.geo.setup.booth.service.arbitrator">
     <term>Starting the Booth Services on Arbitrators</term>
     <!--taroth 2014-08-11: https://bugzilla.novell.com/show_bug.cgi?id=877817: [Test
      Case 1378526] [316123] booth - Configuring startup 'complicated'-->
     <listitem>
      <para>Starting with &sle; 12, booth arbitrators are managed with systemd. The unit file is
       named <filename>booth@.service</filename>. The <literal>@</literal> denotes the possibility
       to run the service with a parameter, which is in this case the name of the configuration
       file.</para>
      <para>To <emphasis>enable</emphasis> the booth service on an arbitrator, use the following
       command:</para>
      <screen>&prompt.root;<command>systemctl</command> enable booth@booth</screen>
      <para>After the service has been enabled from command line, &yast; &ycc_runlevel; can
       then be used to manage the service, as long as it is not disabled. In that case, it will
       disappear from the service list in &yast; next time systemd is restarted.</para>
      <para>However, the command to <emphasis>start</emphasis> the booth service depends on your
       booth setup: </para>
      <itemizedlist>
       <listitem>
        <para>If you are using the default setup as described in <xref
          linkend="sec.ha.geo.booth.default"/>, only <filename>/etc/booth/booth.conf</filename> is
         configured. In that case, log in to each arbitrator and use the following command: </para>
        <screen>&prompt.root;<command>systemctl</command> start booth@booth</screen>
       </listitem>
       <listitem>
        <para>If you are running booth in multi-tenancy mode as described in <xref
          linkend="sec.ha.geo.booth.multi"/>, you have configured multiple booth configuration files
         in <filename>/etc/booth</filename>. To start the services for the individual booth
         instances, use
         <command>systemctl&nbsp;start&nbsp;booth@<replaceable>NAME</replaceable></command>,
         where <replaceable>NAME</replaceable> stands for the name of the respective configuration
         file <filename>/etc/booth/<replaceable>NAME</replaceable>.conf</filename>.</para>
        <para>For example, if you have the booth configuration files
          <filename>/etc/booth/emea.conf</filename> and <filename>/etc/booth/apac.conf</filename>,
         log in to your arbitrator and execute the following commands:</para>
        <screen>&prompt.root;<command>systemctl</command> start booth@emea
&prompt.root;<command>systemctl</command> start booth@apac</screen>
        <para>
         <!-- taroth 2014-08-25: is it also possible to start the two
          services in one go when using the "@" syntax? if yes, how is the exact
          command? - dejan 2014-09-15: No, don't think so.-->
        </para>
       </listitem>
      </itemizedlist>
      <para> This starts the booth service in arbitrator mode. It can communicate with all other
       booth daemons but in contrast to the booth daemons running on the cluster sites, it cannot be
       granted a ticket. Booth arbitrators take part in elections only. Otherwise, they are
       dormant.</para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 
  <sect2 id="sec.ha.geo.setup.booth.reconfig">
   <title>Reconfiguring Booth While Running</title>
   <!--taroth 2014-08-26: https://fate.suse.com/316126: 
    reconfiguration of boothd while running (prio: important)-->
   <para>In case you need to change the booth configuration while the booth
    services are already running, proceed as follows:
  </para>
   <procedure>
    <step>
     <para>Adjust the booth configuration files as desired.</para>
    </step>
    <step>
     <para>Synchronize the updated booth configuration files to all cluster
      nodes and arbitrators that are part of your &geo; cluster. For
      details, see <xref linkend="sec.ha.geo.booth.sync"/>.</para>
    </step>
    <step>
     <!--taroth 2014-08-26: https://bugzilla.novell.com/show_bug.cgi?id=891399-->
     <para>Restart the booth services on the arbitrators and cluster sites as
      described in <xref linkend="sec.ha.geo.setup.booth.service"/>. This does
      not have any effect on tickets that have already been granted to
      sites.</para>
    </step>
   </procedure>
  </sect2>
 </sect1>
