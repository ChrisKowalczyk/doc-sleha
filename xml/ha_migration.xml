<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<appendix id="app.ha.migration">
 <title>Upgrading Your Cluster to the Latest Product Version</title>
 <para>For upgrading clusters from &productnamereg; 11 to
  &productname; 12, all cluster nodes must be offline and
  the cluster needs to be migrated as a whole. Mixed clusters running on
  &productname; 11/&productname; 12 are not supported.</para>
 <para>Which upgrade path is supported and how to perform the upgrade depends on
  the product version of your cluster and the target version you want to migrate
  the cluster to. For more information, see the <citetitle>&sls;&nbsp;12
   &deploy;</citetitle>, chapter <citetitle>Updating &sle;</citetitle>.
  It is available at &suse-onlinedoc;. </para>
 

 <sect1 id="sec.ha.migration.sle12">
  <title>Upgrading from SLE&nbsp;HA&nbsp;11 SP3 to
   SLE&nbsp;HA&nbsp;12</title>

  <para>In order to successfully upgrade to &productname; 12, your cluster
   needs to run the latest versions of &sls; and &productname;
   (11&nbsp;SP3). If your cluster is still based on an older product
   version, upgrade it to &sls; and &productname; 11&nbsp;SP3 first.
   Find information on this in the <citetitle>High Availability
    Guide</citetitle> for &productname; <emphasis>11</emphasis>, chapter
    <citetitle>Upgrading Your Cluster to the Latest Product Version</citetitle>.
   It is available at &suse-onlinedoc;. </para>

  <para>Due to major changes in various components of the &hasi; 12 (for
   example, &corosync.conf;, disk formats of OCFS2), performing a
    <literal>rolling upgrade</literal> is not supported for this scenario. All
   cluster nodes must be offline and the cluster needs to be migrated as a
   whole as described in <xref linkend="pro.ha.migration.sle12"/>.</para>

   
  <procedure id="pro.ha.migration.sle12">
   <title>Upgrading the Cluster</title>
   <important>
    <title>Required Preparations Before Upgrading</title>
    <itemizedlist>
     <listitem>
      <para>Ensure that your system back-up is up to date and restorable.</para>
     </listitem>
     <listitem>
      <para>Test the upgrade procedure on a staging instance of your cluster
       setup first, before performing it in a production environment.</para>
      <para>This gives you an estimation of the time frame required for the
       maintenance window. It also helps to detect and solve any unexpected
       problems that might arise.</para>
     </listitem>
    </itemizedlist>
   </important>
   <para>Execute the following steps for each cluster node:</para>
   <step>
    <para>Log in to each cluster node and stop the cluster stack with:</para>
    <screen>&prompt.root; # rcopenais stop</screen>
   </step>
   <step>
    <para>For each cluster node, perform an upgrade from &sls;
     11&nbsp;SP3 to &sls; 12 and from &productname; 11&nbsp;SP3
     to &productname; 12. If you want to make use of &geo; clustering,
     install the respective add-on as described in the <citetitle>&hageo;
      &geoquick;</citetitle>.
     <!--  <remark>taroth 090512:
     need to use hard-coded link here as the target is not included in the same
     set</remark>-->
     For information on how to upgrade your product, see the
      <citetitle>&sls;&nbsp;12 &deploy;</citetitle>, chapter
      <citetitle>Updating &sle;</citetitle>. It is available at
     &suse-onlinedoc;.</para>
   </step>
   <step>
    <para>
     <remark>taroth 2014-08-19: DEVs, do we need this step?</remark>If you have
     a &geo; cluster setup and use arbitrators outside of the cluster,
     upgrade them, too.</para>
   </step>
   <step>
    <para>After the upgrade process has finished, reboot each node with version
     12 of &sls; and &productname;.</para>
   </step>
   <step>
    <para>If you use OCFS2 in your cluster setup, update the on-device structure
     by executing the following command:</para>
    <screen>&prompt.root; tunefs.ocfs2 --update-cluster-stack <replaceable>PATH_TO_DEVICE</replaceable></screen>
    <para>It adds additional parameters to the disk which are needed for the
     updated OCFS2 version that is shipped with &productname; 12.</para>
   </step>
   <step>
    <para>To update &corosync.conf; for &corosync; version 2:</para>
    <substeps>
     <step>
      <para>Log in to one node and start the &yast; cluster module.</para>
     </step>
     <step>
      <para>Switch to the <guimenu>Communication Channels</guimenu> category and
       enter values for the following new parameters: <guimenu>Cluster
        Name</guimenu> and <guimenu>Expected Votes</guimenu>. For details, see
        <xref linkend="pro.ha.installation.setup.channel1"/>.</para>
     </step>
     <step>
      <para>Confirm your changes in &yast; to update &corosync.conf;.</para>
     </step>
     <step>
      <para>If &csync; is configured for your cluster, use the following
       command to push the updated &corosync; configuration to the other
       cluster nodes:</para>
      <screen>&prompt.root;<command>csync2</command> <option>-xv</option></screen>
      <para>For details on &csync;, see <xref
        linkend="sec.ha.installation.setup.csync2"/>. </para>
      <para>Alternatively, synchronize the updated &corosync; configuration
       by manually copying &corosync.conf; to all cluster nodes.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>Log in to each node and start the cluster stack with:</para>
    <screen>&prompt.root; systemctl start pacemaker.service</screen>
   </step>
   <step>
    <para> Check the cluster status with <command>crm status</command> or with
     &hawk;.</para>
   </step>
  </procedure>

  
  <note>
   <title>Reverting after Upgrade</title>
   <para>
    <remark>taroth 2014-08-19: DEVs, do we need this note?</remark>
    After the upgrade process to product version 12, reverting back to
    product version 11 is <emphasis>not</emphasis> supported. </para>
  </note>
 </sect1>


 <sect1 id="sec.ha.update">
  <title>Updating Software Packages on Cluster Nodes</title>
  <para></para>
  <para>taroth 2014-08-19: DEVs, is the following still true? it was based on
   bnc#573817#c6. Or should we now point to <xref
    linkend="sec.ha.config.basics.maint.mode"/> instead?</para>
  <important>
   <!--https://bugzilla.novell.com/show_bug.cgi?id=573817#c6-->
   <title>Updating Software Packages</title>
   <para> If you want to update any software packages on a node that is part of
    a running cluster, stop the cluster stack on that node before starting the
    software update:</para>
   <screen>&prompt.root; systemctl stop pacemaker.service</screen>
   <para> If the cluster resource manager is running during the software update,
    this can lead to unpredictable results like fencing of active nodes. </para>
  </important>
 </sect1>
 
  <!--<important>
   <title>Time Limit for Rolling Upgrade</title>
   <para>
    The new features shipped with &productname; ??? will only be
    available after <emphasis>all</emphasis> cluster nodes have been
    upgraded to the latest product version. Mixed SP1/SP2 clusters are only
    supported for a short time frame during the rolling upgrade. Complete
    the rolling upgrade within one week.
   </para>
  </important>-->
 <sect1 id="sec.ha.migration.more">
  <title>For More Information</title>

  <para>
   For detailed information about any changes and new features of the
   product you are upgrading to, refer to its release notes. They are
   available from <ulink url="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
</appendix>
