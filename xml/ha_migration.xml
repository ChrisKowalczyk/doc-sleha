<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<appendix id="app.ha.migration">
 <title>Upgrading Your Cluster to the Latest Product Version</title>
 <para>For upgrading clusters from &productnamereg; 11 to
  &productname; 12, all cluster nodes must be offline and
  the cluster needs to be migrated as a whole. Mixed clusters running on
  &productname; 11/&productname; 12 are not supported.</para>
 <para>Which upgrade path is supported and how to perform the upgrade depends on
  the product version of your cluster and the target version you want to migrate
  the cluster to. For more information, see the <citetitle>&sls;&nbsp;12
   &deploy;</citetitle>, chapter <citetitle>Updating &sle;</citetitle>.
  It is available at &suse-onlinedoc;. </para>
 

 <sect1 id="sec.ha.migration.sle12">
  <title>Upgrading from SLE&nbsp;HA&nbsp;11 SP3 to
   SLE&nbsp;HA&nbsp;12</title>

  <para>In order to successfully upgrade to &productname; 12, your cluster
   needs to run the latest versions of &sls; and &productname;
   (11&nbsp;SP3). If your cluster is still based on an older product
   version, upgrade it to &sls; and &productname; 11&nbsp;SP3 first.
   </para>

  <para>Due to major changes in various components of the &hasi; 12 (for
   example, &corosync.conf;, disk formats of OCFS2), performing a
    <literal>rolling upgrade</literal> is not supported for this scenario. All
   cluster nodes must be offline and the cluster needs to be migrated as a
   whole as described in <xref linkend="pro.ha.migration.sle12"/>.</para>

   
  <procedure id="pro.ha.migration.sle12">
   <title>Upgrading the Cluster</title>
   <important>
    <title>Required Preparations Before Upgrading</title>
    <itemizedlist>
     <listitem>
      <para>Ensure that your system back-up is up to date and restorable.</para>
     </listitem>
     <listitem>
      <para>Test the upgrade procedure on a staging instance of your cluster
       setup first, before performing it in a production environment.</para>
      <para>This gives you an estimation of the time frame required for the
       maintenance window. It also helps to detect and solve any unexpected
       problems that might arise.</para>
     </listitem>
    </itemizedlist>
   </important>
   <para>Execute the following steps for each cluster node:</para>
   <step>
    <para>Log in to each cluster node and stop the cluster stack with:</para>
    <screen>&prompt.root; systemctl stop pacemaker.service</screen>
   </step>
   <step>
    <para>For each cluster node, perform an upgrade from &sls; 11&nbsp;SP3 to &sls; 12 and
     from &productname; 11&nbsp;SP3 to &productname; 12. If you want
     to make use of &geo; clustering, install the respective add-on as
     described in the <citetitle>&hageo; &geoquick;</citetitle>.
     <!--  <remark>taroth 090512:
     need to use hard-coded link here as the target is not included in the same
     set</remark>-->
     For information on how to upgrade your product, see the
      <citetitle>&sls;&nbsp;12 &deploy;</citetitle>, chapter
      <citetitle>Updating &sle;</citetitle>. It is available at
     &suse-onlinedoc;.</para>
   </step>
   <step>
    <para>
     <remark>taroth 2014-08-19: DEVs, do we need this step?</remark>If you have a &geo; cluster setup and use arbitrators outside of
     the cluster, upgrade them, too.</para></step>
   <step>
    <para>After the upgrade process has finished, reboot each node with version
     12 of &sls; and &productname;.</para>
   </step>
   <step>
    <para>If you use OCFS2 in your cluster setup, update the on-device structure
     by executing the following command:</para>
    <screen>&prompt.root; tunefs.ocfs2 --update-cluster-stack <replaceable>PATH_TO_DEVICE</replaceable></screen>
    <para>It adds additional parameters to the disk which are needed for the
     updated OCFS2 version that is shipped with &productname; 12.</para>
   </step>
   <step>
    <para>
      Check the cluster status with <command>crm status</command> or with
     &hawk;.</para>
   </step>
   <step>
    <para><remark>taroth 2014-08-19: DEVs, as it seems to be unclear at the moment
     what is exactly needed to update &corosync.conf;, please let me know
     if restarting pacemaker is enough or if running yast2 cluster is needed
     instead (and what to change there, if necessary)</remark>Start the cluster stack on each node with:</para>
    <screen>&prompt.root; systemctl start pacemaker.service</screen>
   </step>
  <!-- <step>
    <para>Check the cluster status with <command>crm status</command> or with
     &hawk;.</para>
    <para>If the &pace; service has been enabled to start at boot time for a
     cluster node, the node should automatically go online. If nodes are still
     offline, log in to the respective nodes and start the cluster stack
     with:</para>
    <screen>&prompt.root; systemctl start pacemaker.service</screen>
   </step>
  -->  
  </procedure>

  
  <note>
   <title>Reverting after Upgrade</title>
   <para>
    <remark>taroth 2014-08-19: DEVs, do we need this note?</remark>
    After the upgrade process to product version 12, reverting back to
    product version 11 is <emphasis>not</emphasis> supported. </para>
  </note>
 </sect1>


 <sect1 id="sec.ha.update">
  <title>Updating Software Packages on Cluster Nodes</title>
  <para></para>
  <para>taroth 2014-08-19: DEVs, is the following still true? it was based on
   bnc#573817#c6. Or should we now point to <xref
    linkend="sec.ha.config.basics.maint.mode"/> instead?</para>
  <important>
   <!--https://bugzilla.novell.com/show_bug.cgi?id=573817#c6-->
   <title>Updating Software Packages</title>
   <para> If you want to update any software packages on a node that is part of
    a running cluster, stop the cluster stack on that node before starting the
    software update:</para>
   <screen>&prompt.root; systemctl stop pacemaker.service</screen>
   <para> If the cluster resource manager is running during the software update,
    this can lead to unpredictable results like fencing of active nodes. </para>
  </important>
 </sect1>
 
  <!--<important>
   <title>Time Limit for Rolling Upgrade</title>
   <para>
    The new features shipped with &productname; ??? will only be
    available after <emphasis>all</emphasis> cluster nodes have been
    upgraded to the latest product version. Mixed SP1/SP2 clusters are only
    supported for a short time frame during the rolling upgrade. Complete
    the rolling upgrade within one week.
   </para>
  </important>-->
 <sect1 id="sec.ha.migration.more">
  <title>For More Information</title>

  <para>
   For detailed information about any changes and new features of the
   product you are upgrading to, refer to its release notes. They are
   available from <ulink url="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
</appendix>
