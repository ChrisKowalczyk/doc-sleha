<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ha.troubleshooting">
 <title>Troubleshooting</title>
 <abstract>
  <para>
   Often, strange problems may occur that are not easy to understand
   (especially when starting to experiment with &hb;). However, there are
   several utilities that may be used to take a closer look at the &hb;
   internal processes. This chapter recommends various solutions.
  </para>
 </abstract>
 <sect1 id="sec.ha.troubleshooting.install">
  <title>Installation Problems</title>

  <para>
   Troubleshooting difficulties installing the packages or in bringing the
   cluster online..
  </para>

  <variablelist>
   <varlistentry>
    <term>Are the HA packages installed?</term>
    <listitem>
     <para>
      The packages needed for configuring and managing a cluster are
      included in the <literal>High Availability</literal> installation
      pattern, available with the &hasi;.
     </para>
     <para>
      Check if &hasi; is installed as an add-on to &sls; &productnumber; on
      each of the cluster nodes and if the <guimenu>High
      Availability</guimenu> pattern is installed on each of the machines as
      described in <xref linkend="sec.ha.installation.inst"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Is the initial configuration the same for all cluster nodes? </term>
    <listitem>
     <para>
      In order to communicate with each other, all nodes belonging to the
      same cluster need to use the same <literal>bindnetaddr</literal>,
      <literal>mcastaddr</literal> and <literal>mcastport</literal> as
      described in <xref linkend="sec.ha.installation.setup"/>.
     </para>
     <para>
      Check if the communication channels and options configured in
      <filename>/etc/corosync/corosync.conf</filename> are the same for all
      cluster nodes.
     </para>
     <para>
      In case you use encrypted communication, check if the
      <filename>/etc/corosync/authkey</filename> file is available on all
      cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Does the firewall allow communication via the <literal>mcastport</literal>?</term>
    <listitem>
     <para>
      If the mcastport used for communication between the cluster nodes is
      blocked by the firewall, the nodes cannot <literal>see</literal> each
      other. When configuring the initial setup with &yast; as described in
      <xref linkend="sec.ha.installation.inst"/>, the firewall settings are
      usually automatically adjusted.
     </para>
     <para>
      To make sure the mcastport is not blocked by the firewall, check the
      settings in <filename>/etc/sysconfig/SuSEfirewall2</filename> on each
      node. Alternatively, start the &yast; firewall module on each cluster
      node. After clicking <menuchoice> <guimenu>Allowed Service</guimenu>
      <guimenu>Advanced</guimenu> </menuchoice>, add the mcastport to the
      list of allowed <guimenu>UDP Ports</guimenu> and confirm your changes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Is &ais; started on each cluster node?</term>
    <listitem>
     <para>
      Check the &ais; status on each cluster node with
      <command>/etc/init.d/openais status</command>. In case &ais; is not
      running, start it by executing <command>/etc/init.d/openais
      start</command>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.troublehooting.debugging">
  <title><quote>Debugging</quote> a HA Cluster</title>

  <para>
   Troubleshooting your cluster. The following displays for you the resource
   operation history (option <option>-o</option>) and inactive resources
   (<option>-r</option>):
  </para>

<screen>crm_mon -o -r</screen>

  <para>
   The display is refreshed every 10 seconds (you can cancel it with
   <keycombo> <keycap function="control"/> <keycap>C</keycap> </keycombo>.)
   An example could look like:
  </para>

  <example>
   <title>Stopped Resources</title>
<screen>Refresh in 10s...

============
Last updated: Mon Jan 19 08:56:14 2009
Current DC: d42 (d42)
3 Nodes configured.
3 Resources configured.
============

Online: [ d230 d42 ]
OFFLINE: [ clusternode-1 ]

Full list of resources:

Clone Set: o2cb-clone
         Stopped: [  o2cb:0 o2cb:1o2cb:2 ]
Clone Set: dlm-clone
         Stopped [ dlm:0 dlm:1 dlm:2 ]
mySecondIP      (ocf::heartbeat:IPaddr):        Stopped

Operations:
* Node d230:
   aa: migration-threshold=1000000
    + (5) probe: rc=0 (ok)
    + (37) stop: rc=0 (ok)
    + (38) start: rc=0 (ok)
    + (39) monitor: interval=15000ms rc=0 (ok)
* Node d42:
   aa: migration-threshold=1000000
    + (3) probe: rc=0 (ok)
    + (12) stop: rc=0 (ok)</screen>
  </example>

  <para>
   First get your node online (see
   <xref linkend="sec.ha.troubleshooting.faq"
    xrefstyle="select:label"/>).
   After that, check your resources and operations.
  </para>

  <para>
   The <citetitle>Configuration Explained</citetitle> PDF under
   <ulink
    url="http://clusterlabs.org/wiki/Documentation"/> explains in
   section <citetitle>How Does the Cluster Interpret the OCF Return
   Codes?</citetitle> three different recovery types.
  </para>

<!-- Insert the table here? -->
 </sect1>
 <sect1 id="sec.ha.troubleshooting.faq">
  <title>FAQs</title>

  <variablelist>
   <varlistentry>
    <term>What is the state of my cluster?</term>
    <listitem>
     <para>
      To check the current state of your cluster, use the program
      <literal>crm_mon</literal>. This displays the current DC as well as
      all of the nodes and resources that are known to the current node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Several nodes of my cluster do not see each other.</term>
    <listitem>
     <para>
      There could be several reasons:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Look first in the configuration file
        <filename>/etc/corosync/corosync.conf</filename> and check if the
        multicast address is the same for every node in the cluster (look in
        the <literal>interface</literal> section with the key
        <literal>mcastaddr</literal>.)
       </para>
      </listitem>
      <listitem>
       <para>
        Check your firewall settings.
       </para>
      </listitem>
      <listitem>
       <para>
        Check if your switch supports multicast addresses
       </para>
      </listitem>
      <listitem>
       <para>
        Check if the connection between your nodes is broken. Most often,
        this is the result of a badly configured firewall. This also may be
        the reason for a <emphasis>split brain</emphasis> condition, where
        the cluster is partitioned.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I want to list my currently known resources.</term>
    <listitem>
     <para>
      Use the command <command>crm_resource -L</command> to learn about your
      current resources.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I configured a resource, but it always fails.</term>
    <listitem>
     <para>
      Try to run the resource agent manually. With LSB, just run
      <command><replaceable>scriptname</replaceable> start</command> and
      <command><replaceable>scriptname</replaceable> stop</command>. To
      check an OCF script, set the needed environment variables first. For
      example, when testing the <literal>IPaddr</literal> OCF script, you
      must set the value for the variable <literal>ip</literal> by setting
      an environment variable that prefixes the name of the variable with
      <literal>OCF_RESKEY_</literal>. For this example, run the command:
     </para>
     <remark>Is this still needed or is it dangerous?</remark>
<screen>export OCF_RESKEY_ip=&lt;your_ip_address&gt;
/usr/lib/ocf/resource.d/heartbeat/IPaddr validate-all
/usr/lib/ocf/resource.d/heartbeat/IPaddr start
/usr/lib/ocf/resource.d/heartbeat/IPaddr stop </screen>
     <para>
      If this fails, it is very likely that you missed some mandatory
      variable or just mistyped a parameter.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I just get a failed message. Is it possible to get more information?</term>
    <listitem>
     <para>
      You may always add the <literal>-V</literal> parameter to your
      commands. If you do that multiple times, the debug output becomes very
      verbose.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>How can I clean up my resources?</term>
    <listitem>
     <para>
      <remark>rwalter: something is missing between the hostname in the command and "with the
       name of..."</remark>
      If you know the IDs of your resources (which you can get with
      <command>crm_resource -L</command>), remove a specific one with
      <command>crm_resource -C -r <replaceable>resource id</replaceable> -H
      <replaceable>HOST</replaceable></command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I can not mount an ocfs2 device.</term>
    <listitem>
     <para>
      Check <filename>/var/log/message</filename> if there is the following
      line:
     </para>
<screen>Jan 12 09:58:55 clusternode2 lrmd: [3487]: info: RA output: (o2cb:1:start:stderr) 2009/01/12_09:58:55 
  ERROR: Could not load ocfs2_stackglue
Jan 12 16:04:22 clusternode2 modprobe: FATAL: Module ocfs2_stackglue not found.</screen>
     <para>
      In this case the kernel module <filename>ocfs2_stackglue.ko</filename>
      is missing. Install the package
      <filename>ocfs2-kmp-default</filename>,
      <filename>ocfs2-kmp-pae</filename> or
      <filename>ocfs2-kmp-xen</filename> depending on the installed kernel.
     </para>
    </listitem>
   </varlistentry>
<!-- 
    <varlistentry>
      <term>How can I create a report with an analysis of all my cluster nodes?</term>
      <listitem>
          <para>Use the tool <command>hb_report</command> to create a
            report. Usually run <command>hb_report</command> with the
            following command:</para>
        <screen><command>hb_report</command> </screen>
        <para>After the command was successful, you will find an tar.bz2 archive.</para>
        <warning>
          <title>Remove Sensitive Information</title>
            <para>The <command>hb_report</command> tool tries to remove
              any sensitive information from the CIB and the peinput
              files, however, it can not do everything. If you have more
              sensitive information, supply additional patterns
              yourself. The logs and the crm_mon, ccm_tool, and
              crm_verify output are *not* sanitized.</para>
            <para>Before publishing your data to mailinglists or other
              parties, first look into the archive to protect your data
              and remove any information you do not want to
              expose.</para>
        </warning>
      </listitem>
    </varlistentry>
   -->
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.troubleshooting.moreinfo">
  <title>Fore More Information</title>

  <para>
   For additional information about high availability on Linux and &hb;
   including configuring cluster resources and managing and customizing a
   &hb; cluster, see
   <ulink
    url="http://clusterlabs.org/wiki/Documentation"/>.
  </para>
 </sect1>
</chapter>
