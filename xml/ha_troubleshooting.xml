<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!--taroth 2011-10-12: toms, please also have a look at dmuhamedagic's
 troubleshooting guide and either refer to it for more info (or include what's
 useful if the guide is not publically avaialable)--> 

<!--taroth 2011-10-17: toms, please check any contents here for the
 following changes:
 * new deplyoment methods (installation as add-on or as appliance)
 * manual vs. automatic configuration 
 * apart from multicast, unicast is also supported now-->
<chapter id="cha.ha.troubleshooting">
 <title>Troubleshooting</title>
 <abstract>
  <para>
   Often, strange problems may occur that are not easy to understand
   (especially when starting to experiment with &ha;). However, there are
   several utilities that may be used to take a closer look at the &ha;
   internal processes. This chapter recommends various solutions.
  </para>
 </abstract>
 <!--taroth 2011-10-12: just found some useful information in the NTS HowTo that
  could be added in this chapter (troubleshooting communication problems): 
   example for a 2-node cluster: the /etc/hosts file on both nodes should
   contain
   149.44.174.132  fozzie.hwlab.suse.de fozzie
   149.44.174.158  boursin.hwlab.suse.de boursin
   (if 149.44.174.0/24 is used as network for cluster communication). 
   *If another network is used additionally*, more entries are needed. For example
    if fozzie and boursin are supposed to use 2 rings and one would be
    on 149.44.174.0/24 and the other one on 192.168.67.0 then the hosts
    should read
    149.44.174.132  fozzie.hwlab.suse.de fozzie
    149.44.174.158  boursin.hwlab.suse.de boursin
    192.168.67.2    fozzie.hwlab.suse.de fozzie
    192.168.67.1    boursin.hwlab.suse.de boursin-->
 <sect1 id="sec.ha.troubleshooting.install">
  <title>Installation Problems</title>

  <para>
   Troubleshooting difficulties installing the packages or in bringing the
   cluster online.
  </para>

  <variablelist>
   <varlistentry>
    <term>Are the HA packages installed?</term>
    <listitem>
     <para>
      The packages needed for configuring and managing a cluster are
      included in the <literal>High Availability</literal> installation
      pattern, available with the &hasi;.
     </para>
     <para>
      Check if &hasi; is installed as an add-on to &sls; &productnumber; on
      each of the cluster nodes and if the <guimenu>High
      Availability</guimenu> pattern is installed on each of the machines as
      described in <xref linkend="sec.ha.installation.add-on"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Is the initial configuration the same for all cluster nodes? </term>
    <listitem>
     <para>
      In order to communicate with each other, all nodes belonging to the
      same cluster need to use the same <literal>bindnetaddr</literal>,
      <literal>mcastaddr</literal> and <literal>mcastport</literal> as
      described in <xref linkend="sec.ha.installation.setup.manual"/>.
     </para>
     <para>
      Check if the communication channels and options configured in
      <filename>/etc/corosync/corosync.conf</filename> are the same for all
      cluster nodes.
     </para>
     <para>
      In case you use encrypted communication, check if the
      <filename>/etc/corosync/authkey</filename> file is available on all
      cluster nodes.
     </para>
     <para>
      All <filename>corosync.conf</filename> settings with the exception of
      <literal>nodeid</literal> must be the same;
      <filename>authkey</filename> files on all nodes must be identical.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Does the firewall allow communication via the <literal>mcastport</literal>?</term>
    <listitem>
     <para>
      If the mcastport used for communication between the cluster nodes is
      blocked by the firewall, the nodes cannot see each other. When
      configuring the initial setup with &yast; as described in
      <!--taroth 2011-10-17: FIXME, chapter structure has changed: <xref linkend="sec.ha.installation.inst"/>-->, the firewall settings are
      usually automatically adjusted.
     </para>
     <para>
      To make sure the mcastport is not blocked by the firewall, check the
      settings in <filename>/etc/sysconfig/SuSEfirewall2</filename> on each
      node. Alternatively, start the &yast; firewall module on each cluster
      node. After clicking <menuchoice> <guimenu>Allowed Service</guimenu>
      <guimenu>Advanced</guimenu> </menuchoice>, add the mcastport to the
      list of allowed <guimenu>UDP Ports</guimenu> and confirm your changes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Is &ais; started on each cluster node?</term>
    <listitem>
     <para>
      Check the &ais; status on each cluster node with
      <command>/etc/init.d/openais status</command>. In case &ais; is not
      running, start it by executing <command>/etc/init.d/openais
      start</command>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.troublehooting.debugging">
  <title><quote>Debugging</quote> a HA Cluster</title>

  <para>
<!--Troubleshooting your cluster.-->
   The following displays the resource operation history (option
   <option>-o</option>) and inactive resources (<option>-r</option>):
  </para>

<screen>crm_mon -o -r</screen>

  <para>
   The display is refreshed when status changes (to cancel this press
   <keycombo> <keycap function="control"/> <keycap>C</keycap> </keycombo>.)
   An example could look like:
  </para>

  <example>
   <title>Stopped Resources</title>
<screen>Refresh in 10s...

============
Last updated: Mon Jan 19 08:56:14 2009
Current DC: d42 (d42)
3 Nodes configured.
3 Resources configured.
============

Online: [ d230 d42 ]
OFFLINE: [ clusternode-1 ]

Full list of resources:

Clone Set: o2cb-clone
         Stopped: [  o2cb:0 o2cb:1o2cb:2 ]
Clone Set: dlm-clone
         Stopped [ dlm:0 dlm:1 dlm:2 ]
mySecondIP      (ocf::heartbeat:IPaddr):        Stopped

Operations:
* Node d230:
   aa: migration-threshold=1000000
    + (5) probe: rc=0 (ok)
    + (37) stop: rc=0 (ok)
    + (38) start: rc=0 (ok)
    + (39) monitor: interval=15000ms rc=0 (ok)
* Node d42:
   aa: migration-threshold=1000000
    + (3) probe: rc=0 (ok)
    + (12) stop: rc=0 (ok)</screen>
  </example>

  <para>
   First get your node online (see
   <xref linkend="sec.ha.troubleshooting.faq"
    xrefstyle="select:label"/>).
   After that, check your resources and operations.
  </para>

  <para>
   The <citetitle>Configuration Explained</citetitle> PDF covers three
   different recovery types in the 
   <citetitle>How Does the Cluster Interpret the OCF Return Codes?</citetitle> section. 
   It is available from available from <xref linkend="vle.ha.configuration.basics.more.clusterlabs.doc"
     xrefstyle="select: title nopage"/>.
  </para>

<!-- Insert the table here? -->
 </sect1>
 <sect1 id="sec.ha.troubleshooting.managing">
   <title>Managing a HA Cluster</title>
   <!-- FATE#311413 -->
   <para>When configuring a cluster setup, sometimes it requires to run
     commands identical on all cluster nodes.</para>
 </sect1>
  <sect1 id="sec.ha.troubleshooting.faq">
  <title>FAQs</title>

  <variablelist>
   <varlistentry>
    <term>What is the state of my cluster?</term>
    <listitem>
     <para>
      To check the current state of your cluster, use one of the programs
      <literal>crm_mon</literal> or <command>crm</command>
      <option>status</option>. This displays the current DC as well as all
      the nodes and resources known by the current node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Several nodes of my cluster do not see each other.</term>
    <listitem>
     <para></para>
     <para>
      There could be several reasons:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Look first in the configuration file
        <filename>/etc/corosync/corosync.conf</filename> and check if the
        multicast address is the same for every node in the cluster (look in
        the <literal>interface</literal> section with the key
        <literal>mcastaddr</literal>.)
       </para>
      </listitem>
      <listitem>
       <para>
        Check your firewall settings.
       </para>
      </listitem>
      <listitem>
       <para>
        Check if your switch supports multicast addresses
       </para>
      </listitem>
      <listitem>
       <para>
        Check if the connection between your nodes is broken. Most often,
        this is the result of a badly configured firewall. This also may be
        the reason for a <emphasis>split brain</emphasis> condition, where
        the cluster is partitioned.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I want to list my currently known resources.</term>
    <listitem>
     <para>
      Use the command <command>crm_resource -L</command> to learn about your
      current resources.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I configured a resource, but it always fails.</term>
    <listitem>
     <para>
      To check an OCF script use <command>ocf-tester</command>, for
      instance:
     </para>
<screen>ocf-tester -n ip1 -o ip=<replaceable
>YOUR_IP_ADDRESS</replaceable> \
  /usr/lib/ocf/resource.d/heartbeat/IPaddr</screen>
     <para>
      Use <option>-o</option> multiple times for more parameters. The list
      of required and optional parameters can be obtained by running
      <command>crm</command> <option>ra</option> <option>info</option>
      <replaceable>AGENT</replaceable>, for example:
     </para>
<screen>crm ra info ocf:heartbeat:IPaddr</screen>
     <para>
      Before running ocf-tester, make sure the resource is not managed by
      the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I just get a failed message. Is it possible to get more information?</term>
    <listitem>
     <para>
      You may always add the <literal>--verbose</literal> parameter to your
      commands. If you do that multiple times, the debug output becomes very
      verbose. See <filename>/var/log/messages</filename> for useful hints.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>How can I clean up my resources?</term>
    <listitem>
     <para>
      Use the following commands :
     </para>
<screen>crm resource list
crm resource cleanup <replaceable>rscid</replaceable> [<replaceable>node</replaceable>]</screen>
     <para>
      If you leave out the node, the resource is cleaned on all nodes. More
      information can be found in
      <xref
       linkend="sec.ha.manual_config.cleanup"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I can not mount an ocfs2 device.</term>
    <listitem>
     <para>
      Check <filename>/var/log/messages</filename> if there is the following
      line:
     </para>
<screen>Jan 12 09:58:55 clusternode2 lrmd: [3487]: info: RA output: (o2cb:1:start:stderr) 2009/01/12_09:58:55 
  ERROR: Could not load ocfs2_stackglue
Jan 12 16:04:22 clusternode2 modprobe: FATAL: Module ocfs2_stackglue not found.</screen>
     <para>
      In this case the kernel module <filename>ocfs2_stackglue.ko</filename>
      is missing. Install the package
      <filename>ocfs2-kmp-default</filename>,
      <filename>ocfs2-kmp-pae</filename> or
      <filename>ocfs2-kmp-xen</filename> depending on the installed kernel.
     </para>
    </listitem>
   </varlistentry>
<!-- 
    <varlistentry>
      <term>How can I create a report with an analysis of all my cluster nodes?</term>
      <listitem>
          <para>Use the tool <command>hb_report</command> to create a
            report. Usually run <command>hb_report</command> with the
            following command:</para>
        <screen><command>hb_report</command> </screen>
        <para>After the command was successful, you will find an tar.bz2 archive.</para>
        <warning>
          <title>Remove Sensitive Information</title>
            <para>The <command>hb_report</command> tool tries to remove
              any sensitive information from the CIB and the peinput
              files, however, it can not do everything. If you have more
              sensitive information, supply additional patterns
              yourself. The logs and the crm_mon, ccm_tool, and
              crm_verify output are *not* sanitized.</para>
            <para>Before publishing your data to mailinglists or other
              parties, first look into the archive to protect your data
              and remove any information you do not want to
              expose.</para>
        </warning>
      </listitem>
    </varlistentry>
   -->
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.troubleshooting.moreinfo">
  <title>Fore More Information</title>

  <para>
   For additional information about high availability on Linux,
   including configuring cluster resources and managing and customizing a
   &ha; cluster, see
   <ulink
    url="http://clusterlabs.org/wiki/Documentation"/>.
  </para>
 </sect1>
</chapter>
