<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!--taroth 2011-10-12: toms, please also have a look at dmuhamedagic's
 troubleshooting guide and either refer to it for more info (or include what's
 useful if the guide is not publically avaialable)-->
<!--taroth 2011-10-17: toms, please check any contents here for the
 following changes:
 * new deplyoment methods (installation as add-on or as appliance)
 * manual vs. automatic configuration 
 * apart from multicast, unicast is also supported now
   => done
-->
<appendix id="cha.ha.troubleshooting">
 <title>Troubleshooting</title>
 <abstract>
  <para>
   Strange problems may occur that are not easy to understand, especially
   when starting to experiment with &ha;. However, there are several
   utilities that allow you to take a closer look at the &ha; internal
   processes. This chapter recommends various solutions.
  </para>
 </abstract>
 <sect1 id="sec.ha.troubleshooting.install">
  <title>Installation and First Steps</title>

  <para>
   Troubleshooting difficulties when installing the packages or bringing the
   cluster online.
  </para>

  <variablelist>
   <varlistentry>
    <term>Are the HA packages installed?</term>
    <listitem>
     <para>
      The packages needed for configuring and managing a cluster are
      included in the <literal>High Availability</literal> installation
      pattern, available with the &hasi;.
     </para>
     <para>
      Check if &hasi; is installed as an add-on to &sls; &productnumber; on
      each of the cluster nodes and if the <guimenu>High
      Availability</guimenu> pattern is installed on each of the machines as
      described in <xref linkend="sec.ha.installation.add-on"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Is the initial configuration the same for all cluster nodes?</term>
    <listitem>
     <para>
      To communicate with each other, all nodes belonging to the same
      cluster need to use the same <literal>bindnetaddr</literal>,
      <literal>mcastaddr</literal> and <literal>mcastport</literal> as
      described in <xref linkend="sec.ha.installation.setup.manual"/>.
     </para>
     <para>
      Check if the communication channels and options configured in
      <filename>/etc/corosync/corosync.conf</filename> are the same for all
      cluster nodes.
     </para>
     <para>
      In case you use encrypted communication, check if the
      <filename>/etc/corosync/authkey</filename> file is available on all
      cluster nodes.
     </para>
     <para>
      All <filename>corosync.conf</filename> settings with the exception of
      <literal>nodeid</literal> must be the same;
      <filename>authkey</filename> files on all nodes must be identical.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Does the Firewall allow communication via the
            <literal>mcastport</literal>?</term>
    <listitem>
     <para>
      If the mcastport used for communication between the cluster nodes is
      blocked by the firewall, the nodes cannot see each other. When
      configuring the initial setup with &yast; as described in
<!--taroth 2011-10-17: FIXME, chapter structure has changed: <xref linkend="sec.ha.installation.inst"/>-->
      , the firewall settings are usually automatically adjusted.
     </para>
     <para>
      To make sure the mcastport is not blocked by the firewall, check the
      settings in <filename>/etc/sysconfig/SuSEfirewall2</filename> on each
      node. Alternatively, start the &yast; firewall module on each cluster
      node. After clicking <menuchoice> <guimenu>Allowed Service</guimenu>
      <guimenu>Advanced</guimenu> </menuchoice>, add the mcastport to the
      list of allowed <guimenu>UDP Ports</guimenu> and confirm your changes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Is &corosync;;  started on each cluster node?</term>
    <listitem>
     <para>
      Check the &corosync; status on each cluster node with:
     </para>
     <screen>&prompt.root;<command>systemctl</command> status corosync.service</screen> 
     <para>In case &corosync; is not running, start it by executing the following command:
     </para>
     <screen>&prompt.root;<command>systemctl</command> start corosync.service</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.troubleshooting.log">
  <title>Logging</title>

  <variablelist>
   <varlistentry>
    <term>I enabled monitoring but there is no trace of monitoring operations in
          the logs?</term>
    <listitem>
     <para>
      The <systemitem class="daemon">lrmd</systemitem> daemon does not log
      recurring monitor operations unless an error occurred. Logging all
      recurring operations would produce too much noise. Therefore recurring
      monitor operations are logged only once an hour.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I only get a failed message. Is it possible to get more
          information?</term>
    <listitem>
     <para>
      Add the <literal>--verbose</literal> parameter to your commands. If
      you do that multiple times, the debug output becomes quite verbose.
      See <filename>/var/log/messages</filename> for useful hints.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>How can I get an overview of all my nodes and resources?</term>
    <listitem>
     <para>
      Use the <command>crm_mon</command> command. The following displays the
      resource operation history (option <option>-o</option>) and inactive
      resources (<option>-r</option>):
     </para>
<screen>crm_mon -o -r</screen>
     <para>
      The display is refreshed when the status changes (to cancel this press
      <keycombo> <keycap function="control"/> <keycap>C</keycap>
      </keycombo>.) An example may look like:
     </para>
     <example>
      <title>Stopped Resources</title>
<screen>Refresh in 10s...

============
Last updated: Mon Jan 19 08:56:14 2009
Current DC: d42 (d42)
3 Nodes configured.
3 Resources configured.
============

Online: [ d230 d42 ]
OFFLINE: [ clusternode-1 ]

Full list of resources:

Clone Set: o2cb-clone
         Stopped: [  o2cb:0 o2cb:1o2cb:2 ]
Clone Set: dlm-clone
         Stopped [ dlm:0 dlm:1 dlm:2 ]
mySecondIP      (ocf::heartbeat:IPaddr):        Stopped

Operations:
* Node d230:
   aa: migration-threshold=1000000
    + (5) probe: rc=0 (ok)
    + (37) stop: rc=0 (ok)
    + (38) start: rc=0 (ok)
    + (39) monitor: interval=15000ms rc=0 (ok)
* Node d42:
   aa: migration-threshold=1000000
    + (3) probe: rc=0 (ok)
    + (12) stop: rc=0 (ok)</screen>
     </example>
     <para>
      First get your node online, then check your resources and operations.
     </para>
     <para>
      The <citetitle>Configuration Explained</citetitle> PDF covers three
      different recovery types in the <citetitle>How Does the Cluster
      Interpret the OCF Return Codes?</citetitle> section. It is available
      at
      <xref
              linkend="vle.ha.configuration.basics.more.clusterlabs.doc"
              xrefstyle="select: title nopage"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
     <term>Is &corosync; started on each cluster node?</term>
     <listitem>
       <para>Check the OpenAIS status on each cluster node with 
         <command>systemctl</command> <option>status corosync.service</option>.
         In case &corosync; is not running, start it by executing 
         <command>systemctl</command> <option>start corosync.service</option>.
       </para>
     </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.troubleshooting.resource">
  <title>Resources</title>

  <variablelist>
   <varlistentry>
    <term>How can I clean up my resources?</term>
    <listitem>
     <para>
      Use the following commands :
     </para>
<screen>crm resource list
crm resource cleanup <replaceable>rscid</replaceable> [<replaceable>node</replaceable>]</screen>
     <para>
      If you leave out the node, the resource is cleaned on all nodes. More
      information can be found in
      <xref
              linkend="sec.ha.manual_config.cleanup"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>How can I list my currently known resources?</term>
    <listitem>
     <para>
      Use the command <command>crm resource list</command> to display your
      current resources.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>I configured a resource, but it always fails. Why?</term>
    <listitem>
     <para>
      To check an OCF script use <command>ocf-tester</command>, for
      instance:
     </para>
<screen>ocf-tester -n ip1 -o ip=<replaceable>YOUR_IP_ADDRESS</replaceable> \
  /usr/lib/ocf/resource.d/heartbeat/IPaddr</screen>
     <para>
      Use <option>-o</option> multiple times for more parameters. The list
      of required and optional parameters can be obtained by running
      <command>crm</command> <option>ra</option> <option>info</option>
      <replaceable>AGENT</replaceable>, for example:
     </para>
<screen>crm ra info ocf:heartbeat:IPaddr</screen>
     <para>
      Before running ocf-tester, make sure the resource is not managed by
      the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Why do resources not fail over and why are there no errors?</term>
    <listitem>
     <para>
      If your cluster is a two node cluster, killing one node will leave the
      remaining node without quorum. Unless you set the
      <option>no-quorum-policy</option> property to
      <literal>ignore</literal>, nothing happens. For two-node clusters you
      need:
     </para>
<screen>property no-quorum-policy="ignore"</screen>
     <para>
      Another possibility is that the killed node is considered unclean.
      Then it is necessary to fence it. If the stonith resource is not
      operational or does not exist, the remaining node will waiting for the
      fencing to happen. The fencing timeouts are typically high, so it may
      take quite a while to see any obvious sign of problems (if ever).
     </para>
     <para>
      Yet another possible explanation is that a resource is simply not
      allowed to run on this node. That may be due to a failure which
      happened in the past and which was not <quote>cleaned</quote>. Or it
      may be due to an earlier administrative action, i.e. a location
      constraint with a negative score. Such a location constraint is for
      instance inserted by the <command>crm resource migrate</command>
      command.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Why can I never tell where my resource will run?</term>
    <listitem>
     <para>
      If there are no location constraints for a resource, its placement is
      subject to an (almost) random node choice. You are well advised to
      always express a preferred node for resources. That does not mean that
      you need to specify location preferences for <emphasis>all</emphasis>
      resources. One preference suffices for a set of related (colocated)
      resources. A node preference looks like this:
     </para>
<screen>location rsc-prefers-node1 rsc 100: node1</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.troubleshooting.stonith">
  <title>&stonith; and Fencing</title>

  <variablelist>
   <varlistentry>
    <term>Why does my &stonith; resource not start?</term>
    <listitem>
     <para>
      Start (or enable) operation includes checking the status of the
      device. If the device is not ready, the &stonith; resource will fail
      to start.
     </para>
     <para>
      At the same time the &stonith; plugin will be asked to produce a host
      list. If this list is empty, there is no point in running a &stonith;
      resource which cannot shoot anything. The name of the host on which
      &stonith; is running is filtered from the list, since the node cannot
      shoot itself.
     </para>
     <para>
      If you want to use single-host management devices such as lights-out
      devices, make sure that the stonith resource is
      <emphasis>not</emphasis> allowed to run on the node which it is
      supposed to to fence. Use an infinitely negative location node
      preference (constraint). The cluster will move the stonith resource to
      another place where it can start, but not before informing you.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Why does fencing not happen, although I have the &stonith; resource?</term>
    <listitem>
     <para>
      Each &stonith; resource must provide a host list. This list may be
      inserted by hand in the &stonith; resource configuration or retrieved
      from the device itself, for instance from outlet names. That depends
      on the nature of the &stonith; plugin.
      <systemitem>stonithd</systemitem> uses the list to find out which
      &stonith; resource can fence the target node. Only if the node appears
      in the list can the &stonith; resource shoot (fence) the node.
     </para>
     <para>
      If <systemitem>stonithd</systemitem> does not find the node in any of
      the host lists provided by running &stonith; resources, it will ask
      <systemitem>stonithd</systemitem> instances on other nodes. If the
      target node does not show up in the host lists of other
      <systemitem>stonithd</systemitem> instances, the fencing request ends
      in a timeout at the originating node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Why does my &stonith; resource fail occasionally?</term>
    <listitem>
     <para>
      Power management devices may give up if there is too much broadcast
      traffic. Space out the monitor operations. Given that fencing is
      necessary only once in a while (and hopefully never), checking the
      device status once a few hours is more than enough.
     </para>
     <para>
      Also, some of these devices may refuse to talk to more than one party
      at the same time. This may be a problem if you keep a terminal or
      browser session open while the cluster tries to test the status.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.troubleshooting.misc">
  <title>Miscellaneous</title>

  <variablelist>
   <varlistentry>
<!-- FATE#311413 -->
    <term>How can I run commands on all cluster nodes?</term>
    <listitem>
     <para>
      Use the command <command>pssh</command> for this task. If necessary,
      install <systemitem class="resource">pssh</systemitem>. Create a file
      (for example <filename>hosts.txt</filename>) where you collect all
      your IP addresses or hostnames you want to visit. Make sure you can
      log in with <command>ssh</command> to each host listed in your
      <filename>hosts.txt</filename> file. If everything is correctly
      prepared, execute <command>pssh</command> and use the
      <filename>hosts.txt</filename> file (option <option>-h</option>) and
      the interactive mode (option <option>-i</option>) as shown in this
      example:
     </para>
<screen>pssh -i -h hosts.txt "ls -l /corosync/*.conf"
[1] 08:28:32 [SUCCESS] root@&wsII;.&exampledomain;
-rw-r--r-- 1 root root 1480 Nov 14 13:37 /etc/corosync/corosync.conf
[2] 08:28:32 [SUCCESS] root@&wsIIIip;
-rw-r--r-- 1 root root 1480 Nov 14 13:37 /etc/corosync/corosync.conf</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>What is the state of my cluster?</term>
    <listitem>
     <para>
      To check the current state of your cluster, use one of the programs
      <literal>crm_mon</literal> or <command>crm</command>
      <option>status</option>. This displays the current DC as well as all
      the nodes and resources known by the current node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Why can several nodes of my cluster not see each other?</term>
    <listitem>
     <para>
      There could be several reasons:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Look first in the configuration file
        <filename>/etc/corosync/corosync.conf</filename> and check if the
        multicast or unicast address is the same for every node in the
        cluster (look in the <literal>interface</literal> section with the
        key <literal>mcastaddr</literal>.)
       </para>
      </listitem>
      <listitem>
       <para>
        Check your firewall settings.
       </para>
      </listitem>
      <listitem>
       <para>
        Check if your switch supports multicast or unicast addresses.
       </para>
      </listitem>
      <listitem>
       <para>
        Check if the connection between your nodes is broken. Most often,
        this is the result of a badly configured firewall. This also may be
        the reason for a <emphasis>split brain</emphasis> condition, where
        the cluster is partitioned.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Why can an OCFS2 device not be mounted?</term>
    <listitem>
     <para>
      Check <filename>/var/log/messages</filename> for the following line:
     </para>
<screen>Jan 12 09:58:55 clusternode2 lrmd: [3487]: info: RA output: (o2cb:1:start:stderr) 2009/01/12_09:58:55 
  ERROR: Could not load ocfs2_stackglue
Jan 12 16:04:22 clusternode2 modprobe: FATAL: Module ocfs2_stackglue not found.</screen>
     <para>
      In this case the Kernel module <filename>ocfs2_stackglue.ko</filename>
      is missing. Install the package
      <filename>ocfs2-kmp-default</filename>,
      <filename>ocfs2-kmp-pae</filename> or
      <filename>ocfs2-kmp-xen</filename>, depending on the installed Kernel.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.crm_report">
<!-- FATE#310174 -->
    <term>How can I create a report with an analysis of all my cluster nodes?</term>
    <listitem>
     <para>
      Use the tool <command>crm_report</command> to create a report. The tool
      is used to compile:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Cluster-wide log files,
       </para>
      </listitem>
      <listitem>
       <para>
        Package states,
       </para>
      </listitem>
      <listitem>
       <para>
        DLM/OCFS2 states,
       </para>
      </listitem>
      <listitem>
       <para>
        System information,
       </para>
      </listitem>
      <listitem>
       <para>
        CIB history,
       </para>
      </listitem>
      <listitem>
       <para>
        Parsing of core dump reports, if a debuginfo package is installed.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Usually run <command>crm_report</command> with the following command:
     </para>
<screen>&prompt.root;<command
>crm_report</command> -f 0:00 -n &wsI; -n &wsII;</screen>
     <para>
      The command extracts all information since 0am on the hosts
      &wsI; and &wsII; and creates a tar.bz2 archive
      named
      <filename>crm_report-<replaceable>DATE</replaceable>.tar.bz2</filename>
      in the current directory, for example,
      <filename>crm_report-Wed-03-Mar-2012</filename>. If you are only
      interested in a specific time frame, add the end time with the
      <option>-t</option> option.
     </para>
     <warning>
      <title>Remove Sensitive Information</title>
      <para>
       The <command>crm_report</command> tool tries to remove any sensitive
       information from the CIB and the peinput files, however, it can not
       do everything. If you have more sensitive information, supply
       additional patterns. The logs and the <command>crm_mon</command>,
       <command>ccm_tool</command>, and <command>crm_verify</command> output
       are <emphasis>not</emphasis> sanitized.
      </para>
      <para>
       Before sharing your data in any way, check the archive and remove all
       information you do not want to expose.
      </para>
     </warning>
     <para>
      Customize the command execution with further options. For example, if
      you have a Pacemaker cluster, you certainly want to add the option
      <option>-A</option>. In case you have another user who has permissions
      to the cluster, use the <option>-u</option> option and specify this
      user (in addition to &rootuser; and
      <systemitem class="username">hacluster</systemitem>). In case you have
      a non-standard SSH port, use the <option>-X</option> option to add the
       port (for example, with the port 3479, use <literal>-X "-p 3479"</literal>). 
      Further options can be found in the man page of <command>crm_report</command>.
     </para>
     <para>
      After <command>crm_report</command> analyzed all the relevant log files
      and created the directory (or archive), check the logs for an
      uppercase <literal>ERROR</literal> string. The most important files in
      the top level directory of the report are:
     </para>
     <variablelist>
      <varlistentry>
       <term><filename>analysis.txt</filename>
       </term>
       <listitem>
        <para>
         Compares files that should be identical on all nodes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><filename>crm_mon.txt</filename>
       </term>
       <listitem>
        <para>
         Contains the output of the <command>crm_mon</command> command.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><filename>corosync.txt</filename>
       </term>
       <listitem>
        <para>
         Contains a copy of the &corosync; configuration file.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><filename>description.txt</filename>
       </term>
       <listitem>
        <para>
         Contains all cluster package versions on your nodes. There is also
         the <filename>sysinfo.txt</filename> file which is node specific.
         It is linked to the top directory.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Node-specific files are stored in a subdirectory named by the node's
      name.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.troubleshooting.moreinfo">
  <title>Fore More Information</title>

  <para>
   For additional information about high availability on Linux, including
   configuring cluster resources and managing and customizing a &ha;
   cluster, see
   <ulink
    url="http://clusterlabs.org/wiki/Documentation"/>.
  </para>
 </sect1>
</appendix>
