<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- 
  toms 2010-03-02: Future idea(s)
  * It seems to me, this chapter needs a strong editor and proofreader.
  Some paragraphs can be converted into a procedure to remove the blurb.
  * A section "For More Information" is missing.
-->
<!--taroth 2010-03-19: done in a last minute tour de force action now, for next revision,
please rephrase and restructure directly when XML-izing developer input-->

<!--taroth 2010-03-19: some sections have IDs that do not match our style guide
 conventions due to the last minute changes down here, no time left to fix this now--> 

<chapter id="cha.ha.storage.protect">
 <title>Storage Protection</title>
 <abstract>
<!--
   toms 2010-03-02:
   This abstract has to be shortend or moved to another sect, but 
   for the time being, I've inserted it here:
  -->
  <para>
   The &ha; cluster stack's highest priority is protecting the integrity of
   data. This is achieved by preventing uncoordinated concurrent access to
   data storage: For example, ext3 file systems are only mounted once in the
   cluster, OCFS2 volumes will not be mounted unless coordination with other
   cluster nodes is available. In a well-functioning cluster Pacemaker will
   detect if resources are active beyond their concurrency limits and
   initiate recovery. Furthermore, its policy engine will never exceed these
   limitations.
  </para>

  <para>
   However, network partitioning or software malfunction could potentially
   cause scenarios where several coordinators are elected. If this so-called
   split brain scenarios were allowed to unfold, data corruption might
   occur. Hence, several layers of protection have been added to the cluster
   stack to mitigate this.
  </para>

  <para>
   The primary component contributing to this goal is IO fencing/&stonith;
   since it ensures that all other access prior to storage activation is
   terminated. Other mechanisms are cLVM2 exclusive activation or OCFS2 file
   locking support to protect your system against administrative or
   application faults. Combined appropriately for your setup, these can
   reliably prevent split-brain scenarios from causing harm.
  </para>

  <para>
   This chapter describes an IO fencing mechanism that leverages the storage
   itself, followed by the description of an additional layer of protection
   to ensure exclusive storage access. These two mechanisms can be combined
   for higher levels of protection.
  </para>
 </abstract>
 <sect1 id="sec.ha.storage.protect.fencing">
  <title>Storage-based Fencing</title>

  <para>
   You can reliably avoid split-brain scenarios by using Split Brain
   Detector (SBD), <literal>watchdog</literal> support and the
   <literal>external/sbd</literal> &stonith; agent.
  </para>

  <sect2 id="sec.ha.storage.protect.fencing.oview">
   <title>Overview</title>
   <para>
    In an environment where all nodes have access to shared storage, a small
    partition (1MB) is formated for the use with SBD. After the respective
    daemon is configured, it is brought online on each node before the rest
    of the cluster stack is started. It is terminated after all other
    cluster components have been shut down, thus ensuring that cluster
    resources are never activated without SBD supervision.
   </para>
   <para>
    The daemon automatically allocates one of the message slots on the
    partition to itself, and constantly monitors it for messages addressed
    to itself. Upon receipt of a message, the daemon immediately complies
    with the request, such as initiating a power-off or reboot cycle for
    fencing.
   </para>
   <para>
    The daemon constantly monitors connectivity to the storage device, and
    terminates itself in case the partition becomes unreachable. This
    guarantees that it is not disconnected from fencing messages. If the
    cluster data resides on the same logical unit in a different partition,
    this is not an additional point of failure: The work-load will terminate
    anyway if the storage connectivity has been lost.
   </para>
   <para>
    Increased protection is offered through <literal>watchdog</literal>
    support. Modern systems support a <literal>hardware watchdog</literal>
    that has to be updated by the software client, otherwise the hardware
    will enforce a system restart. This protects against failures of the SBD
    process itself, such as dying, or becoming stuck on an IO error.
   </para>
  </sect2>

  <sect2 id="sec.ha.storageprotection.fencing.setup">
   <title>Setting Up Storage-based Protection</title>
   <para>
    The following steps are necessary to set up storage-based protection:
   </para>
   <procedure>
    <step>
     <para>
      <xref linkend="pro.ha.storage.protect.sbd.create" xrefstyle="select:title"
      />
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro.ha.storage.protect.watchdog" xrefstyle="select:title"/>
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro.ha.storage.protect.sbd.daemon" xrefstyle="select:title"
      />
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro.ha.storage.protect.sbd.test" xrefstyle="select:title"/>
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro.ha.storage.protect.fencing" xrefstyle="select:title"/>
     </para>
    </step>
   </procedure>
   <para>
    All of the following procedures must be executed as &rootuser;. Before
    you start, make sure the following requirements are met:
   </para>
   <important>
    <title>Requirements</title>
    <itemizedlist>
     <listitem>
      <para>
       The environment must have shared storage reachable by all nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       The shared storage segment must not make use of host-based RAID,
       cLVM2, nor DRBD.
      </para>
     </listitem>
     <listitem>
      <para>
       However, using storage-based RAID and multipathing is recommended for
       increased reliability.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <sect3 id="pro.ha.storage.protect.sbd.create">
    <title>Creating the SBD Partition</title>
    <para>
     It is recommended to create a 1MB partition at the start of the device.
     If your SBD device resides on a multipath group, you need to adjust the
     timeouts SBD uses, as MPIO's path down detection can cause some
     latency. After the <literal>msgwait</literal> timeout, the message is
     assumed to have been delivered to the node. For multipath, this should
     be the time required for MPIO to detect a path failure and switch to
     the next path. You may have to test this in your environment. The node
     will commit suicide if it has not updated the watchdog timer fast
     enough.
<!--mdejmek: the node updates watchdog??-->
     The <literal>watchdog</literal> timeout must be shorter than the
     <literal>msgwait</literal> timeout&mdash;half the value is a good
     estimate.
    </para>
    <note>
     <title>Device Name for SBD Partition</title>
     <para> In the following, this SBD partition is referred to by
        <filename>/dev/<replaceable>SBD</replaceable></filename>. Replace it
      with your actual pathname, for example: <filename>/dev/sdc1</filename>.
     </para>
    </note>
    <important>
     <title>Overwriting Existing Data</title>
     <para>
      Make sure the device you want to use for SBD does not hold any data.
      The <command>sdb</command> command will overwrite the device without
      further requests for confirmation.
     </para>
    </important>
    <procedure>
     <step>
      <para>
       Initialize the SBD device with the following command:
      </para>
<screen>sbd -d /dev/<replaceable>SBD</replaceable> create</screen>
      <para>
       This will write a header to the device, and create slots for up to
       255 nodes sharing this device with default timings.
      </para>
     </step>
     <step>
      <para> If your SBD device resides on a multipath group, adjust the
       timeouts SBD uses. This can be specified when the SBD device is
       initialized (all timeouts are given in seconds): </para>
      <remark>taroth 2010-06-22: todo - fix
       http://doccomments.provo.novell.com/admin/viewcomment/14391#:
       
       1. specifying $msgwait $watchdogtimeout in the example is misleading. 
       A more standard usage like &lt;msgwait&gt; and &lt;watchdogtimeout&gt; should be used. 
       lso a real example should be given (ex. sbd -d /dev/mapper/mpathc -4 180 -1 90 create)
       
       taroth 2010-08-12: changed below, waiting for confirmation by [ha-devel]
      </remark>
<screen>/usr/sbin/sbd -d /dev/<replaceable>SDB</replaceable> -4 180<co id="co.msgwait"/> -1 90<co id="co.watchdog"/> create</screen>
      <calloutlist>
       <callout arearefs="co.msgwait">
        <para>The <option>-4</option> option is used to specify the
          <literal>msgwait</literal> timeout. In the example above, it is set to
          <literal>180</literal> seconds.</para>
       </callout>
       <callout arearefs="co.watchdog">
        <para>The <option>-1</option> option is used to specify the
          <literal>watchdog</literal> timeout. In the example above, it is set
         to <literal>90</literal> seconds.</para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       With the following command, check what has been written to the
       device:
      </para>
<screen>sbd -d /dev/<replaceable>SBD</replaceable> dump 
Header version     : 2
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 5
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 10</screen>
     </step>
    </procedure>
    <para>
     As you can see, the timeouts are also stored in the header, to ensure
     that all participating nodes agree on them.
    </para>
   </sect3>
   <sect3 id="pro.ha.storage.protect.watchdog">
    <title>Setting Up the Software Watchdog</title>
    
    <remark>taroth 2010-06-22: todo - fix
     http://doccomments.provo.novell.com/admin/viewcomment/14237#
     Need precisions as how to set up watchdog - taroth 2010-08-16: asked
     ha-devel and NTS for more info - waiting for reply</remark>
    
    <para>
     It is highly recommended to set up your Linux system to use a watchdog.
     <remark>lmb: Please refer to the SLES manual for this step(?). -
      taroth 2010-03-19: watchdog this isn't covered anywhere in our manuals so
      far, include for next revision</remark>
     This involves loading the proper watchdog driver on system boot.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       On HP hardware, this is the <systemitem>hpwdt</systemitem> module.
      </para>
     </listitem>
     <listitem>
      <para>
       For systems with a Intel TCO, <literal>iTCO_wdt</literal> can be
       used. <literal>softdog</literal> is the most generic driver, but it
       is recommended to use a driver with actual hardware integration.
      </para>
     </listitem>
    </itemizedlist>
    <para>For a list of choices, refer to
       <filename>/usr/src/<replaceable>your_kernel_version</replaceable>/drivers/watchdog</filename>.</para>
<!-- mdejmek: it is not really explained here HOW to set up a watchdog-->
   </sect3>
   <sect3 id="pro.ha.storage.protect.sbd.daemon">
    <title>Starting the SBD Daemon</title>
    <para>
     The SBD daemon is a critical piece of the cluster stack. It has to be
     running when the cluster stack is running, or even when part of it has
     crashed, so that it
     <remark>taroth 2010-03-19: unclear reference: what is
     "it" referring to here? clarify for next revision!</remark>
     can be fenced.
    </para>
    <procedure>
     <step>
      <para>
       To make the &ais; init script start and stop SDB, add the following
       to /etc/sysconfig/sbd:
      </para>
<screen>SBD_DEVICE="/dev/<replaceable>SBD</replaceable>"
# The next line enables the watchdog support:
SBD_OPTS="-W"</screen>
      <para>
       If the SBD device is not accessible, the daemon will fail to start
       and inhibit &ais; startup.
      </para>
      <note>
       <para>
        If the SBD device becomes inaccessible from a node, this could cause
        the node to enter an infinite reboot cycle. That is technically
        correct, but depending on your administrative policies, might be
        considered a nuisance. You may wish to not automatically start up
        &ais; on boot in such cases.
       </para>
      </note>
     </step>
     <step>
      <para>
       Before proceeding, ensure that SBD has started on all nodes by
       executing <command>rcopenais</command>&nbsp;<option>restart</option>.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 id="pro.ha.storage.protect.sbd.test">
    <title>Testing SBD</title>
    <procedure>
     <step>
      <para>
       The following command will dump the node slots and their current
       messages from the SBD device:
      </para>
<screen>sbd -d /dev/<replaceable>SBD</replaceable> list</screen>
      <para>
       Now you should see all cluster nodes that have ever been started with
       SBD listed here, the message slot should show
       <literal>clear</literal>.
      </para>
     </step>
     <step>
      <para>
       Try sending a test message to one of the nodes:
      </para>
<screen>sbd -d /dev/<replaceable>SBD</replaceable> message nodea test</screen>
     </step>
     <step>
      <para>
       The node will acknowledge the receipt of the message in the system
       logs:
      </para>
<screen>Aug 29 14:10:00 nodea sbd: [13412]: info: Received command test from nodeb</screen>
      <para>
       This confirms that SBD is indeed up and running on the node, and that
       it is ready to receive messages.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 id="pro.ha.storage.protect.fencing">
    <title>Configuring the Fencing Resource</title>
    <procedure>
     <step>
      <para>
       To complete the SBD setup, it is necessary to activate SBD as a
       &stonith;/fencing mechanism in the CIB as follows:
      </para>
      <!--taroth 2010-06-29: fixed bnc#617920-->
<screen><command>crm</command> configure
crm(live)configure# <command>property</command> stonith-enabled="true"
crm(live)configure# <command>property</command> stonith-timeout="30s"
crm(live)configure# <command>primitive</command> stonith_sbd stonith:external/sbd params sbd_device="/dev/<replaceable>SBD</replaceable>"
crm(live)configure# <command>commit</command>
crm(live)configure# <command>quit</command></screen>
      <para>Which value to set for <literal>stonith-timeout</literal> depends on
       the <literal>msgwait</literal> timeout. Provided you kept the default
        <literal>msgwait</literal> timeout value (<literal>10</literal>
       seconds), setting <literal>stonith-timeout</literal> to
        <literal>30</literal> seconds is appropriate.</para>
      <para>
       Since node slots are allocated automatically, no manual hostlist
       needs to be defined.
      </para>
     </step>
     <step>
      <para>
       Disable any other fencing devices you might have configured before,
       since the SBD mechanism is used for this function now.
      </para>
     </step>
    </procedure>
    <para>
     Once the resource has started, your cluster is now successfully
     configured for shared-storage fencing, and will utilize this method in
     case a node needs to be fenced.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 id="sec.ha.storageprotection.exstoract">
  <title>Ensuring Exclusive Storage Activation</title>

  <para>
   This section introduces <literal>sfex</literal>, an additional low-level
   mechanism to lock access to shared storage exclusively to one node. Note
   that sfex does not replace &stonith;. Since sfex requires shared storage,
   it is recommended that the <literal>external/sbd</literal> fencing
   mechanism described above is used on another partition of the storage.
  </para>

<!--mdejmek: what does By Design mean??-->

  <para>
   By design, sfex cannot be used in conjunction with workloads that require
   concurrency (such as OCFS2), but serves as a layer of protection for
   classic fail-over style workloads. This is similar to a SCSI-2
   reservation in effect, but more general.
  </para>

  <sect2 id="sec.ha.storageprotection.exstoract.description">
   <title>Overview</title>
   <para>
    In a shared storage environment, a small partition of the storage is set
    aside for storing one or more locks.
   </para>
   <para>
    Before acquiring protected resources, the node must first acquire the
    protecting lock. The ordering is enforced by Pacemaker, and the sfex
    component ensures that even if Pacemaker were subject to a split-brain
    situation, the lock will never be granted more than once.
   </para>
   <para>
    These locks must also be refreshed periodically, so that a node's death
    does not permanently block the lock and other nodes can proceed.
   </para>
  </sect2>

  <sect2 id="sec.ha.storageprotection.exstoract.requirements">
   <title>Setup</title>
   <para>
    In the following, learn how to create a shared partition for use with
    sfex and how to configure a resource for the sfex lock in the CIB. A
    single sfex partition can hold any number of locks, it
    <remark>taroth 2010-03-19: again, unclear reference - what is "it"referring
     to here? clarify for next revision!</remark>
    defaults to one, and needs 1 KB of storage space allocated per lock.
   </para>
   <important>
    <title>Requirements</title>
    <itemizedlist>
     <listitem>
      <para>
       The shared partition for sfex should be on the same logical unit as
       the data you wish to protect.
      </para>
     </listitem>
     <listitem>
      <para>
       The shared sfex partition must not make use of host-based RAID, nor
       DRBD.
      </para>
     </listitem>
     <listitem>
      <para>
       Using a cLVM2 logical volume is possible.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <procedure>
    <title>Creating an sfex Partition</title>
    <step>
     <para>
      Create a shared partition for use with sfex. Note the name of this
      partition and use it as a substitute for
      <filename>/dev/sfex</filename> below.
     </para>
    </step>
    <step>
     <para>
      Create the sfex meta data with the following command:
     </para>
<screen>sfex_init -i 1 /dev/sfex</screen>
    </step>
    <step>
     <para>
      Verify that the meta data has been created correctly:
     </para>
<screen>sfex_stats -i 1 /dev/sfex ; echo $?</screen>
     <para>
      This should return <literal>2</literal>, since the lock is not
      currently held.
     </para>
    </step>
   </procedure>
   <procedure>
    <title>Configuring a Resource for the sfex Lock</title>
    <step>
     <para>
      The sfex lock is represented via a resource in the CIB, configured as
      follows:
     </para>
<screen>primitive sfex_1 ocf:heartbeat:sfex \
#	params device="/dev/sfex" index="1" collision_timeout="1" \
      lock_timeout="70" monitor_interval="10" \
#	op monitor interval="10s" timeout="30s" on_fail="fence"</screen>
    </step>
    <step>
     <para>
      To protect resources via a sfex lock, create mandatory ordering and
      placement constraints between the protectees and the sfex resource. If
      the resource to be protected has the id
      <literal>filesystem1</literal>:
     </para>
<screen># order order-sfex-1 inf: sfex_1 filesystem1
# colocation colo-sfex-1 inf: filesystem1 sfex_1</screen>
    </step>
    <step>
     <para>
      If using group syntax, add the sfex resource as the first resource to
      the group:
     </para>
<screen># group LAMP sfex_1 filesystem1 apache ipaddr</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
