<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- 
  toms 2010-03-02: Future idea(s)
  * It seems to me, this chapter needs a strong editor and proofreader.
  Some paragraphs can be converted into a procedure to remove the blurb.
  * A section "For More Information" is missing.
-->
<chapter id="cha.ha.storage.protect">
 <title>Storage Protection</title>
 <abstract>
<!--
   toms 2010-03-02:
   This abstract has to be shortend or moved to another sect, but 
   for the time being, I've inserted it here:
  -->
  <para>
   The &ha; cluster stack's highest priority is protecting the integrity of
   data. This is achieved by preventing uncoordinated concurrent access to
   data storage, such as mounting an ext3 file system more than once in the
   cluster, but also by preventing OCFS2 from being mounted if coordination
   with other cluster nodes is not available. In a well-functioning cluster
   Pacemaker will detect, if resources are active beyond their concurrency
   limits and initiate recovery; furthermore, its policy engine will never
   exceed these limitations.
  </para>

  <para>
   However, network partitioning or software malfunction could potentially
   cause scenarios where several coordinators are elected. If this so-called
   split brain scenario are allowed to unfold, data corruption might occur.
   Hence, several layers of protection have been added to the cluster stack
   to mitigate this.
  </para>

  <para>
   IO fencing/&stonith; is the primary component contributing to this goal,
   since it ensures that all other access prior to storage activation is terminated.
   cLVM2 exclusive activation or OCFS2 file locking support are
   other mechanisms, to protect your system against administrative or application
   faults. Combined appropriately for your setup, these can reliably prevent
   split-brain scenarios from causing harm.
  </para>

  <para>
   This chapter describes an IO fencing mechanism that leverages the storage
   itself, followed by the description of an additional layer of protection
   to ensure exclusive storage access. These two mechanisms can even be
   combined for higher levels of protection.
  </para>
 </abstract>
 <sect1 id="sec.ha.storageprotection.fencing">
  <title>Storage-based Fencing</title>

  <para>
   This section describes how to use shared storage for very reliable I/O fencing and 
   how to avoid split-brain scenarios.
  </para>

  <para>
   The storage-based fencing has been used successfully with the Novell Cluster Suite
   and is also available in a similar fashion for the &sle; &ha;&nbsp;11
   product using the <quote>external/sbd</quote> &stonith; agent.
  </para>

  <sect2 id="sec.ha.storageprotection.fencing.description">
   <title>Description</title>
   <para>
    In an environment where all nodes have access to shared storage, a small
    (1MB) partition is formated for the use with SBD. The daemon, once
    configured, is brought online on each node before the rest of the
    cluster stack is started. It is then terminated after all other cluster
    components have been shut down&mdash; ensuring that cluster resources are
    never activated without SBD supervision.
   </para>
   <para>
    The daemon automatically allocates one of the message slots on the
    partition to itself, and constantly monitors it for messages.
    Upon the receipt of a message, the daemon immediately complies with the
    request, such as initiating a power-off or reboot cycle for fencing.
   </para>
   <para>
    The daemon constantly monitors connectivity to the storage device,
    and commits suicide in case the partition becomes unreachable,
    guaranteeing that it is not disconnected from fencing messages. (If the
    cluster data resides on the same logical unit in a different partition,
    this is not an additional point of failure; the work-load would
    terminate anyway if the storage connectivity was lost.)
   </para>
   <para>
    Increased protection is offered through <quote>watchdog</quote> support.
    Modern systems support a <quote>hardware watchdog</quote> that has to be
    updated by the software client, otherwise the hardware will enforce a
    system restart. This protects against failures of the SBD process
    itself, such as dying, or becoming stuck on an IO error.
   </para>
  </sect2>

  <sect2 id="sec.ha.storageprotection.fencing.setup">
   <title>Setup Guide</title>
   <para></para>
   <sect3 id="sec.ha.storageprotection.fencing.requirement">
    <title>Requirements</title>
    <para>
     The environment must have shared storage reachable by all nodes. It is
     recommended to create a 1MB partition at the start of the device.
     In the following, this is referred to as
     <filename>/dev/SBD</filename>, please substitute your actual pathname
     (for example, <filename>/dev/sdc1</filename>) for this below.
    </para>
    <para>
     This shared storage segment must not make use of host-based RAID,
     cLVM2, nor DRBD.
    </para>
    <para>
     However, using storage-based RAID and multipathing is recommended for
     increased reliability.
    </para>
   </sect3>
   <sect3 id="sec.ha.storageprotection.fencing.sdbpartition">
    <title><remark>MISSING TITLE</remark></title>
   <procedure>
   <title>SBD Partition</title>
   <!-- mdejmek: find better title here-->
   <step>
    <para>
     Log in as &rootuser;.
    </para>
    </step>
    <step>
    <para>
    Make sure you choose the correct device. The <command>sdb</command> 
    command will overwrite it without further requests for confirmation&mdash;.
    </para>
    </step>
    <step>
    <para>
    Initialize the SBD device with the following command:
     </para>
    <screen>sbd -d /dev/SBD create</screen>
    <para>
     This will write a header to the device, and create slots for up to 255
     nodes sharing this device with default timings.
    </para>
    </step>
    <step>
    <para>
    Adjust the timeouts SBD uses, if your SBD device resides on a multipath group, 
    as MPIO's path down detection can cause some latency. After the msgwait timeout, the message is assumed to have been
     delivered to the node. For multipath, this should be the time required
     for MPIO to detect a path failure and switch to the next path. You may
     have to test this in your environment. The node will perform suicide if
     it has not updated the watchdog timer fast enough; the watchdog timeout
     must be shorter than the msgwait timeout&mdash;half the value is a good
     estimate. This can be specified when the SBD device is initialized:
    </para>
<screen>/usr/sbin/sbd -d /dev/SBD -4 $msgwait -1 $watchdogtimeout create</screen>
    <para>
     All timeouts are in seconds. See what has been written to the
     device using the following command:
    </para>
<screen>sbd -d /dev/SBD dump 
 Header version     : 2
 Number of slots    : 255
 Sector size        : 512
 Timeout (watchdog) : 5
 Timeout (allocate) : 2
 Timeout (loop)     : 1
 Timeout (msgwait)  : 10</screen>
    <para>
     As you can see, the timeouts are also stored in the header, to ensure
     that all participating nodes agree on them.
    </para>
    </step>
    </procedure>
  <procedure>
    <title>Setup the Software Watchdog</title>
    <para>
     It is highly recommended to set up your Linux system to use a watchdog.
     <remark>Please refer to the SLES manual for
     this step(?).</remark>
    </para>
    <step>
    <para>
     This involves loading the proper watchdog driver on system boot. On HP
     hardware, this is the <systemitem>hpwdt</systemitem> module. 
     </para>
     </step>
     <step>
     <para>
     For
     systems with a Intel TCO, <literal>iTCO_wdt</literal> can be used.
     <literal>softdog</literal> is the most generic driver, but it is
     recommended that you use one with actual hardware integration. 
     </para>
     </step>
     <step>
     <para>
     See <filename>drivers/watchdog</filename> in the kernel package for a list
     of choices.
    </para>
    </step>
    <!-- mdejmek: it is not really explained here HOW to set up a watchdog-->
   </sect3>
   <sect3 id="sec.ha.storageprotection.fencing.setupsdbdaemon">
    <title>Starting the SBD Daemon</title>
    <para>
     The SBD daemon is a critical piece of the cluster stack. It has to be
     running when the cluster stack is running, or even when part of it
     has crashed, so that it can be fenced.
    </para>
 
 <procedure>
 <step>
    <para>
     The &ais; init script starts and stops SBD, if configured. Add the
     following to /etc/sysconfig/sbd:
    </para>
<screen>SBD_DEVICE="/dev/SBD"
# The next line enables the watchdog support:
SBD_OPTS="-W"</screen>
</step>
<step>
    <para>
     If the SBD device is not accessible, the daemon will fail to start and
     inhibit openais startup.
    </para>
    <note>
     <para>
      If the SBD device becomes inaccessible from a node, this could cause
      the node to enter an infinite reboot cycle. That is technically
      correct, but depending on your administrative policies, might be a
      considered a nuisance. You may wish to not automatically start up
      openais on boot in such cases.
     </para>
    </note>
    </step>
    <step>
    <para>
     Before proceeding, ensure that SBD has indeed started on all nodes
     through <command>rcopenais</command> <option>restart</option>.
    </para>
    </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 id="sec.ha.storageprotection.fencing.testing">
   <title>Testing SBD</title>
   <procedure>
   <step>
   <para>
    The following command will dump the node slots and their current
    messages from the SBD device:
   </para>
<screen>sbd -d /dev/SBD list</screen>
   <para>
    Now you should see all cluster nodes that have ever been started with SBD
    listed here. Most likely with the message slot showing
    <literal>clear</literal>.
   </para>
   </step>
<step>
   <para>
    Try sending a test message to one of the nodes:
   </para>
<screen>sbd -d /dev/SBD message nodea test</screen>
</step>
<step>
   <para>
    The node will acknowledge the receipt of the message in the system logs:
   </para>
<screen>Aug 29 14:10:00 nodea sbd: [13412]: info: Received command test from nodeb</screen>
   <para>
    This confirms that SBD is indeed up and running on the node, and that it
    is ready to receive messages.
   </para>
   <sect3 id="sec.ha.storageprotection.fencingresources">
    <title>Configuring the Fencing Resource</title>
    <para>
     To complete the SBD setup, it is necessary to activate SBD as a
     &stonith;/fencing mechanism in the CIB as follows:
    </para>
<screen><command>crm</command> configure
crm(live)configure# <command>property</command> stonith-enabled="true"
crm(live)configure# <command>property</command> stonith-timeout="30s"
crm(live)configure# <command>primitive</command> stonith:external/sbd params sbd_device="/dev/SBD"
crm(live)configure# <command>commit</command>
crm(live)configure# <command>quit</command></screen>
    <para>
     Note that since node slots are allocated automatically, no manual
     hostlist needs to be defined.
    </para>
    <para>
     The SBD mechanism is used instead of other fencing/&stonith;
     mechanisms; please disable any others you might have configured before.
    </para>
    <para>
     Once the resource has started, your cluster is now successfully
     configured for shared-storage fencing, and will utilize this method in
     case a node needs to be fenced.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 id="sec.ha.storageprotection.exstoract">
  <title>Ensuring Exclusive Storage Activation</title>

  <para>
   This section introduces "sfex", an additional low-level mechanism to
   <quote>lock</quote> access to shared storage exclusively to one node. By
   design, this cannot be used in conjunction with workloads that require
   concurrency (such as OCFS2), but serves as a layer of protection for
   classic fail-over style workloads. This is similar to a SCSI-2
   reservation in effect, but more general.
  </para>

  <sect2 id="sec.ha.storageprotection.exstoract.description">
   <title>Description</title>
   <para>
    In a shared storage environment, a small partition of the storage is set
    aside for storing one or more locks.
   </para>
   <para>
    Before acquiring protected resources, the node must first acquire the
    protecting lock; the ordering is enforced by Pacemaker, and the sfex
    component ensures that even if Pacemaker were subject to a split-brain
    situation, the lock will never be granted more than once.
   </para>
   <para>
    These locks must also be refreshed periodically, so that a node's death
    does not permanently block the lock and other nodes can proceed.
   </para>
  </sect2>

  <sect2 id="sec.ha.storageprotection.exstoract.requirements">
   <title>Requirements</title>
   <para>
    Create a shared partition for use with sfex. Note the name of this
    partition and substitute it for <filename>/dev/sfex</filename> below.
   </para>
   <para>
    This partition should ideally be on the same logical unit as the data
    you wish to protect; similarly to the SBD device, this may not use
    host-based RAID or DRBD, though a cLVM2 logical volume could be used.
   </para>
   <para>
    A single sfex partition can hold any number of locks; it defaults to
    one, and needs 1kB of storage space allocated per lock.
   </para>
  </sect2>

  <sect2 id="sec.ha.storageprotection.exstoract.setup">
   <title>Setup</title>
   <para>
    Create the sfex metadata with the following command:
   </para>
<screen>sfex_init -i 1 /dev/sfex</screen>
   <para>
    Verify that the meta-data has been created correctly:
   </para>
<screen>sfex_stats -i 1 /dev/sfex ; echo $?</screen>
   <para>
    This should return <literal>2</literal>, since the lock is not currently
    held.
   </para>
  </sect2>

  <sect2 id="sec.ha.storageprotection.exstoract.sfex2cib">
   <title>Integrate sfex into the CIB</title>
   <para>
    The sfex lock is represented via a resource in the CIB, configured as
    follows:
   </para>
<screen>primitive sfex_1 ocf:heartbeat:sfex \
#	params device="/dev/sfex" index="1" collision_timeout="1" \
      lock_timeout="70" monitor_interval="10" \
#	op monitor interval="10s" timeout="30s" on_fail="fence"</screen>
   <para>
    To protect resources via a sfex lock, create mandatory ordering and
    placement constraints between the protectees and the sfex resource. If
    the resource to be protected has the id <literal>filesystem1</literal>:
   </para>
<screen># order order-sfex-1 inf: sfex_1 filesystem1
# colocation colo-sfex-1 inf: filesystem1 sfex_1</screen>
   <para>
    If using group syntax, add the sfex resource as the first resource to
    the group:
   </para>
<screen># group LAMP sfex_1 filesystem1 apache ipaddr</screen>
  </sect2>

  <sect2 id="sec.ha.storageprotection.exstoract.fencing">
   <title>Interaction with Fencing</title>
   <para>
    "sfex" does not replace &stonith;. In particular since "sfex" requires
    shared storage, it is recommended that the "external/sbd" fencing
    mechanism described above is used on another partition of the storage.
   </para>
  </sect2>
 </sect1>
</chapter>
