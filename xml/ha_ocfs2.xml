<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!--https://fate.novell.com/307371: Read-only GFS2 support - 
        taroth 2010-02-01: mentioned this in ha_news.xml, as it makes no sense
        to mention it here IMHO-->
<chapter id="cha.ha.ocfs2">
 <title>Oracle Cluster File System 2</title><indexterm class="startofrange" id="idx.filesystems.ocfs2">
 <primary>file systems</primary>
 <secondary>OCFS2</secondary></indexterm>
 <abstract>
  <para>
   Oracle Cluster File System 2 (OCFS2) is a general-purpose journaling file
   system that has been fully integrated since the Linux 2.6 kernel. OCFS2
   allows you to store application binary files, data files, and databases
   on devices on shared storage. All nodes in a cluster have concurrent read
   and write access to the file system. A user-space control daemon, managed
   via a clone resource, provides the integration with the HA stack, in
   particular with &ais;/&corosync; and the Distributed Lock Manager (DLM).
  </para>
 </abstract>
 <sect1 id="sec.ha.ocfs2.features">
  <title>Features and Benefits</title>

  <para>
   OCFS2 can be used for the following storage solutions for example:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     General applications and workloads.
    </para>
   </listitem>
   <listitem>
    <para>
     &xen; image store in a cluster. &xen; virtual machines and virtual
     servers can be stored on OCFS2 volumes that are mounted by cluster
     servers. This provides quick and easy portability of &xen; virtual
     machines between servers.
    </para>
   </listitem>
   <listitem>
    <para>
     LAMP (Linux, Apache, MySQL, and PHP &verbar; Perl &verbar; Python)
     stacks.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   As a high-performance, symmetric and parallel cluster file system, OCFS2
   supports the following functions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     An application's files are available to all nodes in the cluster. Users
     simply install it once on an OCFS2 volume in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of
     applications that run across the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it
     contends with the DLM to coordinate file access.
    </para>
   </listitem>
   <listitem>
    <para>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   OCFS2 also provides the following capabilities:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Metadata caching.
    </para>
   </listitem>
   <listitem>
    <para>
     Metadata journaling.
    </para>
   </listitem>
   <listitem>
    <para>
     Cross-node file data consistency.
    </para>
   </listitem>
<!--
    <listitem>
     <para>
      A GTK GUI-based administration via the
      <command>ocfs2console</command> utility
     </para>
    </listitem> -->
<!--
    <listitem>
     <para>
      Operation as a shared-root file system
     </para>
    </listitem> -->
   <listitem>
    <para>
     Support for multiple-block sizes up to 4 KB, cluster sizes up to 1 MB,
     for a maximum volume size of 4 PB (Petabyte).
    </para>
   </listitem>
   <listitem>
    <para>
     Support for up to 16 cluster nodes.
    </para>
   </listitem>
<!-- taroth 2010-03-12: ocfs2cdsl not on our media - 
         not supported according to coly-->
<!--      
    <listitem>
     <para>
      Context-dependent symbolic link (CDSL) support for node-specific
      local files
     </para>
    </listitem> -->
   <listitem>
    <para>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.ocfs2.utils">
  <title>OCFS2 Packages and Management Utilities</title>

  <para>
   The OCFS2 kernel module (<literal>ocfs2</literal>) is installed
   automatically in the &hasi; on &slsreg; &productnumber;. To use OCFS2,
   make sure the following packages are installed on each node in the
   cluster: <systemitem class="resource">ocfs2-tools</systemitem> and the
   matching <systemitem class="resource">ocfs2-kmp-*</systemitem> packages
   for your kernel.
  </para>

  <para>
   The <systemitem class="resource">ocfs2-tools</systemitem> package
   provides the following utilities for management of OFS2 volumes. For
   syntax information, see their man pages.
  </para>

  <table>
   <title>OCFS2 Utilities</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        OCFS2 Utility
       </para>
      </entry>
      <entry>
       <para>
        Description
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        debugfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Examines the state of the OCFS file system for the purpose of
        debugging.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        fsck.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Checks the file system for errors and optionally repairs errors.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mkfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Creates an OCFS2 file system on a device, usually a partition on a
        shared physical or logical disk.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mounted.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Detects and lists all OCFS2 volumes on a clustered system. Detects
        and lists all nodes on the system that have mounted an OCFS2 device
        or lists all OCFS2 devices.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        tunefs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Changes OCFS2 file system parameters, including the volume label,
        number of node slots, journal size for all node slots, and volume
        size.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect1>
 <sect1 id="sec.ha.ocfs2.create.service">
  <title>Configuring OCFS2 Services</title>

<!--https://bugzilla.novell.com/show_bug.cgi?id=520714-->

  <para> Before you can create OCFS2 volumes, you must configure the following
   resources as services in the cluster: DLM, O2CB and a &stonith; resource.
   OCFS2 uses the cluster membership services from Pacemaker which run in user
   space. Therefore, DLM and O2CB need to be configured as clone resources that
   are present on each node in the cluster.</para>
  
  <note>
   <title>&stonith; Device Needed</title>
   <para>You need to configure a fencing device. Without a
    &stonith; mechanism (like <literal>external/sbd</literal>) in place the
    configuration will fail.</para>
  </note>
  
  <note>
   <title>DLM Resource for Both cLVM and OCFS2</title>
   <para>Both cLVM and OCFS2 need to have a DLM resource that runs on all
    nodes in the cluster and is therefore usually configured as a clone
    resource. If you have a setup that includes both OCFS2 and cLVM, you have
    probably already configured a DLM resource as described in 
    <xref linkend="pro.ha.clvm.scenarios.iscsi.dlmresource"/>. One DLM resource for both OCFS2 and
    cLVM is enough&mdash;you do not need to create a second DLM resource
    in that case. </para>
  </note>

  <procedure id="proc.ocfs2.resources">
   <title>Configuring DLM and O2CB Resources</title>
   <para>The following procedure uses the <command>crm</command> shell to
    configure the cluster resources. The configuration consists of a base group
    and a base clone that can be used in different scenarios afterwards (for
    both OCFS2 and cLVM, for example)&mdash;you only need to extended them
    with the respective contents, attributes, constraints or monitoring
    operations as needed. This facilities the overall setup as you do not have
    to specify lots of individual groups, clones and their dependencies.</para>
   <para>Follow the steps below for one node in the cluster. Alternatively, you
   can also use the &hbgui; to configure the resources.</para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm <option>configure</option></command>.
    </para>
   </step>
   <step>
    <para>
     Enter the following to create the primitive resources for DLM and O2CB,
     a <literal>base-group</literal> with a monitoring operation and a
     <literal>base-clone</literal>:
     <remark>taroth 2010-08-18: DEVs, is the following a useful configuration
      and the correct way to do it: first define the primitives, then a group
      that holds both primitives (and that can be extended for clvm) and then
      finally clone the group? (as I need to clone both dlm and o2cb but clones
      must only contain one regular resource or a group) - what about the
      monitoring operation and the meta attribute? did I add them to the correct
     items below?</remark>
    </para>
<screen><command>primitive</command> dlm ocf:pacemaker:controld
<command>primitive</command> o2cb ocf:ocfs2:o2cb
<command>group</command> base-group dlm o2cb op monitor interval=120s
<command>clone</command> base-clone base-group \
      meta globally-unique=false interleave=true</screen>
   </step>
   <step>
    <para>
     To make sure that the O2CB service is only started on nodes that also
     have a copy of the dlm service already running, add a collocational
     constraint:<remark>taroth 2010-08-18: DEVs, here I definitely need your
      help as I have no idea how to modify this to make it match the new
      construct of base group and base-clone above...</remark>
    </para>
    <screen>colocation o2cb-with-dlm INFINITY: o2cb-clone dlm-clone
     order start-o2cb-after-dlm mandatory: dlm-clone o2cb-clone
     </screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, enter <command>commit</command> and leave
     <command>crm</command> with <command>exit</command>.
    </para>
   </step>
   <!--<step>
    <para>
     Open a terminal window and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     To add the DLM (Distributed Lock Manager) as a resource:
    </para>
    <substeps>
     <step>
      <para>
       Start the crm shell and create a new configuration from scratch:
      </para>
<screen>crm
cib new stack-glue</screen>
     </step>
     <step>
      <para>
       Create the DLM service and make it run on all machines in the
       cluster:
      </para>
<screen>configure
primitive dlm ocf:pacemaker:controld op monitor interval=120s
clone dlm-clone dlm meta globally-unique=false interleave=true
end</screen>
      <para>
       The <literal>dlm</literal> clone resource controls the distributed
       lock manager service, and makes sure this service is started on all
       nodes in the cluster.
      </para>
     </step>
     <step>
      <para>
       Verify the changes you made before committing them to the CIB:
      </para>
<screen>cib diff
configure verify</screen>
     </step>
     <step>
      <para>
       Upload the configuration to the cluster and exit the shell:
      </para>
<screen>cib commit stack-glue
quit</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To add the O2CB configuration:
    </para>
    <substeps>
     <step>
      <para>
       Start the crm shell and create a new configuration from scratch:
      </para>
<screen>crm
cib new oracle-glue</screen>
     </step>
     <step>
      <para>
       Make the O2CB service start on every node in the cluster:
      </para>
<screen>configure
primitive o2cb ocf:ocfs2:o2cb op monitor interval=120s
clone o2cb-clone o2cb meta globally-unique=false interleave=true</screen>
     </step>
     <step>
      <para>
       To make sure that the O2CB service is only started on nodes that also
       have a copy of the dlm service already running, add a collocational
       constraint:
      </para>
<screen>colocation o2cb-with-dlm INFINITY: o2cb-clone dlm-clone
order start-o2cb-after-dlm mandatory: dlm-clone o2cb-clone
end
</screen>
     </step>
     <step>
      <para>
       Upload the configuration to the cluster and exit the shell:
      </para>
<screen>cib commit oracle-glue
quit</screen>
     </step>
    </substeps>
   </step>-->
   <step>
      
    <para>
     To configure a fencing device:
    </para>
    <substeps>
     <step>
      <para>
       Start the crm shell and create a new configuration from scratch:
      </para>
<screen>crm
cib new fencing</screen>
     </step>
     <step>
      <para>
       Configure <literal>external/sdb</literal> as fencing device with
       <literal>/dev/sdb2</literal> being a dedicated partition on the
       shared storage for heartbeating and fencing:
      </para>
<screen>configure
primitive sbd_stonith stonith:external/sbd \
meta target-role="Started"op monitor \
interval=15 timeout=15 start-delay=15 \
params sbd_device=/dev/sdb2
end
</screen>
     </step>
     <step>
      <para>
       Upload the configuration to the cluster and exit the shell:
      </para>
<screen>cib commit fencing
quit</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.ocfs2.create">
  <title>Creating OCFS2 Volumes</title>

  <para>
   After you have configured DLM and O2CB as cluster resources as described
   in <xref linkend="sec.ha.ocfs2.create.service"/>, configure your system
   to use OCFS2 and create OCFs2 volumes.
  </para>

  <note>
   <title>OCFS2 Volumes for Application and Data Files</title>
   <para>
    We recommend that you generally store application files and data files
    on different OCFS2 volumes. If your application volumes and data volumes
    have different requirements for mounting, it is mandatory to store them
    on different volumes.
   </para>
  </note>

  <para>
   Before you begin, prepare the block devices you plan to use for your
   OCFS2 volumes. Leave the devices as free space.
  </para>

  <para>
   Then create and format the OCFS2 volume with the
   <command>mkfs.ocfs2</command> as described in
   <xref linkend="proc.ocfs2.volume"/>. The most important parameters for
   the command are listed in
   <xref linkend="tab.ha.ofcs2.mkfs.ocfs2.params"/>. For more information
   and the command syntax, refer to the <command>mkfs.ocfs2</command> man
   page.
  </para>

  <table id="tab.ha.ofcs2.mkfs.ocfs2.params">
   <title>Important OCFS2 Parameters</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        OCFS2 Parameter
       </para>
      </entry>
      <entry>
       <para>
        Description and Recommendation
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Volume Label (<option>-L</option>)
       </para>
      </entry>
      <entry>
       <para>
        A descriptive name for the volume to make it uniquely identifiable
        when it is mounted on different nodes. Use the
        <command>tunefs.ocfs2</command> utility to modify the label as
        needed.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Cluster Size (<option>-C</option>)
       </para>
      </entry>
      <entry>
       <para>
        Cluster size is the smallest unit of space allocated to a file to
        hold the data. For the available options and recommendations, refer
        to the <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Number of Node Slots (<option>-N</option>)
       </para>
      </entry>
      <entry>
       <para>
        The maximum number of nodes that can concurrently mount a volume.
        For each of the nodes, OCFS2 creates separate system files, such as
        the journals, for each of the nodes. Nodes that access the volume
        can be a combination of little-endian architectures (such as x86,
        x86-64, and ia64) and big-endian architectures (such as ppc64 and
        s390x).
       </para>
       <para>
        Node-specific files are referred to as local files. A node slot
        number is appended to the local file. For example:
        <literal>journal:0000</literal> belongs to whatever node is assigned
        to slot number <literal>0</literal>.
       </para>
       <para>
        Set each volume's maximum number of node slots when you create it,
        according to how many nodes that you expect to concurrently mount
        the volume. Use the <command>tunefs.ocfs2</command> utility to
        increase the number of node slots as needed. Note that the value
        cannot be decreased.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Block Size (<option>-b</option>)
       </para>
      </entry>
      <entry>
       <para>
        The smallest unit of space addressable by the file system. Specify
        the block size when you create the volume. For the available options
        and recommendations, refer to the <command>mkfs.ocfs2</command> man
        page.
       </para>
      </entry>
     </row>
<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c11-->
     <row>
      <entry>
       <para>
        Specific Features On/Off (<option>--fs-features</option>)
       </para>
      </entry>
      <entry>
       <para>
        A comma separated list of feature flags can be provided, and
        <systemitem>mkfs.ocfs2</systemitem> will try to create the file
        system with those features set according to the list. To turn a
        feature on, include it in the list. To turn a feature off, prepend
        <literal>no</literal> to the name.
       </para>
       <para>
        For on overview of all available flags, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Pre-Defined Features (<option>--fs-feature-level</option>)
       </para>
      </entry>
      <entry>
       <para>
        Allows you to choose from a set of pre-determined file system
        features. For the available options, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>

<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c15-->

  <para>
   If you do not specify any specific features when creating and formatting
   the volume with <command>mkfs.ocfs2</command>, the following features are
   enabled by default: <option>backup-super</option>,
   <option>sparse</option>, <option>inline-data</option>,
   <option>unwritten</option>, <option>metaecc</option>,
   <option>indexed-dirs</option>, and <option>xattr</option>.
  </para>

<!--<para>The most important flags are:</para>
   <itemizedlist>
   <listitem>
   <para><option>local</option>: Create the file system as a local mount, so
   that it can be mounted without a cluster stack.</para>
   </listitem>
   <listitem>
   <para><option>sparse</option>: Enable support for sparse files. With
   this, OCFS2 can avoid allocating (and zeroing) data to fill holes. Turn
   this feature on if you can, otherwise extends and some writes might be
   less performant.</para>
   </listitem>
   <listitem>
   <para>
   <option>back-super</option>: By default,
   <systemitem>mkfs.ocfs2</systemitem> makes up to 6 backup copies of the
   super block at offsets 1G, 4G, 16G, 64G, 256G and 1T depending on the
   size of the volume. This can be useful in disaster recovery. This
   feature is fully compatible with all versions of the file system and
   generally should not be disabled. </para>
   </listitem>
   <listitem>
   <para><option>unwritten</option>: Enable unwritten extents support. With
   this turned on, an application can request that a range of clusters be
   pre-allocated within a file. OCFS2 will mark those extents with a
   special flag so that expensive data zeroing does not have to be
   performed. Reads and writes to a pre-allocated region act as reads and
   writes to a hole, except a write will not fail due to lack of data
   allocation. This feature requires <option>sparse</option> file support
   to be turned on. </para>
   </listitem>
   <listitem>
   <para><option>extended-slotmap</option>: The slot-map is a hidden file on
   an OCFS2 file system which is used to map mounted nodes to system file
   resources. The extended slot map allows a larger range of possible node
   numbers, which is useful for userspace cluster stacks. This feature is
   automatically turned on when needed.</para>
   </listitem>
   <listitem>
   <para><option>inline-data</option>: Enable inline-data support. If this
   feature is turned on, OCFS2 will store small files and directories
   inside the inode block. Data is transparently moved out to an extent
   when it no longer fits inside the inode block. In some cases, this can
   also make a positive impact on cold cache directory and file operations.
   </para>
   </listitem>
   <listitem>
   <para>
   <option>metaecc</option>: Enables metadata checksums. With this enabled,
   the file system computes and stores the checksums in all metadata
   blocks. It also computes and stores an error correction code capable of
   fixing single bit errors. </para>
   </listitem>
   <listitem>
   <para><option>xattr</option>: Enable extended attributes support. With
   this enabled, users can attach <literal>name:value</literal> pairs to
   objects within the file system. In OCFS2, the names can be up to 255
   bytes in length, terminated by the first NULL byte. While it is not
   required, printable names (ASCII) are recommended. The values can be up
   to 64KB of arbitrary binary data. Attributes can be attached to all
   types of inodes: regular files, directories, symbolic links, device
   nodes, etc. This feature is required for users wanting to use extended
   security facilities like POSIX ACLs or SELinux. </para>
   </listitem>
   
   <listitem>
   <para><option>indexed-dirs</option>: Enable directory indexing support.
   With this feature enabled, the file system creates indexed tree for
   non-inline directory entries. For large scale directories, directory
   entry lookup performance from the indexed tree is faster then from the
   legacy directory blocks.</para>
   </listitem>
   <listitem>
   <para><option>usrquota</option>: Enable user quota support. With this
   feature enabled, the file system will track amount of space and number of
   inodes (files, directories, symbolic links) each user owns. It is then
   possible to limit the maximum amount of space or inodes a user can have.
   See a documentation of the quota-tools package for more details. </para>
   </listitem>
   <listitem>
   <para><option>grpquota</option>: Enable group quota support. With this
   feature enabled, file system will track amount of space and number of
   inodes (files, directories, symbolic links) each group owns. It is then
   possible to limit the maximum amount of space or inodes a group can
   have. See a documentation of the quota-tools package for more details.
   </para>
   </listitem>
   <listitem>
   <para><option>refcount</option>: Enables creation of reference counted trees. With this
   enabled, the file system allows users to create inode-based snapshots
   and clones known as reflinks. </para>
   </listitem>-->

  <procedure id="proc.ocfs2.volume">
   <title>Creating and Formatting an OCFS2 Volume</title>
   <para>
    Execute the following steps only on <emphasis>one</emphasis> of the
    cluster nodes.
   </para>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command
     <command>crm_mon</command>.
    </para>
   </step>
   <step>
    <para>
     Create and format the volume using the <command>mkfs.ocfs2</command>
     utility. For information about the syntax for this command, refer to
     the <command>mkfs.ocfs2</command> man page.
    </para>
    <para>
     For example, to create a new OCFS2 file system on
     <filename>/dev/sdb1</filename> that supports up to 16 cluster nodes,
     use the following command:
    </para>
<screen>mkfs.ocfs2 -N 16 /dev/sdb1</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.ocfs2.mount">
  <title>Mounting OCFS2 Volumes</title>

  <para>
   You can either mount an OCFS2 volume manually or with the cluster
   manager, as described in <xref linkend="proc.ocfs2.mount.cluster"/>.
  </para>

  <procedure id="proc.ocfs2.mount.manual">
   <title>Manually Mounting an OCFS2 Volume</title>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command
     <command>crm_mon</command>.
    </para>
   </step>
   <step>
    <para>
     Mount the volume from the command line, using the
     <command>mount</command> command.
    </para>
   </step>
  </procedure>

  <warning>
   <title>Manually Mounted OCFS2 Devices</title>
   <para>
    If you mount the OCFS2 file system manually for testing purposes, make
    sure to unmount it again before starting to use it by means of &ais;.
   </para>
  </warning>

  <procedure id="proc.ocfs2.mount.cluster">
   <title>Mounting an OCFS2 Volume with the Cluster Manager</title>
   <para>
    To mount an OCFS2 volume with the &ha; software, configure an ocf
    <systemitem class="resource">File System</systemitem> resource in the
    cluster. The following procedure uses the <command>crm</command> shell
    to configure the cluster resources. Alternatively, you can also use the
    &hb; to configure the resources.<remark>taroth 2010-08-18: DEVs, how to
     change the following steps to make them match the new setup in procedure
     12.1 (Configuring DLM and O2CB Resources)???</remark>
   </para>
   <step>
    <para>
     Start the crm shell and create a new configuration from scratch:
    </para>
<screen>crm
cib new filesystem
        </screen>
   </step>
   <step>
    <para>
     Configure Pacemaker to mount the OCFS2 file system on every node in the
     cluster:
    </para>
<screen>configure
primitive fs ocf:heartbeat:Filesystem \
   params device="/dev/sdb1" directory="/mnt/shared" fstype="ocfs2" \
   op monitor interval=120s
clone fs-clone fs meta interleave="true" ordered="true"
        </screen>
   </step>
   <step>
    <para>
     To make sure that Pacemaker only starts the
     <systemitem
      class="resource">fs</systemitem> clone resource on
     nodes that also have a clone of the o2cb resource already running, add
     a collocational constraint:
    </para>
<screen>colocation fs-with-o2cb INFINITY: fs-clone o2cb-clone
order start-fs-after-o2cb mandatory: o2cb-clone fs-clone
end</screen>
   </step>
   <step>
    <para>
     Upload the configuration to the CIB and exit the shell:
    </para>
<screen>cib commit filesystem
quit</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.ocfs2.more">
  <title>For More Information</title>

  <para>
   For more information about OCFS2, see the following links:
  </para>

  <variablelist>
   <varlistentry>
    <term><ulink url="http://oss.oracle.com/projects/ocfs2/"/>
    </term>
    <listitem>
     <para>
      OCFS2 project home page at Oracle.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><ulink url="http://oss.oracle.com/projects/ocfs2/documentation"/>
    </term>
    <listitem>
     <para>
      <citetitle>OCFS2 User's Guide</citetitle>, available from the project
      documentation home page.
     </para>
    </listitem>
   </varlistentry>
  </variablelist><indexterm class="endofrange" startref="idx.filesystems.ocfs2"/>
 </sect1>
</chapter>
<!--
 Date: Tue, 17 Mar 2009 20:42:16 +0100
 From: Lars Marowsky-Bree 
 Subject: Re: [Clusters] HOWTO get started with SLES11 HAE?
 ...
 You can now run crm_mon to watch the cluster state, or use "crm" to
 configure it - the "crm" shell has an online help. For example:
 
 # crm configure edit <<END
 primitive fencing stonith:external/ssh \
 params hostlist="node1 node2 ..."
 primitive dlm ocf:pacemaker:controld
 primitive o2cb ocf:ocfs2:o2cb
 clone dlm-clone dlm \
 meta interleave="true" ordered="true"
 clone o2cb-clone o2cb \
 meta interleave="true" ordered="true"
 colocation dlm-o2cb-colo inf: o2cb-clone dlm-clone
 order dlm-o2cb-order inf: dlm-clone o2cb-clone
 verify
 ptest
 commit
 END
 
 And then you're ready to run mkfs.ocfs2 and mount the filesystem, which
 you can also do using the CLI or the commandline of course. No need to
 configure /etc/ocfs2/*.
 
 Once you did that, you can also add it to the cluster configuration
 (shown in shell syntax, not XML):
 
 primitive ocfs2-1 ocf:heartbeat:Filesystem \
 params device="/dev/drbd0" directory="/mnt" fstype="ocfs2" \
 operations $id="ocfs2-1-operations" \
 op monitor interval="20" timeout="40" start-delay="10"
 clone c-ocfs2-1 ocfs2-1 \
 meta target-role="Started" interleave="true" ordered="true"
 colocation ocfs2-colo-1 inf: c-ocfs2-1 o2cb-clone
 order ocfs2-o2cb-order inf: o2cb-clone c-ocfs2-1
 
 If you want to use the GUI, you need to set a passwd for the "hacluster"
 user and then you can use hb_gui to connect to the server.
 
 
 And be careful to not install ocfs2-tools-o2cb; it's not installed by
 default, and you don't want it for a pacemaker cluster.
-->
