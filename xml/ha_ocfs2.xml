<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<!--https://bugzilla.novell.com/show_bug.cgi?id=520714:
 In SLE HAE, OCFS2 uses the cluster membership services from Pacemaker. As these
 run in user space, ocfs2 has to be configured on each cluster node to tap into
 them. This is what the clone o2cb cluster resource is doing. The DLM clone
 resource controls the distributed lock manager service, and makes sure this
 service is started on all nodes in the cluster.-->
<!--taroth 2010-02-01: https://fate.novell.com/307371: 
       Read-only GFS2 support
       LMB: call mount-->
<chapter id="cha.ha.ocfs2">
 <title>Oracle Cluster File System 2</title><indexterm class="startofrange" id="idx.filesystems.ocfs2">
 <primary>file systems</primary>
 <secondary>OCFS2</secondary></indexterm>
 <abstract>
  <para>Oracle Cluster File System 2 (OCFS2) is a general-purpose journaling
   file system that has been fully integrated since the Linux 2.6 kernel. OCFS2
   allows you to store application binary files, data files, and databases on
   devices on shared storage. All nodes in a cluster have concurrent read and
   write access to the file system. A user-space control daemon, managed via a
   clone resource, provides the integration with the HA stack, in particular
   with &ais;/&corosync; and the Distributed Lock Manager (DLM). </para>
 </abstract>
    
 <sect1 id="sec.ha.ocfs2.features">
  <title>Features and Benefits</title>

  <para>OCFS2 can be used for the following storage solutions for example:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     General applications and workloads.
    </para>
   </listitem>
   <listitem>
    <para>&xen; image store in a cluster. &xen; virtual machines and virtual
     servers can be stored on OCFS2 volumes that are mounted by cluster servers.
     This provides quick and easy portability of &xen; virtual machines between
     servers. </para>
   </listitem>
   <listitem>
    <para>
     LAMP (Linux, Apache, MySQL, and PHP &verbar; Perl &verbar; Python)
     stacks.
    </para>
   </listitem>
  </itemizedlist>

    <para>
   As a high-performance, symmetric and parallel cluster file system, OCFS2
   supports the following functions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     An application's files are available to all nodes in the cluster.
     Users simply install it once on an OCFS2 volume in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of
     applications that run across the cluster.
    </para>
   </listitem>
   <listitem>
    <para> File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it
     contends with the DLM to coordinate file access. </para>
   </listitem>
   <listitem>
    <para>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   OCFS2 also provides the following capabilities:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Metadata caching.
    </para>
   </listitem>
   <listitem>
    <para>
     Metadata journaling.
    </para>
   </listitem>
   <listitem>
    <para>
     Cross-node file data consistency.
    </para>
   </listitem>
<!--
    <listitem>
     <para>
      A GTK GUI-based administration via the
      <command>ocfs2console</command> utility
     </para>
    </listitem> -->
<!--
    <listitem>
     <para>
      Operation as a shared-root file system
     </para>
    </listitem> -->
   <listitem>
    <para>
     Support for multiple-block sizes (each volume can have a different
     block size) up to 4 KB, for a maximum volume size of 16 TB.
    </para>
   </listitem>
   <listitem>
    <para>
     Support for up to 16 cluster nodes.
    </para>
       </listitem>
<!-- taroth 2010-03-12: ocfs2cdsl not on our media - 
         not supported according to coly-->
  <!--      
    <listitem>
     <para>
      Context-dependent symbolic link (CDSL) support for node-specific
      local files
     </para>
    </listitem> -->
   <listitem>
    <para>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 

 <sect1 id="sec.ha.ocfs2.utils">
  <title>OCFS2 Packages and Management Utilities</title>

  <para>The OCFS2 kernel module (<literal>ocfs2</literal>) is installed
   automatically in the &hasi; on &slsreg; &productnumber;. To use
   OCFS2, make sure the following packages are installed on each node in the
   cluster: <systemitem class="resource">ocfs2-tools</systemitem> and the
   matching <systemitem class="resource">ocfs2-kmp-*</systemitem> packages for
   your kernel. </para>

  <para>The <systemitem class="resource">ocfs2-tools</systemitem> package
   provides the following utilities for management of OFS2 volumes. For syntax
   information, see their man pages. </para>

  <table>
   <title>OCFS2 Utilities</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        OCFS2 Utility
       </para>
      </entry>
      <entry>
       <para>
        Description
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        debugfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Examines the state of the OCFS file system for the purpose of
        debugging.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        fsck.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Checks the file system for errors and optionally repairs errors.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mkfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Creates an OCFS2 file system on a device, usually a partition on a
        shared physical or logical disk.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mounted.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Detects and lists all OCFS2 volumes on a clustered system. Detects
        and lists all nodes on the system that have mounted an OCFS2 device
        or lists all OCFS2 devices.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        tunefs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Changes OCFS2 file system parameters, including the volume label,
        number of node slots, journal size for all node slots, and volume
        size.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect1>
 
 <sect1 id="sec.ha.ocfs2.create.service">
  <title>Configuring OCFS2 Services</title>
  <para>
   Before you can create OCFS2 volumes, you must configure the following 
   resources as services in the cluster: DLM and O2CB.
  </para>
 
  <procedure id="proc.ocfs2.resources">
   <title>Configuring DLM and O2CB Resources</title>
   <para>The following procedure uses the <command>crm</command> shell to
    configure the cluster resources. Alternatively, you can also configure the
    resources with the &hbgui;. Follow the steps below for one node in the
    cluster. After you have committed your changes to the CIB, the changes are
    automatically synchronized to all cluster nodes.</para>
   <step>
    <para>Open a terminal window and log in as the
      &rootuser; user or equivalent. </para>
   </step>
   <step>
    <para>To add the DLM (Distributed Lock Manager) as a resource: </para>
    <substeps>
     <step>
      <para> Start the crm shell and create a new scratch configuration: </para>
<screen>crm
cib new stack-glue</screen>
     </step>
     <step>
      <para>Create the DLM service and make it run on all machines in the
       cluster: </para>
<screen>configure
primitive dlm ocf:pacemaker:controld op monitor interval=120s
clone dlm-clone dlm meta globally-unique=false interleave=true
end</screen>
     </step>
     <step>
      <para> Verify the changes you made before committing them to the CIB: </para>
<screen>cib diff
configure verify</screen>
     </step>
     <step>
      <para> Upload the configuration to the cluster and exit the shell: </para>
<screen>cib commit stack-glue
quit</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>To add the O2CB configuration: </para>
    <substeps>
     <step>
      <para> Start the crm shell and create a new scratch configuration: </para>
<screen>crm
cib new oracle-glue</screen>
     </step>
     <step>
      <para>Make the O2CB service start on every node in the
       cluster:</para>
<screen>configure
primitive o2cb ocf:ocfs2:o2cb op monitor interval=120s
clone o2cb-clone o2cb meta globally-unique=false interleave=true</screen>
     </step>
     <step>
      <para>To make sure that the O2CB service is only started on nodes that
       also have a copy of the dlm service already running, add a collocational
       constraint: </para>
<screen>colocation o2cb-with-dlm INFINITY: o2cb-clone dlm-clone
order start-o2cb-after-dlm mandatory: dlm-clone o2cb-clone
end</screen>
     </step>
     <step>
      <para> Upload the configuration to the cluster and exit the shell: </para>
<screen>cib commit oracle-glue
quit</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 
 <sect1 id="sec.ha.ocfs2.create">
  <title>Creating OCFS2 Volumes</title>

  <para>After you have configured DLM and O2CB as cluster resources as described
   in <xref linkend="sec.ha.ocfs2.create.service"/>, configure your system to
   use OCFS2 and create OCFs2 volumes. </para>

  <note>
   <title>OCFS2 Volumes for Application and Data Files</title>
   <para>We recommend that you generally store application files and data files
    on different OCFS2 volumes. If your application volumes and data volumes
    have different requirements for mounting, it is mandatory to store them on
    different volumes.</para>
  </note>

  <para>Before you begin, prepare the block devices you plan to use for your
   OCFS2 volumes. Leave the devices as free space. </para>

  <procedure id="proc.ocfs2.volume">
   <title>Creating and Formatting an OCFS2 Volume</title>
   <para>Execute the following steps only on <emphasis>one</emphasis> of the
    cluster nodes.</para>
   <step>
    <para> Open a terminal window and log in as &rootuser;. </para>
   </step>
   <step>
    <para> Check if the cluster is online with the command
     <command>crm_mon</command>. </para>
   </step>
   <step>
    <para> Create and format the volume using the <command>mkfs.ocfs2</command>
     utility. For information about the syntax for this command, refer to the
     <command>mkfs.ocfs2</command> man page. </para>
    <para>For example, to create a new OCFS2 file system on
      <filename>/dev/sdb1</filename> that supports up to 16 cluster nodes, use
     the following command:</para>
    <screen>mkfs.ocfs2 -N 16 /dev/sdb1</screen>
    <para>The following list shows the OCFS2 parameters and the recommended
     settings:</para>
    <variablelist>
     <varlistentry>
      <term>Volume Label </term>
      <listitem>
       <para> A descriptive name for the volume to make it uniquely identifiable
        when it is mounted on different nodes. </para>
       <para> Use the <command>tunefs.ocfs2</command> utility to modify the
        label as needed. </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Cluster Size </term>
      <listitem>
       <para> Cluster size is the smallest unit of space allocated to a file to
        hold the data. </para>
       <para> Options are 4, 8, 16, 32, 64, 128, 256, 512, and 1024 KB. Cluster
        size cannot be modified after the volume is formatted. </para>
       <para> Oracle recommends a cluster size of 128 KB or larger for database
        volumes. Oracle also recommends a cluster size of 32 or 64 KB for Oracle
        Home. </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term> Number of Node Slots </term>
      <listitem>
       <para> The maximum number of nodes that can concurrently mount a volume.
        For each of the nodes, OCFS2 creates separate system files, such as the
        journals, for each of the nodes. Nodes that access the volume can be a
        combination of little-endian architectures (such as x86, x86-64, and
        ia64) and big-endian architectures (such as ppc64 and s390x). </para>
       <para> Node-specific files are referred to as local files. A node slot
        number is appended to the local file. For example: journal:0000 belongs
        to whatever node is assigned to slot number 0. </para>
       <para> Set each volume&rsquo;s maximum number of node slots when you
        create it, according to how many nodes that you expect to concurrently
        mount the volume. Use the <command>tunefs.ocfs2</command> utility to
        increase the number of node slots as needed; the value cannot be
        decreased. </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Block Size</term>
      <listitem>
       <para> The smallest unit of space addressable by the file system. Specify
        the block size when you create the volume. </para>
       <para> Options are 512 bytes (not recommended), 1 KB, 2 KB, or 4 KB
        (recommended for most volumes). Block size cannot be modified after the
        volume is formatted. </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </step>
  </procedure>

 </sect1>
 <sect1 id="sec.ha.ocfs2.mount">
  <title>Mounting OCFS2 Volumes</title>

  <para>You can either mount an OCFS2 volume manually or with the cluster
   manager, as described in <xref linkend="proc.ocfs2.mount.cluster"/>.</para>
  <procedure id="proc.ocfs2.mount.manual">
   <title>Manually Mounting an OCFS2 Volume</title>
   <step>
    <para>Open a terminal window and log in as &rootuser;. </para>
   </step>
   <step>
    <para>Check if the cluster is online with the command
     <command>crm_mon</command>. </para>
   </step>
   <step>
    <para> Mount the volume from the command line, using the
     <command>mount</command> command. </para>
   </step>
  </procedure>
  <warning>
   <title>Manually Mounted OCFS2 Devices</title>
   <para> If you mount the ocfs2 file system manually for testing purposes,
    you are required to umount the file system again before starting to use it
    by means of &ais;. </para>
  </warning>

   

   <para>The ocf resource
    <systemitem>Filesystem</systemitem> can be used for this task. 
   To configure the file system resource, use the following procedure:
  </para>

  <procedure id="proc.ocfs2.mount.cluster">
   <title>Mounting the File system with the Cluster Manager</title>
   <step>
    <para>
     Start the crm shell and create a new scratch conﬁguration:
    </para>
<screen>crm
cib new filesystem
        </screen>
   </step>
   <step>
    <para>
     Conﬁgure Pacemaker to mount the file system on every node in the
     cluster.
    </para>
<screen>configure
primitive fs ocf:heartbeat:Filesystem \
   params device="/dev/sdb1" directory="/mnt/shared" fstype="ocfs2" \
   op monitor interval=120s
clone fs-clone fs meta interleave="true" ordered="true"
        </screen>
   </step>
   <step>
    <para>
     Make sure that Pacemaker only starts the fs clone resource on nodes
     that also have a clone of the o2cb resource already running:
    </para>
<screen>colocation fs-with-o2cb INFINITY: fs-clone o2cb-clone
order start-fs-after-o2cb mandatory: o2cb-clone fs-clone
end
        </screen>
   </step>
   <step>
    <para>
     Upload the configuration to the cluster and exit the shell:
    </para>
<screen>cib commit filesystem
quit
        </screen>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.ocfs2.more">
  <title>For More Information</title>

  <para>
   For information about using OCFS2, see the
   <citetitle><ulink
     url="http://oss.oracle.com/projects/ocfs2/documentation/">OCFS2
   User Guide</ulink> </citetitle> on the
   <ulink url="http://oss.oracle.com/projects/ocfs2/">OCFS2 project at
   Oracle</ulink>.
  </para><indexterm class="endofrange" startref="idx.filesystems.ocfs2"/>
 </sect1>
</chapter>
<!--
 Date: Tue, 17 Mar 2009 20:42:16 +0100
 From: Lars Marowsky-Bree 
 Subject: Re: [Clusters] HOWTO get started with SLES11 HAE?
 ...
 You can now run crm_mon to watch the cluster state, or use "crm" to
 configure it - the "crm" shell has an online help. For example:
 
 # crm configure edit <<END
 primitive fencing stonith:external/ssh \
 params hostlist="node1 node2 ..."
 primitive dlm ocf:pacemaker:controld
 primitive o2cb ocf:ocfs2:o2cb
 clone dlm-clone dlm \
 meta interleave="true" ordered="true"
 clone o2cb-clone o2cb \
 meta interleave="true" ordered="true"
 colocation dlm-o2cb-colo inf: o2cb-clone dlm-clone
 order dlm-o2cb-order inf: dlm-clone o2cb-clone
 verify
 ptest
 commit
 END
 
 And then you're ready to run mkfs.ocfs2 and mount the filesystem, which
 you can also do using the CLI or the commandline of course. No need to
 configure /etc/ocfs2/*.
 
 Once you did that, you can also add it to the cluster configuration
 (shown in shell syntax, not XML):
 
 primitive ocfs2-1 ocf:heartbeat:Filesystem \
 params device="/dev/drbd0" directory="/mnt" fstype="ocfs2" \
 operations $id="ocfs2-1-operations" \
 op monitor interval="20" timeout="40" start-delay="10"
 clone c-ocfs2-1 ocfs2-1 \
 meta target-role="Started" interleave="true" ordered="true"
 colocation ocfs2-colo-1 inf: c-ocfs2-1 o2cb-clone
 order ocfs2-o2cb-order inf: o2cb-clone c-ocfs2-1
 
 If you want to use the GUI, you need to set a passwd for the "hacluster"
 user and then you can use hb_gui to connect to the server.
 
 
 And be careful to not install ocfs2-tools-o2cb; it's not installed by
 default, and you don't want it for a pacemaker cluster.
-->
