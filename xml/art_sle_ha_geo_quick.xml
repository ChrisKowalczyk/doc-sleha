<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml" 
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--taroth 2014-12-18: todo for next release: fix IDs in xincludes-->
<?provo dirname="geo_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art.ha.geo.quick"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&geoquick;</title>
 <subtitle>&productname; &productnumber;</subtitle>
 <info>
  <productnumber>&productnumber;</productnumber>
   <productname>&productname;</productname>
   <date><?dbtimestamp?></date>
    <abstract>
    <para>
     &abstract-geoquick;
     This document guides you through the basic setup of a &geo; cluster,
     using the &geo; cluster bootstrap scripts provided by the
      <systemitem xmlns='http://docbook.org/ns/docbook'
        class='resource'>ha-cluster-bootstrap</systemitem> package.
      </para>
      <para>
        <remark>
          taroth 2017-06-22: todo - more details? restrictions?
        </remark>
      </para>
      </abstract>
      <dm:docmanager>
        <dm:bugtracker>
          <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
          <dm:product>SUSE Linux Enterprise High Availability Extension 12 SP3</dm:product>
          <dm:component>GEO Clustering</dm:component>
        </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
 </info>
  <sect1 xml:id="sec.ha.geo.quick.concept">
    <title>Conceptual Overview</title>
    <para>
      &geo; clusters based on &productname; can be considered
      <quote>overlay</quote> clusters where each cluster site corresponds to a
      cluster node in a traditional cluster. The overlay cluster is managed by
      the booth mechanism. Each of the parties involved in a &geo; cluster runs
      a service, the &boothd;. It connects to the booth daemons running at the
      other sites and exchanges connectivity details. For making cluster
      resources highly available across sites, booth relies on cluster objects
      called tickets. A ticket grants the right to run certain resources on a
      specific cluster site. Booth guarantees that every ticket is granted to no
      more than one site at a time.
    </para>
    <para>
    If the communication between two booth instances breaks down, it might be
    due to a network breakdown between the cluster sites <emphasis>or</emphasis>
    to an outage of one cluster site). In this case, you need an additional instance
    (an arbitrator) to reach consensus about decisions such as failover of resources
    across sites. Arbitrators are single machines (outside of the clusters) that
    run a booth instance in a special mode. Each &geo; cluster can have one or
    multiple arbitrators. They are especially important for &geo; cluster
    setups with an even number of sites.
    </para>
    <para>
      <remark>taroth 2017-06-27: todo: fine-tune graphic</remark>
    </para>
    <figure xml:id="fig.ha.geo.quick.example.geosetup">
      <title>Two-Site Cluster (2x2 Nodes + Arbitrator)</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="ha_geocluster.svg" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="ha_geocluster.png" width="85%" format="PNG"/>
        </imageobject>
      </mediaobject>
    </figure>
    <para>
      For more details on components and ticket management for &geo; clusters,
      see .<remark>FIXME - taroth 2017-06-27: add link to
      conceptual overview in Geo  Guide</remark>
    </para>
  </sect1>
  <sect1 xml:id="sec.ha.geo.quick.scenario">
    <title>Usage Scenario</title>
    <para>
      The procedures in this document will lead to an example setup of a
      &geo; cluster with the following properties:
    </para>
    <example>
      <title>Example Setup of a &geo; Cluster</title>
      <para> A &geo; cluster with two sites,
          <literal>&cluster1;</literal> and
          <literal>&cluster2;</literal>, including one arbitrator. </para>
    </example>
    <para>
      Currently, the &geo; cluster bootstrap script only takes care of setting up
      booth and of creating the minimum set of resources that booth needs.
      <remark>taroth 2017-06-26:
        add link to description of bootstrap scripts</remark>
    </para>
    <para>
      In addition, you need to take the following steps to get a functioning
      &geo; cluster:
      <remark>taroth 2017-06-26: @krig, could you help to provide a list of things that
        the admin needs to execute before/after running the init scripts (in order
        to get a functioning geo cluster)?</remark>
    </para>
  </sect1>
  <sect1 xml:base="sec.ha.geo.quick.req">
    <title>Requirements</title>
    <para>
      <remark>taroth 2017-06-27: todo - for future, use phrases entities like
      in Install Quick</remark>
    </para>
     <itemizedlist>
      <title>Software Requirements</title>
      <listitem>
        <para>
          All machines (cluster nodes and arbitrators) that will be part of the &geo; cluster must be based on
          &productname; &productnumber; and must have the following software installed:
        </para>
       <itemizedlist>
        <listitem>
        <para>
          &slsreg; &productnumber;
        </para>
        </listitem>
        <listitem>
         <para>
          &productname; &productnumber;
         </para>
        </listitem>
        <listitem>
          <para>
           &hageo; &productnumber;
           <!--<remark>taroth 2014-08-20: booth package would be enough (GEO pattern only consists of the booth
             package) but to make it clear where to get the package from, phrasing like this</remark>-->
          </para>
         </listitem>
       </itemizedlist>
   </listitem>
    </itemizedlist>
     <itemizedlist>
      <title>Network Requirements</title>
      <listitem>
        <para>
          Each cluster site has a private network routed to the other site:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              &cluster1;: <literal>192.168.201.x</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              &cluster2;: <literal>192.168.202.x</literal>
            </para>
          </listitem>
        </itemizedlist>
      </listitem>
      <listitem>
        <para>
          The sites must be reachable on one UDP and TCP port per booth instance.
          That means any firewalls or IPsec tunnels in between must be configured
          accordingly.
        </para>
      </listitem>
      <listitem>
        <para>
          Other setup decision may require to open more ports (for example, for
          DRBD or database replication).
        </para>
      </listitem>
    </itemizedlist>
    
    <itemizedlist>
      <title>Other Requirements and Recommendations</title>
      <listitem>
        <para>
          All cluster nodes on all sites should synchronize to an NTP server
          outside the cluster. For more information, see the
          <citetitle>&admin;</citetitle> for &sls; &productnumber;,
          available at <link xlink:href="http://www.suse.com/documentation/"/>. Refer to the chapter <citetitle>Time
            Synchronization with NTP</citetitle>.
        </para>
        <para>
          If nodes are not synchronized, log files and cluster reports are very
          hard to analyze.
        </para>
      </listitem>
      <listitem>
        <para>The cluster on each site has a meaningful name, for example:
          <literal>&cluster1;</literal> and
          <literal>&cluster2;</literal>.
        </para>
        <para>
          The cluster names for each site are defined in the respective &corosync.conf; files:
        </para>
        <screen>totem {
      [...]
      cluster_name: &cluster1;
      }
        </screen>
        <para>Set the cluster name during the initial setup of a cluster (with
         <command>ha-cluster-init -n <replaceable>CLUSTERNAME</replaceable></command>), or manually
         (by editing &corosync.conf;). Alternatively, set it with the
          &yast; cluster module (by switching to the <guimenu>Communication Channels</guimenu>
          category and defining a <guimenu>Cluster Name</guimenu>).
          For the changes to take effect, restart the &pace; service afterwards.
        </para>
      </listitem>
    </itemizedlist>
  </sect1>
  <sect1 xml:id="sec.ha.geo.scripts">
    <title>Overview of the Bootstrap Scripts</title>
    <para>
      All commands from the <package>ha-cluster-bootstrap</package> package
      execute bootstrap scripts that require only a minimum of time and manual
      intervention.
    </para>
    <itemizedlist>
      <listitem>
        <para>
          With <command>ha-cluster-geo-init</command>, turn a cluster into the first
          member of a &geo; cluster. The script creates &booth.conf; (by taking
          several parameters like the names of the clusters, an arbitrator, and one or
          more tickets). It also configures the following cluster resources needed
          for booth:</para>
        <itemizedlist>
          <listitem>
            <para>A primitive resource for the booth daemon
              (<literal>ocf:pacemaker:booth-site</literal>). It communicates with the
              booth daemons on the other cluster sites.</para>
          </listitem>
          <listitem>
            <para>
              A virtual IP address for each cluster site
              (<literal>ocf:heartbeat:IPaddr2</literal>). It is required by the booth daemon
              who needs a persistent IP address on each cluster site.
            </para>
          </listitem>
          <listitem>
            <para>A cluster resource group (<literal>g-booth</literal>) for both
              primitives.</para>
          </listitem>
        </itemizedlist>
      </listitem>
      <listitem>
        <para>
          With <command>ha-cluster-geo-join</command>, add the current cluster to an
          existing &geo; cluster.
        </para>
      </listitem>
      <listitem>
        <para>
          With <command>ha-cluster-geo-init-arbitrator</command>, make the current node an
          arbitrator for the &geo; cluster.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      All bootstrap scripts log to <filename>/var/log/ha-cluster-bootstrap.log</filename>.
      Check this file for any details of the bootstrap process. Any options set
      during the bootstrap process can be modified later.
    </para>
    <para>Each script comes with a man page covering the range of functions, the
      script's options, and an overview of the files the script can create and modify.
    </para>
  </sect1>
  <sect1 xml:id="sec.ha.geo.quick.inst">
    <title>Installation as Extension</title>
    <para>
      For using the &hasi; and &hageo;, you need the packages included in
      the following installation patterns:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <literal>&ha;</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>&geo; Clustering for &ha;</literal>
        </para>
      </listitem>
    </itemizedlist>
    <para>
      Both patterns are only available if you have registered your system at
      &scc; (or a local registration server) and have added the respective
      product channels or installation media as an extension. For information on how
      to install extensions, see the <citetitle>&sle; &productnumber;
        &deploy;</citetitle>, available at <link xlink:href="http://www.suse.com/documentation/"/>. Refer to
      chapter <citetitle>Installing Modules, Extensions, and Third Party Add-On Products</citetitle>.
      <!--taroth: need to use hard-coded link here as the target is not included in the same set-->
    </para>
    <procedure xml:id="pro.ha.geo.inst">
      <title>Installing the Packages</title>
      <step>
        <para>
          To install the packages from both patterns via command line, use
          Zypper:
        </para>
        <screen>sudo <command>zypper</command> in -t pattern ha_sles ha_geo</screen>
      </step>
      <step xml:id="step.ha.geo.inst.yast">
        <para>
          Alternatively, use &yast; for a graphical installation:
        </para>
        <substeps performance="required">
          <step>
            <para>
              Start &yast; as &rootuser; user and select <menuchoice>
                <guimenu>Software</guimenu> <guimenu>Software Management</guimenu>
              </menuchoice>.
            </para>
          </step>
          <step>
            <para>
              Click <menuchoice> <guimenu>View</guimenu>
                <guimenu>Patterns</guimenu> </menuchoice> and activate the following
              patterns:
            </para>
            <itemizedlist>
              <listitem>
                <para>
                  <literal>&ha;</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>&geo; Clustering for &ha;</literal>
                </para>
              </listitem>
            </itemizedlist>
          </step>
          <step>
            <para>
              Click <guimenu>Accept</guimenu> to start installing the packages.
            </para>
          </step>
        </substeps>
      </step>
    </procedure>
    <important>
      <title>Installing Software Packages on all Parties</title>
      <para>
        The software packages needed for &ha; and &geo; clusters are
        <emphasis>not</emphasis> automatically copied to the cluster nodes.
      </para>
      <itemizedlist>
        <listitem>
          <para>
            Install &sls; &productnumber; and the <literal>&ha;</literal>
            and <literal>&geo; Clustering for &ha;</literal> patterns on
            <emphasis>all</emphasis> machines that will be part of your &geo;
            cluster.
          </para>
        </listitem>
        <listitem>
          <para>
            If you do not want to install the packages manually on all nodes that
            will be part of your cluster, use &ay; to clone existing nodes.
            Find more information in the <citetitle>&haguide;</citetitle> for
            &productname; &productnumber;, available from
            <link xlink:href="http://www.suse.com/documentation/"/>. Refer to the
            chapter <citetitle>Installation the &hasi;</citetitle>, section
            <citetitle>Mass Installation and Deployment with &ay;</citetitle>.
          </para>
          <para>
            For all machines that need the &geo; clustering extension, you currently need
            to install the packages for &geo; clusters manually. &ay;
            support for &hageo; is not yet available.
          </para>
        </listitem>
      </itemizedlist>
    </important>
  </sect1>
  <sect1 xml:base="sec.ha.geo.quick.rsc">
    <title>Configuring Cluster Resources and Constraints</title>
    <para>
  Apart from the resources and constraints that you need to define for your
  specific cluster setup, &geo; clusters require additional resources and
  constraints as described below. You can either configure them with the
  &crmshell; (&crmsh;) as demonstrated in the examples below, or with
  the &haweb; (&hawk2;).
 </para>

 <para>
  This section focuses on tasks specific to &geo; clusters. For an
  introduction to your preferred cluster management tool and general
  instructions on how to configure resources and constraints with it, refer
  to one of the following chapters in the
  <citetitle>&haguide;</citetitle> for &productname;, available from
  <link xlink:href="http://www.suse.com/documentation/"/>:
 </para>

 <itemizedlist>
  <listitem>
   <para>
    &hawk2;: Chapter <citetitle>Configuring and Managing Cluster Resources
    (Web Interface)</citetitle>
   </para>
  </listitem>
  <listitem>
   <para>
    &crmsh;: Chapter <citetitle>Configuring and Managing Cluster
    Resources (Command Line)</citetitle>
   </para>
  </listitem>
 </itemizedlist>

 <important>
  <title>No CIB Synchronization Across Sites</title>
  <para>
   The CIB is <emphasis>not</emphasis> automatically synchronized across
   cluster sites of a &geo; cluster. This means you need to configure all
   resources that must be highly available across the &geo; cluster for
   each site accordingly.
  </para>
  <para>
   To simplify transfer of the configuration to other cluster sites, any
   resources with site-specific parameters can be configured in such a way
   that the parameters' values depend on the name of the cluster site where
   the resource is running.
  </para>
  <para>
   To make this work, the cluster names for each site must be defined in the
   respective &corosync.conf; files. For example, &corosync.conf; of
   site 1 (<literal>&cluster1;</literal>) must contain the following
   entry:
  </para>
<screen>totem {
   [...]
   cluster_name: &cluster1;
   }</screen>
  <para>
   After you have configured the resources on one site, you can tag the
   resources that are needed on all cluster sites, export them from the
   current CIB, and import them into the CIB of another cluster site. For
   details, see <xref linkend="sec.ha.geo.rsc.sync.cib"/>.
  </para>
 </important>

 <sect2 xml:id="sec.ha.geo.rsc.drbd">
  <title>Resources and Constraints for DRBD</title>
  <para>
   To complete the DRBD setup, you need to configure some resources and
   constraints as shown in
   <xref linkend="pro.ha.geo.rsc.drbd" xrefstyle="select:label"/> and
   transfer them to the other cluster sites as explained in
   <xref linkend="sec.ha.geo.rsc.sync.cib"/>.
  </para>
  <procedure xml:id="pro.ha.geo.rsc.drbd">
   <title>Configuring Resources for a DRBD Setup</title>
   <step>
    <para>
     On one of the nodes of cluster <literal>&cluster1;</literal>, start
     a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Configure the (site-dependent) service IP for NFS as a basic primitive:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip_nfs ocf:heartbeat:IPaddr2 \
  params iflabel="nfs" nic="eth1" cidr_netmask="24"
  params rule #cluster-name eq &cluster1; ip="192.168.201.151" \
  params rule #cluster-name eq &cluster2; ip="192.168.202.151" \
  op monitor interval=10</screen>
   </step>
   <step>
    <para>
     Configure a file system resource and a resource for the NFS server:
    </para>
    <screen>&prompt.crm.conf;<command>primitive</command> nfs_fs ocf:heartbeat:Filesystem \
  params device="/dev/drbd/by-res/nfs/0" directory="/mnt/nfs" \
  fstype="ext4"
&prompt.crm.conf;<command>primitive</command> nfs_service systemd:nfs-server</screen>
   </step>
   <step>
    <para>
     Configure the following primitives and multi-state resources for DRBD:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> drbd_nfs ocf:linbit:drbd \
  params drbd_resource="nfs-upper" \
  op monitor interval="31" role="Slave" \
  op monitor interval="30" role="Master"
&prompt.crm.conf;<command>primitive</command> drbd_nfs_lower ocf:linbit:drbd \
  params rule #cluster-name eq &cluster1; \
  drbd_resource="nfs-lower-&cluster1;" \
  params rule #cluster-name eq &cluster2; \
  drbd_resource="nfs-lower-&cluster2;" \                                
  op monitor interval="31" role="Slave" \
  op monitor interval="30" role="Master"
&prompt.crm.conf;<command>ms</command> ms_drbd_nfs drbd_nfs \
  meta master-max="1" master-node-max="1" \
  clone-max="1" clone-node-max="1" notify="true"
&prompt.crm.conf;<command>ms</command> ms_drbd_nfs_lower drbd_nfs_lower \
  meta master-max="1" master-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true"</screen>
   </step>
   <step>
    <para>
     Add a group with the following colocation and ordering constraints:
    </para>
<screen>&prompt.crm.conf;<command>group</command> g_nfs nfs_fs nfs_service
&prompt.crm.conf;<command>colocation</command> col_nfs_ip_with_lower \
   inf: ip_nfs:Started  ms_drbd_nfs_lower:Master
&prompt.crm.conf;<command>colocation</command> col_nfs_g_with_upper \
   inf: g_nfs:Started  ms_drbd_nfs:Master
&prompt.crm.conf;<command>colocation</command> col_nfs_upper_with_ip \
   inf: ms_drbd_nfs:Master  ip_nfs:Started
&prompt.crm.conf;<command>order</command> o_lower_drbd_before_ip_nfs \
   inf: ms_drbd_nfs_lower:promote  ip_nfs:start
&prompt.crm.conf;<command>order</command> o_ip_nfs_before_drbd \
   inf: ip_nfs:start  ms_drbd_nfs:promote
&prompt.crm.conf;<command>order</command> o_drbd_nfs_before_svc \
   inf: ms_drbd_nfs:promote  g_nfs:start</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec.ha.geo.rsc.booth">
  <title>Ticket Dependencies, Constraints and Resources for booth</title>
  <para>
   To complete the booth setup, you need to execute the following steps to
   configure the resources and constraints needed for booth and failover of
   resources:
  </para>
  <itemizedlist>
   <listitem>
    <para>
<!--Configuring Ticket Dependencies-->
     <xref linkend="pro.ha.geo.setup.rsc.constraints" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
<!--Configuring a Resource Group for <systemitem class="daemon"
      >boothd</systemitem>-->
     <xref linkend="pro.ha.geo.setup.rsc.boothd" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
<!--Adding an Ordering Constraint for <systemitem class="daemon"
       >boothd</systemitem> and the Resource Group-->
     <xref linkend="pro.ha.geo.setup.rsc.order" xrefstyle="select:title"/>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The resource configurations need to be available on each of the cluster
   sites. Transfer them to the other sites as described in
   <xref linkend="sec.ha.geo.rsc.sync.cib"/>.
  </para>
  <procedure xml:id="pro.ha.geo.setup.rsc.constraints">
   <title>Configuring Ticket Dependencies of Resources</title>
   &ticket-dependency-loss-policy; 
   <step>
    <para>
     On one of the nodes of cluster &cluster1;, start a shell and log in
     as &rootuser; or equivalent.
<!--<remark>toms 2011-10-04: Shorten it to "Start a root shell on one
       of the cluster nodes"? I guess we deal with admin-minded people here</remark>
       taroth 2011-10-06: because of new ACL support of the crm shell, also
       non-root users may have the rights to do so, therefore phrased it like
       that -->
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step xml:id="step.ha.geo.setup.rsc.constraints">
    <para>
     Configure constraints that define which resources depend on a certain
     ticket. For example, we need the following constraint for the DRBD
     scenario outlined in <!--FIXME<xref linkend="sec.ha.geo.drbd.scenario"/>-->:
    </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> nfs-req-ticket-nfs ticket-nfs: \ 
   ms_drbd_nfs:Master loss-policy=demote</screen>
    <para>
     This command creates a constraint with the ID
     <literal>nfs-req-ticket-nfs</literal>. It defines that the multi-state
     resource <literal>ms_drbd_nfs</literal> depends on
     <literal>ticket-nfs</literal>. However, only the resource's master mode
     depends on the ticket. In case <literal>ticket-nfs</literal> is
     revoked, <literal>ms_drbd_nfs</literal> is automatically demoted to
     <literal>slave</literal> mode, which in return will put DRBD into
     <literal>Secondary</literal> mode. That way, it is ensured that DRBD
     replication is still running, even if a site does not have the ticket.
    </para>
   </step>
   <step>
    <para>
     If you want other resources to depend on further tickets, create as
     many constraints as necessary with <command>rsc_ticket</command>.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
  <example xml:id="ex.ha.geo.setup.rsc.ticket.dep">
   <title>Ticket Dependency for Primitives</title>
   <para>
    Here is another example for a constraint that makes a primitive resource
    <literal>rsc1</literal> depend on <literal>&ticket1;</literal>:
   </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-&ticket1; &ticket1;: \
   rsc1 loss-policy="fence"</screen>
   <para>
    In case <literal>&ticket1;</literal> is revoked, the node running the
    resource should be fenced.
   </para>
  </example>
  <procedure xml:id="pro.ha.geo.setup.rsc.boothd">
   <title>Configuring a Resource Group for <systemitem class="daemon">boothd</systemitem></title> 
   &boothd-resource-group; 
   <step>
    <para>
     On one of the nodes of cluster <literal>&cluster1;</literal>, start
     a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Enter the following to create both primitive resources and to add them
     to one group, <literal>g-booth</literal>:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip-booth ocf:heartbeat:IPaddr2 \
  params iflabel="ha" nic="eth1" cidr_netmask="24"
  params rule #cluster-name eq &cluster1; ip="192.168.201.151" \
  params rule #cluster-name eq &cluster2; ip="192.168.202.151" 
&prompt.crm.conf;<command>primitive</command> booth ocf:pacemaker:booth-site \
  meta resource-stickiness="INFINITY" \
  params config="nfs" op monitor interval="10s"
&prompt.crm.conf;<command>group</command> g-booth ip-booth booth</screen>
    <para>
     With this configuration, each booth daemon will be available at its
     individual IP address, independent of the node the daemon is running
     on.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
<!--taroth 2012-02-14: fix for bnc#746863-->
  <procedure xml:id="pro.ha.geo.setup.rsc.order">
   <title>Adding an Ordering Constraint</title> 
   &booth-order-constraint; 
   <step>
    <para>
     On one of the nodes of cluster &cluster1;, start a shell and log in
     as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Create an ordering constraint:
    </para>
<screen>&prompt.crm.conf;<command>order</command> o-booth-before-nfs inf: g-booth ms_drbd_nfs:promote</screen>
    <para>
     The ordering constraint <literal>o-booth-before-nfs</literal> defines
     that the resource <literal>ms_drbd_nfs</literal> can only be promoted
     to master mode after the <literal>g-booth</literal> resource group has
     started.
    </para>
   </step>
   <step>
    <para>
     For any other resources that depend on a certain ticket, define further
     ordering constraints.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
  <example xml:id="ex.ha.geo.rsc.order">
   <title>Ordering Constraint for Primitives</title>
   <para>
    If the resource that depends on a certain ticket is not a multi-state
    resource, but a primitive, the ordering constraint would look like the
    following:
   </para>
<screen>&prompt.crm.conf;<command>order</command> o-booth-before-rsc1 inf: g-booth rsc1</screen>
   <para>
    It defines that <literal>rsc1</literal> (which depends on
    <literal>&ticket1;</literal>) can only be started after the
    <literal>g-booth</literal> resource group.
   </para>
  </example>
 </sect2>

 <sect2 xml:id="sec.ha.geo.rsc.sync.cib">
<!--taroth 2014-11-25: fate#316118: [BETA 7] CIB replication between sites-->
  <title>Transferring the Resource Configuration to Other Cluster Sites</title>
  <para>
   If you have configured resources for one cluster site as described in
   <xref linkend="sec.ha.geo.rsc.drbd" xrefstyle="select:label"/> and
   <xref linkend="sec.ha.geo.rsc.booth" xrefstyle="select:label"/>, you are
   not done yet. You need to transfer the resource configuration to the
   other sites of your &geo; cluster.
  </para>
  <para>
   To simplify the transfer, you can tag any resources that are needed on
   <emphasis>all</emphasis> cluster sites, export them from the current CIB,
   and import them into the CIB of another cluster site.
   <xref linkend="pro.ha.geo.rsc.sync.cib"/> gives an example of how to do
   so. It is based on the following prerequisites:
  </para>
  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     You have a &geo; cluster with two sites: cluster
     <literal>&cluster1;</literal> and cluster
     <literal>&cluster2;</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     The cluster names for each site are defined in the respective
     &corosync.conf; files:
    </para>
<screen>totem {
     [...]
     cluster_name: &cluster1;
     }</screen>
    <para>
     This can either be done manually (by editing &corosync.conf;) or
     with the &yast; cluster module as described in the
     <citetitle>&admin;</citetitle> for &productname;
     &productnumber;, available at <link xlink:href="http://www.suse.com/documentation/"/>. Refer to the
     chapter <citetitle>Installation and Basic Setup</citetitle>, procedure
     <citetitle>Defining the First Communication Channel</citetitle>.
    </para>
   </listitem>
   <listitem>
    <para>
     You have configured the necessary resources for DRBD and booth as
     described in <xref linkend="sec.ha.geo.rsc.drbd"/> and
     <xref linkend="sec.ha.geo.rsc.booth"/>.
    </para>
   </listitem>
  </itemizedlist>
  <procedure xml:id="pro.ha.geo.rsc.sync.cib">
   <title>Transferring the Resource Configuration to Other Cluster Sites</title>
   <step>
    <para>
     Log in to one of the nodes of cluster <literal>&cluster1;</literal>.
    </para>
   </step>
   <step>
    <para>
     Start the cluster with:
    </para>
<screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Tag the resources and constraints that are needed across the &geo;
     cluster:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Review the current CIB configuration:
      </para>
<screen>&prompt.crm.conf;show</screen>
     </step>
     <step>
      <para>
       Enter the following command to group the &geo; cluster-related
       resources with the tag <literal>geo_resources</literal>:
      </para>
<screen>&prompt.crm.conf;<command>tag</command> geo_resources: \
  ip_nfs nfs_fs nfs_service drbd_nfs drbd_nfs_lower ms_drbd_nfs \
  ms_drbd_nfs_lower g_nfs <co xml:id="co.geo.rsc.drbd"/>\
  col_nfs_ip_with_lower  col_nfs_g_with_upper col_nfs_upper_with_ip <xref linkend="co.geo.rsc.drbd" xrefstyle="select:label"/>\
  o_lower_drbd_before_ip_nfs o_ip_nfs_before_drbd \
  o_drbd_nfs_before_svc <xref linkend="co.geo.rsc.drbd" xrefstyle="select:label"/>\
  nfs-req-ticket-nfs ip-booth booth g-booth o-booth-before-nfs <co xml:id="co.geo.rsc.booth"/>
  [...] <co xml:id="co.geo.rsc.any"/></screen>
      <para>
       Tagging does not create any colocation or ordering relationship
       between the resources.
      </para>
      <calloutlist>
       <callout arearefs="co.geo.rsc.drbd">
        <para>
         Resources and constraints for DRBD, see
         <xref linkend="sec.ha.geo.rsc.drbd"/>.
        </para>
       </callout>
       <callout arearefs="co.geo.rsc.booth">
        <para>
         Resources and constraints for boothd, see
         <xref linkend="sec.ha.geo.rsc.booth"/>.
        </para>
       </callout>
       <callout arearefs="co.geo.rsc.any">
        <para>
         Any other resources of your specific setup that you need on all
         sites of the &geo; cluster.
        </para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Review your changes with <command>show</command>.
      </para>
     </step>
     <step>
      <para>
       If the configuration is according to your wishes, submit your changes
       with <command>submit</command> and leave the crm live shell with
       <command>exit</command>.
      </para>
     </step>
    </substeps>
   </step>
   <step xml:id="st.ha.geo.rsc.sync.cib.export.start">
    <para>
     Export the tagged resources and constraints to a file named
     <filename>exported.cib</filename>:
    </para>
<screen>&prompt.root;<command>crm configure show</command> tag:geo_resources geo_resources &gt; exported.cib</screen>
    <para>
     The command <command>crm configure show tag:</command><replaceable>TAGNAME</replaceable> 
     shows all resources that belong to
     the tag <replaceable>TAGNAME</replaceable>.
    </para>
   </step>
   <step>
    <para>
     Log in to one of the nodes of cluster <literal>&cluster2;</literal>
     and proceed as follows:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Start the cluster with:
      </para>
<screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
     </step>
     <step>
      <para>
       Copy the file <filename>exported.cib</filename> from cluster
       <literal>&cluster1;</literal> to this node.
       <remark>taroth
        2014-11-26: alternatively, the CIB can be loaded from an URL - consider
        if to mention this, too</remark>
      </para>
     </step>
     <step>
      <para>
       Import the tagged resources and constraints from the file
       <filename>exported.cib</filename> into the CIB of cluster
       <literal>&cluster2;</literal>:
      </para>
<screen>&prompt.root;<command>crm configure load</command> update <replaceable>PATH_TO_FILE/exported.cib</replaceable></screen>
      <para>
       When using the <option>update</option> parameter for the <command>crm
       configure load</command> command, &crmsh; tries to integrate the
       contents of the file into the current CIB configuration (instead of
       replacing the current CIB with the file contents).
      </para>
     </step>
     <step xml:id="st.ha.geo.rsc.sync.cib.import.stop">
      <para>
       View the updated CIB configuration with the following command:
      </para>
<screen>&prompt.root;<command>crm configure show</command></screen>
      <para>
       The imported resources and constraints will appear in the CIB.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   This configuration will result in the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     When granting <literal>ticket-nfs</literal> to cluster
     <literal>&cluster1;</literal>, the node hosting the resource
     <literal>ip_nfs</literal> will get the IP address
     <literal>192.168.201.151</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     When granting <literal>ticket-nfs</literal> to cluster
     <literal>&cluster2;</literal>, the node hosting the resource
     <literal>ip_nfs</literal> will get the IP address
     <literal>192.168.202.151</literal>.
    </para>
   </listitem>
  </itemizedlist>
  <example xml:id="ex.ha.geo.rsc.refer.params">
   <title>Referencing Site-Dependent Parameters in Resources</title>
   <para>
    Based on the example in
    <xref linkend="pro.ha.geo.rsc.drbd" xrefstyle="select:label"/>, you can
    also create resources that reference site-specific parameters of another
    resource, for example, the IP parameters of <literal>ip_nfs</literal>.
    Proceed as follows:
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      On cluster <literal>&cluster1;</literal> create a dummy resource
      that references the IP parameters of <literal>ip_nfs</literal> and
      uses them as the value of its <literal>state</literal> parameter:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> dummy1 ocf:pacemaker:Dummy \
  params rule #cluster-name eq &cluster1; \
  @ip_nfs-instance_attributes-0-ip:state \
  params rule #cluster-name eq &cluster2; \
  @ip_nfs-instance_attributes-1-ip:state \
  op monitor interval=10</screen>
    </listitem>
    <listitem>
     <para>
      Add a constraint to make the <literal>dummy1</literal> resource depend
      on <literal>ticket-nfs</literal>, too:
     </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> dummy1-dep-ticket-nfs \
  ticket-nfs: dummy1 loss-policy=stop</screen>
    </listitem>
    <listitem>
     <para>
      Tag the resource and the constraint:
     </para>
<screen>&prompt.crm.conf;<command>tag</command> geo_resources_2: dummy1 \
  dummy1-dep-ticket-nfs</screen>
    </listitem>
    <listitem>
     <para>
      Review your changes with <command>show</command>, submit your changes
      with <command>submit</command>, and leave the crm live shell with
      <command>exit</command>.
     </para>
    </listitem>
    <listitem>
     <para>
      Export the resources tagged with <literal>geo_resources_2</literal>
      from cluster <literal>&cluster1;</literal> and import them into the
      CIB of cluster <literal>&cluster2;</literal>, similar to
      <xref linkend="st.ha.geo.rsc.sync.cib.export.start"/> through
      <xref linkend="st.ha.geo.rsc.sync.cib.import.stop"/> of
      <xref linkend="pro.ha.geo.rsc.sync.cib" xrefstyle="select:label"/>.
     </para>
    </listitem>
   </orderedlist>
   <para>
    This configuration will result in the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      When granting <literal>ticket-nfs</literal> to cluster
      <literal>&cluster1;</literal>, the following file will be created
      on the node hosting the <literal>dummy</literal> resource:
      <filename>/var/lib/heartbeat/cores/192.168.201.151</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      When granting <literal>ticket-nfs</literal> to cluster
      <literal>&cluster2;</literal>, the following file will be created
      on the node hosting the <literal>dummy</literal> resource:
      <filename>/var/lib/heartbeat/cores/192.168.202.151</filename>.
     </para>
    </listitem>
   </itemizedlist>
  </example>
 </sect2>
</sect1>
<xi:include href="common_legal.xml"/>
</article>
