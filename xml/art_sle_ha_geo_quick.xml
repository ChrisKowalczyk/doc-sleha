<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:novdoc-profile.xsl"
  type="text/xml" 
  title="Profiling step"?>
<!DOCTYPE article PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
                      "novdocx.dtd"
[
<!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<!--taroth 2014-08-07: check screens (and add line-breaks)-->
<?provo dirname="geo_quick/"?>
<article lang="en" id="art.ha.geo.quick">
<?suse-quickstart columns="no" version="2"?>
 <title>&geoquick;</title>
<!-- <subtitle>GEO Clustering for &productname;
 </subtitle>-->
 <articleinfo><productname>GEO Clustering for &productname;</productname>
  <productnumber>&productnumber;</productnumber>
  <authorgroup>
   <author><firstname>Tanja</firstname><surname>Roth</surname>
   </author>
  </authorgroup>
 </articleinfo>
 <abstract>
  <para> Apart from local clusters and metro area clusters, &productnamereg;
   &productnumber; also supports &geo; clusters. That means you can have
   multiple, geographically dispersed sites with a local cluster each. Failover
   between these clusters is coordinated by a higher level entity: the booth
   daemon (&boothd;). Support for &geo; clusters is available as
   a separate extension to &hasi;, called &hageo;. </para>
  <para>
   <remark>taroth 2014-08-11: todo - https://fate.suse.com/316120: [docu]
    Quickstart guide for SLE HA GEO, for DRBD-related part, see also 
    https://mailman.suse.de/mailman/private/ha-devel/2014-February/003095.html
    for input)</remark>
   <remark>taroth 2014-08-11: todo - https://fate.suse.com/316114: Replicated
    storage solution based on DRBD (NEEDINFO, input probably 
    https://mailman.suse.de/mailman/private/ha-devel/2014-February/003095.html)</remark>
  </para>
 </abstract>
 <para>
  <remark>taroth 2014-08-11: https://fate.suse.com/316112: IP relocation via DNS
   update (prio: mandatory): check with devs about doc impact, where/in which
   scenario is it visible for the user? </remark>
  <remark>taroth 2014-08-11: https://fate.suse.com/316126: reconfiguration of boothd while running
   (prio: important): 
   #4: Lars Marowsky-Bree (2013-08-19 16:08) [reply] Currently, to add
   tickets or remove sites or change their IP address, the booth daemon
   has to be restarted, which affects all other tickets and connections
   as well - todo: not sure where to mention this...</remark></para>
 <sect1 id="sec.ha.geo.inst">
  <title>Installation as Add-on</title>
  <para>For using the &hasi; and &hageo;, you need the packages included
   in the following installation patterns:</para>
  
  <itemizedlist>
   <listitem>
    <para>
     <literal>&ha;</literal>
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>&geo; Clustering for &ha;</literal>
    </para>
   </listitem>
  </itemizedlist>
  
  <para>Both patterns are only available if you have registered your system at
   &scc; (or a local registration server) and have added the respective
   product channels or installation media as add-ons. For information on how to
   install add-on products, see the <citetitle>&sle; &productnumber;
    &deploy;</citetitle>, available at <ulink url="http://www.suse.com/doc"
   />. Refer to chapter <citetitle>Installing Add-On Products</citetitle>.
   <!--taroth: need to use hard-coded link here as the target is not included in the same set-->
  </para>
  
  <procedure id="pro.ha.install.pattern">
   <title>Installing the Packages</title>
   <para>In case both &hasi; and &hageo; have been added as add-on
    products, but the packages are not installed yet, proceed as follows:</para>
   
   <step>
    <para>To install the packages from both patterns via command line, use zypper:</para>
    <screen>sudo <command>zypper</command> in -t pattern ha_sles ha_geo</screen>
    </step>
   <step>
    <para>Alternatively, use &yast; for a graphical installation:</para>
    <substeps>
     <step>
      <para> Start &yast; as &rootuser; user and select <menuchoice>
        <guimenu>Software</guimenu>
        <guimenu>Software Management</guimenu>
       </menuchoice>. </para>
     </step>
     <step>
      <para> Click <menuchoice>
        <guimenu>View</guimenu>
        <guimenu>Patterns</guimenu>
       </menuchoice> and activate the following patterns:</para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>&ha;</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>&geo; Clustering for &ha;</literal>
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para> Click <guimenu>Accept</guimenu> to start installing the packages.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>
  
  <important>
   <para> The software packages needed for &ha; and &geo; clusters are
     <emphasis>not</emphasis> automatically copied to the cluster nodes. </para>
   <itemizedlist>
    <listitem>
     <para> Install &sls; &productnumber; and the
       <literal>&ha;</literal> and <literal>&geo; Clustering for
       &ha;</literal> patterns on <emphasis>all</emphasis> machines that
      will be part of your &geo; cluster. </para>
    </listitem>
    <listitem>
     <para> If you do not want to install the packages manually on all nodes
      that will be part of your cluster, use &ay; to clone existing nodes.
      Find more information in the <citetitle>&haguide;</citetitle> for
      &productname;, available from &suse-onlinedoc;. Refer to chapter
       <citetitle>Installation and Basic Setup</citetitle>, section
       <citetitle>Mass Deployment with &ay;</citetitle>.</para>
    </listitem>
   </itemizedlist>
  </important>
</sect1>
 
 
 <sect1 id="sec.ha.geo.challenges">
  <title>Challenges for &geo; Clusters</title>

  <para> Typically, &geo; environments are too far apart to support
   synchronous communication between the sites. That leads to the following
   challenges: </para>

  <itemizedlist>
   <listitem>
    <para>
     How to make sure that a cluster site is up and running?
    </para>
   </listitem>
   <listitem>
    <para>
     How to make sure that resources are only started once?
    </para>
   </listitem>
   <listitem>
    <para>
     How to make sure that quorum can be reached between the different sites
     and a split brain scenario can be avoided?
    </para>
   </listitem>
<!--<listitem>
    <para>How to keep the CIB up-to-date on all nodes and sites?</para>
   </listitem>
   -->
   <listitem>
    <para>
     How to manage failover between the sites?
    </para>
   </listitem>
   <listitem>
    <para>
     How to deal with high latency in case of resources that need to be
     stopped?
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In the following sections, learn how to meet these challenges with
   &productname;.
  </para>
 </sect1>
 <sect1 id="sec.ha.geo.concept">
  <title>Conceptual Overview</title>

  <para> &geo; clusters based on &productname; can be considered as <quote>overlay</quote>
   clusters where each cluster site corresponds to a cluster node in a traditional cluster. The
   overlay cluster is managed by the booth mechanism. It guarantees that the cluster resources will
   be highly available across different cluster sites. This is achieved by using so-called tickets
   that are treated as failover domain between cluster sites, in case a site should be down. Booth
   guarantees that every ticket is owned by only one site at the time.</para>

  <para>
   The following list explains the individual components and mechanisms that
   were introduced for &geo; clusters in more detail.
  </para>

  <variablelist id="vl.ha.geo.components">
   <title>Components and Ticket Management</title>
   <varlistentry id="vle.ha.geo.components.ticket">
    <term>Ticket</term>
    <listitem>
     <para>
      A ticket grants the right to run certain resources on a specific
      cluster site. A ticket can only be owned by one site at a time.
      Initially, none of the sites has a ticket&mdash;each ticket must be
      granted once by the cluster administrator. After that, tickets are
      managed by the booth for automatic failover of resources. But
      administrators may also intervene and grant or revoke tickets
      manually.
     </para>
     <para>After a ticket is administratively revoked, it is not managed by booth anymore. For booth
      to start managing the ticket again, the ticket must be again granted to a site.</para>
     <para>
      Resources can be bound to a certain ticket by dependencies. Only if
      the defined ticket is available at a site, the respective resources
      are started. Vice versa, if the ticket is removed, the resources
      depending on that ticket are automatically stopped.
     </para>
     <para>
      The presence or absence of tickets for a site is stored in the CIB as
      a cluster status. With regards to a certain ticket, there are only two
      states for a site: <literal>true</literal> (the site has the ticket)
      or <literal>false</literal> (the site does not have the ticket). The
      absence of a certain ticket (during the initial state of the
      &geo; cluster) is not treated differently from the situation
      after the ticket has been revoked: both are reflected by the value
      <literal>false</literal>.
     </para>
     <para>
      A ticket within an overlay cluster is similar to a resource in a
      traditional cluster. But in contrast to traditional clusters, tickets
      are the only type of resource in an overlay cluster. They are
      primitive resources that do not need to be configured nor cloned.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.booth">
    <term>Booth</term>
    <listitem>
     <para>
      Booth is the instance managing the ticket distribution and thus,
      the failover process between the sites of a &geo; cluster. Each
      of the participating clusters and arbitrators runs a service, the
      &boothd;. It connects
      to the booth daemons running at the other sites and exchanges
      connectivity details. Once a ticket is granted to a site, the booth
      mechanism can manage the ticket automatically: If the site which
      holds the ticket is out of service, the booth daemons will vote which
      of the other sites will get the ticket. To protect against brief
      connection failures, sites that lose the vote (either explicitly or
      implicitly by being disconnected from the voting body) need to
      relinquish the ticket after a time-out. Thus, it is made sure that a
      ticket will only be re-distributed after it has been relinquished by
      the previous site. See also
      <xref
       linkend="vle.ha.geo.components.deadman"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.arbitrator">
    <term>Arbitrator</term>
    <listitem>
     <para>
      Each site runs one booth instance that is responsible for
      communicating with the other sites. If you have a setup with an even
      number of sites, you need an additional instance to reach consensus
      about decisions such as failover of resources across sites. In this
      case, add one or more arbitrators running at additional sites.
      Arbitrators are single machines that run a booth instance in a special
      mode. As all booth instances communicate with each other, arbitrators
      help to make more reliable decisions about granting or revoking
      tickets. Arbitrators cannot hold any tickets.
     </para>
     <para>
      An arbitrator is especially important for a two-site scenario: For
      example, if site <literal>A</literal> can no longer communicate with
      site <literal>B</literal>, there are two possible causes for that:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        A network failure between <literal>A</literal> and
        <literal>B</literal>.
       </para>
      </listitem>
      <listitem>
       <para>
        Site <literal>B</literal> is down.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      However, if site <literal>C</literal> (the arbitrator) can still
      communicate with site <literal>B</literal>, site <literal>B</literal>
      must still be up and running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Ticket Failover</term>
    <listitem>
     <para>If the ticket gets lost, that means other boot instances do not hear from the ticket
      owner in a sufficiently long time, one of the remaining sites will acquire the ticket. This is
      what is called ticket failover. If the remaining members cannot form a majority, then the
      ticket cannot fail over. </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.deadman">
    <term>Dead Man Dependency (<literal>loss-policy="fence"</literal>)</term>
    <listitem>
     <para>
      <remark>taroth 2014-08-11: todo - https://fate.suse.com/316131: Improve
       timeout handling mechanism to make booth more robust - check if doc
       impact</remark>
      After a ticket is revoked, it can take a long time until all resources
      depending on that ticket are stopped, especially in case of cascaded
      resources. To cut that process short, the cluster administrator can
      configure a <literal>loss-policy</literal> (together with the ticket
      dependencies) for the case that a ticket gets revoked from a site. If
      the loss-policy is set to <literal>fence</literal>, the nodes that are
      hosting dependent resources are fenced.  
     </para>
     <warning>
      <title>Potential Loss of Data</title>
      <para>On the one hand, <literal>loss-policy="fence"</literal> considerably
       speeds up the recovery process of the cluster and makes sure that
       resources can be migrated more quickly. </para>
      <para>On the other hand, it can lead to loss of all unwritten data, such
       as:</para>
      <itemizedlist>
       <listitem>
        <para>Data lying on shared storage (for example, DRBD).</para>
       </listitem>
       <listitem>
        <para>Data in a replicating database (for example, MariaDB or
         PostgreSQL) that has not yet reached the other site, due to a slow
         network link.</para>
       </listitem>
      </itemizedlist>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>

<!--taroth 201110-06: todo - ask eugene to redo the svg graphic 
  (shows some strange artefacts in PDF, maybe due to import from PDF)-->

  <figure>
   <title>Example Scenario: A Two-Site Cluster (4 Nodes + Arbitrator)</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_geocluster.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_geocluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>The most common scenario is probably a &geo; cluster with two sites
   and a single arbitrator on a third site. The upper limit is (currently) 16
   booth instances. </para>

  <para>
   As usual, the CIB is synchronized within each cluster, but it is not
   synchronized across cluster sites of a &geo; cluster. You have to
   configure the resources that will be highly available across the
   &geo; cluster for every site accordingly. <remark>taroth 2014-08-07: todo -
    https://fate.suse.com/316118:  CIB replication between sites (NEEDINFO, see
    fate c#21)</remark>
  </para>
 </sect1>
 <sect1 id="sec.ha.geo.req">
  <title>Requirements</title>

  <itemizedlist>
   <title>Software Requirements</title>
   <listitem>
    <para>
     All clusters that will be part of the &geo; cluster must be based
     on &productname; &productnumber;.
    </para>
   </listitem>
   <listitem>
    <para>
     &slsreg; &productnumber; must be installed on all arbitrators.
    </para>
   </listitem>
   <listitem>
    <para> The &hageo; add-on must be installed on all cluster nodes <emphasis>and</emphasis> on
     all arbitrators that will be part of the &geo; cluster. 
     <!--<remark>taroth 2014-08-20: booth package would be enough (GEO pattern only consists of the booth
      package) but to make it clear where to get the package from, phrasing like this</remark>--></para>
   </listitem>
  </itemizedlist>
  
  <itemizedlist>
   <title>Network Requirements</title>
   <listitem>
    <para>The sites must be reachable on one UDP and TCP port per booth
     instance. That means any firewalls or IPSec tunnels in between must be
     configured accordingly. </para>
   </listitem>
   <listitem>
    <para>Other setup decision may require to allow more open ports (for
     example, for DRBD or database replication). </para>
   </listitem>
   <listitem>
    <para>
    </para>
   </listitem>
  </itemizedlist>
  
  <itemizedlist>
   <title>Other Requirements and Recommendations</title>
   <listitem>
    <para>All cluster nodes on all sites should synchronize to an NTP server
     outside the cluster. For more information, see the
      <citetitle>&admin;,</citetitle> for &sls; &productnumber;,
     available at &suse-onlinedoc;. Refer to the chapter <citetitle>Time
      Synchronization with NTP</citetitle>. </para>
      <para> If nodes are not synchronized, log files and cluster reports are
       very hard to analyze. </para>
     </listitem>
    </itemizedlist>
  

 
 </sect1>
 <sect1 id="sec.ha.geo.oview">
  <title>Basic Setup&mdash; Overview</title>

  <para>
   Configuring a &geo; cluster takes the following basic steps:
   </para>

  <variablelist>
    <varlistentry>
<!--Setting Up the Booth Services-->
    <term><xref linkend="sec.ha.geo.booth" xrefstyle="select:title"/>
    </term>
    <listitem>
     <orderedlist>
      <listitem>
<!--editing + copying booth config-->
       <para>
        <xref linkend="pro.ha.geo.setup.booth.config" xrefstyle="select:title"/>
       </para>
      </listitem>
      <listitem>
<!--starting boothd-->
       <para>
        <xref linkend="pro.ha.geo.setup.booth.service" xrefstyle="select:title"
        />
       </para>
      </listitem>
     </orderedlist>
    </listitem>
    </varlistentry>
   <varlistentry>
    
    <term>Configuring Cluster Resources and Constraints
     <!--<xref linkend="sec.ha.geo.setup.resources" xrefstyle="select:title"/>-->
    </term>
    <listitem>
     <para>Use either &crmsh; or &hawk; for the following steps:</para>
     <orderedlist>
      <listitem>
       <para>Configuring Ticket Dependencies
        <!--<xref linkend="pro.ha.geo.setup.rsc.constraints"
         xrefstyle="select:title"/>-->
       </para>
      </listitem>
      <listitem>
       <para>Configuring a resource group for <systemitem class="daemon"
         >boothd</systemitem>
        <!--<xref linkend="pro.ha.geo.setup.rsc.boothd" xrefstyle="select:title"/>-->
       </para>
      </listitem>
      <listitem>
       <para>
        Adding an ordering constraint for <systemitem class="daemon"
         >boothd</systemitem> and the resource group
        <!--<xref linkend="pro.ha.geo.setup.rsc.order" xrefstyle="select:title"/>-->
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
  </variablelist>
  </sect1>

  <sect1 id="sec.ha.geo.booth">
   <title>Setting Up the Booth Services</title>
   <para>
   <remark>taroth 2014-08-07: todo - https://fate.suse.com/316123: Multi-tenancy
    for booth (prio: important) -  phil 2013-12: Note: you can use more than one booth instance per arbitrator/site,
    by writing multiple ".conf" files in /etc/booth. The init script will automatically
    start/stop all instances, while the Pacemaker primitives are defined independently
    via a "config" parameter. Giving the "short" name is sufficient, ie. for
    /etc/booth/http.conf  you could specify "config=http" within the CRM
    shell - dmuhamedagic 2014-08-14: There's no init script, sle12 is systemd. The unit file is
    called "booth@.service". Individual services can be enabled/disabled by referencing
    booth@&lt;name>, where name stands for the config /etc/booth/&lt;name>.conf</remark>, <remark>taroth 2014-08-11: todo -
     https://bugzilla.novell.com/show_bug.cgi?id=877817: 
     [Test Case 1378526] [316123] booth - Configuring startup 'complicated'</remark>
    
   </para>
      
   <procedure id="pro.ha.geo.setup.booth.config">
    <title>Editing The Booth Configuration File</title>
   <remark>taroth 2014-08-11: todo - https://fate.suse.com/316124: YaST2 module
    for booth (prio: desirable), NEEDINFO (see c#5), for config options, see
    also
    https://github.com/ClusterLabs/booth/blob/master/docs/boothd.8.txt</remark>
    <step>
     <para>
      Log in to a cluster node as &rootuser; or equivalent.
     </para>
    </step>
    <step>
     <para>
      Create <filename>/etc/booth/booth.conf</filename> and edit it
      according to the example below:
     </para>
     <example>
      <title>Example Booth Configuration File</title>
<screen>transport = UDP <co id="co.ha.geo.booth.config.transport"/>
port = 9929 <co id="co.ha.geo.booth.config.port"/>
arbitrator = 147.2.207.14 <co id="co.ha.geo.booth.config.arbitrator"/>
site= 147.4.215.19 <co id="co.ha.geo.booth.config.site"/>
site= 147.18.2.1  <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket="ticket-A" <co id="co.ha.geo.booth.config.ticket"/>
  expire = 600 <co id="co.ha.geo.booth.config.expiry"/>
  timeout = 10 <co id="co.ha.geo.booth.config.timeout"/>
  retries = 5 <co id="co.ha.geo.booth.config.retries"/>
  renewal-freq = 13 <co id="co.ha.geo.booth.config.renewal"/>
  before-acquire-handler = /usr/share/booth/service-runnable d-src1 <co id="co.ha.geo.booth.config.handler"/>
ticket="ticket-B" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
  expire = 600 <xref linkend="co.ha.geo.booth.config.expiry" xrefstyle="select:label nopage"/>
  timeout = 10 <xref linkend="co.ha.geo.booth.config.timeout" xrefstyle="select:label nopage"/>
  retries = 5 <xref linkend="co.ha.geo.booth.config.retries" xrefstyle="select:label nopage"/>
  renewal-freq = 13 <xref linkend="co.ha.geo.booth.config.renewal" xrefstyle="select:label nopage"/>
  before-acquire-handler = /usr/share/booth/service-runnable d-src1 <xref
   linkend="co.ha.geo.booth.config.handler" xrefstyle="select:label nopage"/>
 </screen>
      <calloutlist>
       <callout arearefs="co.ha.geo.booth.config.transport">
        <para>
         Defines the transport protocol used for communication between the
         sites. Only UDP is supported, other transport layers will
         follow.
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.port">
       <para> Defines the port used for communication between the sites. When not using the default
        port (<literal>9929</literal>, make sure to choose a port that is not already used for
        different services. Make sure to open the port in the nodes' and arbitrators' firewalls.
        The booth clients use TCP to communicate with the &boothd;. Booth will always
        bind and listen to both UDP and TCP ports.</para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.arbitrator">
        <para>
         Defines the IP address of the arbitrator. Insert an entry for each
         arbitrator you use in your setup.
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.site">
        <para>
         Defines the IP address used for the
         &boothd; on each
         site. Make sure to insert the correct virtual IP addresses
         (<systemitem>IPaddr2</systemitem>) for each site, otherwise the
         booth mechanism will not work correctly. Booth works with both IPv4 and IPv6 addresses.
         <!--taroth 2014-08-21: https://fate.suse.com/316122: booth should support IPv6 in full (prio:
          important-->
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.ticket">
        <para>
         Defines the ticket to be managed by the booth. For each ticket, add
         a <literal>ticket</literal> entry.
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.expiry">
        <para>
         Optional parameter. Defines the ticket's expiry time in seconds. A
         site that has been granted a ticket will renew the ticket
         regularly. If booth does not receive any information about
         renewal of the ticket within the defined expiry time, the ticket
         will be revoked and granted to another site. If no expiry time is
         specified, the ticket will expire after <literal>600</literal>
         seconds by default.
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.expiry">
        <para>
         Optional parameter. Defines the ticket's expiry time in seconds. A
         site that has been granted a ticket will renew the ticket
         regularly. If booth does not receive any information about
         renewal of the ticket within the defined expiry time, the ticket
         will be revoked and granted to another site. If no expiry time is
         specified, the ticket will expire after <literal>600</literal>
         seconds by default.
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.timeout">
        <para>
         Optional parameter. 
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.retries">
        <para>
         Optional parameter. 
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.renewal">
        <para>
         Optional parameter. 
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.handler">
        <para>
         Optional parameter. 
        </para>
       </callout>
      </calloutlist>
     </example>
     <para>
      An example booth configuration file is available at
      <filename>/etc/booth/booth.conf.example</filename>.
     </para>
    </step>
    <step>
     <para>
      Verify your changes and save the file.
     </para>
    </step>
    <step>
     <para>
      Copy <filename>/etc/booth/booth.conf</filename> to all sites and
      arbitrators. In case of any changes, make sure to update the file
      accordingly on all parties.
     </para>
     <note>
      <title>Synchronize Booth Configuration to All Sites and Arbitrators</title>
      <para>
      <remark>phil 2013-12: Please see the (upcoming) multi-cluster csync2 howto
       (https://mailman.suse.de/mailman/private/ha-devel/2014-January/002902.html)
       - taroth 2014-08-11: todo - https://fate.suse.com/316223: [docu] sync and
       change config files (prio: mandatory)</remark>
       All cluster nodes and arbitrators within the &geo; cluster must
       use the same booth configuration. While you may need to copy the
       files manually to the arbitrators and to one cluster node per site,
       you can use &csync; within each cluster site to synchronize the file
       to all nodes.
      </para>
     </note>
    </step>
   </procedure>
   <procedure id="pro.ha.geo.setup.booth.service">
    <title>Starting the Booth Services</title>
    <para/>
    <step>
     <para>
      Start the booth resource group on each other cluster site. It will
      start one instance of the booth service per site.
     </para>
    </step>
    <step>
     <para>
      Log in to each arbitrator and start the booth service:
     </para>
     <remark>taroth 2014-08-07: todo -https://fate.suse.com/316125: booth should
      include systemd service files instead of init scripts</remark>
      <!--<remark>toms 2014-03-04: FIXME: Not available in SLE12 HA Beta1 anymore; 
        package booth contains still rcbooth-arbitrator script</remark>-->
<screen>&prompt.root;<command>/etc/init.d/booth-arbitrator</command> start</screen>
    <para> This starts the booth service in arbitrator mode. It can communicate with all other booth
     daemons but in contrast to the booth daemons running on the cluster sites, it cannot be granted
     a ticket. Booth arbitrators take part in elections only. Otherwise, they are dormant.</para>
    </step>
   </procedure>
   <!--<para>
    After finishing the booth configuration and starting the booth services,
    you are now ready to start the ticket process.
   </para>-->
  </sect1>
  <sect1 id="sec.ha.geo.rsc.">
   <title>Configuring Cluster Resources and Constraints</title>
   <para>
    Apart from the resources and constraints that you need to define for
    your specific cluster setup, &geo; clusters require additional
    resources and constraints as described below. Instead of configuring
    them with &crmshell; (&crmsh;), you can also do so with the &haweb; (&hawk;). 
   </para>
     
   <sect2 id="sec.ha.geo.rsc.cli">
      <title>From Command Line</title>
   
   <procedure id="pro.ha.geo.setup.rsc.constraints">
    <title>Configuring Ticket Dependencies</title>
    <para>
     The <command>crm configure rsc_ticket</command> command lets you
     specify the resources depending on a certain ticket. Together with the
     constraint, you can set a <literal>loss-policy</literal> that defines
     what should happen to the respective resources if the ticket is
     revoked. The attribute <literal>loss-policy</literal> can have the
     following values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>fence</literal>: Fence the nodes that are running the
       relevant resources.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>stop</literal>: Stop the relevant resources.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>freeze</literal>: Do nothing to the relevant resources.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>demote</literal>: Demote relevant resources that are running
       in <literal>master</literal> mode to <literal>slave</literal> mode.
      </para>
     </listitem>
    </itemizedlist>
    <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
      <!--<remark>toms 2011-10-04: Shorten it to "Start a root shell on one
       of the cluster nodes"? I guess we deal with admin-minded people here</remark>
       taroth 2011-10-06: due to new ACL support of the crm shell, also
       non-root users may have the rights to do so, therefore phrased it like
       that -->
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      shell.
     </para>
    </step>
    <step id="step.ha.geo.setup.rsc.constraints">
     <para>
      Configure a constraint that defines which resources depend on a
      certain ticket. For example:
     </para>
     <screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-ticket-A ticket-A: rsc1 loss-policy="fence"</screen>
     <para>
      This creates a constraint with the ID
      <literal>rsc1-req-ticket-A</literal>. It defines that the resource
      <literal>rsc1</literal> depends on <literal>ticket-A</literal> and that
      the node running the resource should be fenced in case
      <literal>ticket-A</literal> is revoked.
     </para>
     <para>
      If resource <literal>rsc1</literal> was not a primitive, but a special
      clone resource that can run in <literal>master</literal> or
      <literal>slave</literal> mode, you may want to configure that only
      <literal>rsc1</literal>'s master mode depends on
      <literal>ticket-A</literal>. With the following configuration,
      <literal>rsc1</literal> is automatically demoted to
      <literal>slave</literal> mode if <literal>ticket-A</literal> is
      revoked:
     </para>
     <screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-ticket-A ticket-A: rsc1:Master loss-policy="demote"</screen>
    </step>
    <step>
     <para>
      If you want other resources to depend on further tickets, create as
      many constraints as necessary with <command>rsc_ticket</command>.
     </para>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
     <para>
      The constraints are saved to the CIB.
     </para>
     <!--For example, the command in
      <xref linkend="step.ha.geo.setup.rsc.constraints" xreflabel="nopage"/> would result in
      the following constraint configuration in the CIB:
      <screen>&lt;rsc_ticket id="rsc1-req-ticket-A" rsc="rsc1" role="Master" ticket="ticket-A" loss-policy="fence"/></screen>-->
    </step>
   </procedure>
   <procedure id="pro.ha.geo.setup.rsc.boothd">
    <title>Configuring a Resource Group for <systemitem class="daemon"
     >boothd</systemitem></title>
    <para>
     Each site needs to run one instance of
     <systemitem class="daemon"
      >boothd</systemitem> that communicates
     with the other booth daemons. The daemon can be started on any node,
     therefore it should be configured as primitive resource. To make the
     <systemitem>boothd</systemitem> resource stay on the same node, if
     possible, add resource stickiness to the configuration. As each daemon
     needs a persistent IP address, configure another primitive with a
     virtual IP address. Group booth primitives:
    </para>
    <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      shell.
     </para>
    </step>
    <step>
     <para>
      To create both primitive resources and to add them to one group,
      <literal>g-booth</literal>:
     </para>
     <screen>&prompt.crm.conf;<command>primitive</command> booth-ip ocf:heartbeat:IPaddr2 params ip="<replaceable>IP_ADDRESS</replaceable>"
&prompt.crm.conf;<command>primitive</command> booth ocf:pacemaker:booth-site \
  meta resource-stickiness="INFINITY" \
  op monitor interval="10s" timeout="20s"
  group g-booth booth-ip booth</screen>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
    </step>
    <step>
     <para>
      Repeat the resource group configuration on the other cluster sites,
      using a different IP address for each <literal>boothd</literal>
      resource group.
     </para>
     <para>
      With this configuration, each booth daemon will be available at its
      individual IP address, independent of the node the daemon is running
      on.
     </para>
    </step>
   </procedure>
   <!--taroth 2012-02-14: fix for bnc#746863-->
   <procedure id="pro.ha.geo.setup.rsc.order">
    <title>Adding an Ordering Constraint</title>
    <para>
     If a ticket has been granted to a site but all nodes of that site
     should fail to host the <systemitem class="daemon">boothd</systemitem>
     resource group for any reason, a <quote>split-brain</quote> situation
     among the geographically dispersed sites could occur. In that case, no
     <systemitem class="daemon">boothd</systemitem> instance would be
     available to safely manage fail-over of the ticket to another site. To
     avoid a potential concurrency violation of the ticket (the ticket is
     granted to multiple sites simultaneously), add an ordering constraint:
    </para>
    <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      shell.
     </para>
    </step>
    <step>
     <para>
      Create an ordering constraint:
     </para>
     <screen>&prompt.crm.conf;<command>order</command> order-booth-rsc1 inf: g-booth rsc1</screen>
     <para>
      This defines that <literal>rsc1</literal> (that depends on
      <literal>ticket-A</literal>) can only be started after the
      <literal>g-booth</literal> resource group.
     </para>
     <para>
      In case <literal>rsc1</literal> is not a primitive, but a special
      clone resource and configured as described in
      <xref
       linkend="step.ha.geo.setup.rsc.constraints"/>, the
      ordering constraint should be configured as follows:
     </para>
     <screen>&prompt.crm.conf;<command>order</command> order-booth-rsc1 inf: g-booth rsc1:promote</screen>
     <para>
      This defines that <literal>rsc1</literal> can only be promoted to
      master mode after the <literal>g-booth</literal> resource group has
      started.
     </para>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      For any other resources that depend on a certain ticket, define
      further ordering constraints.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
    </step>
   </procedure>
   </sect2>
   <sect2 id="sec.ha.geo.rsc.hawk">
  <title>With the &haweb; (&hawk;)</title>
   <para>
    <remark>taroth 2013-12-13: todo - the following had covered only one of
     three steps - still missing: resource group and ordering constraint
     (specific for GEO stuff)</remark>
    This section focuses on ticket dependencies only as they are specific to
    &geo; clusters. For general instructions on how to configure
    resource groups and order constraints with &hawk;, refer to the &haguide;,
    chapter <citetitle>Configuring and Managing Cluster Resources (Web
     Interface)</citetitle>.
     </para>
   <procedure id="sec.ha.config.hawk.geo.rsc.constraints">
    <title>Configuring Ticket Dependencies</title>
    <para>
     For &geo; clusters, you can specify which resources depend on a
     certain ticket. Together with this special type of constraint, you can
     set a <literal>loss-policy</literal> that defines what should happen to
     the respective resources if the ticket is revoked. The attribute
     <literal>loss-policy</literal> can have the following values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>fence</literal>: Fence the nodes that are running the
       relevant resources.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>stop</literal>: Stop the relevant resources.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>freeze</literal>: Do nothing to the relevant resources.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>demote</literal>: Demote relevant resources that are running
       in <literal>master</literal> mode to <literal>slave</literal> mode.
      </para>
     </listitem>
    </itemizedlist>
    <step>
     <para>
      Start a Web browser and log in to the cluster. <!--as described in
       <xref
       linkend="sec.ha.config.hawk.intro.connect"/>.-->
     </para>
    </step>
    <step>
     <para>
      In the left navigation bar, select <guimenu>Constraints</guimenu>. The
      <guimenu>Constraints</guimenu> screen shows categories for all types
      of constraints and lists all defined constraints.
     </para>
    </step>
    <step>
     <para>
      To add a new ticket dependency, click the plus icon in the
      <guimenu>Ticket</guimenu> category.
     </para>
     <para>
      To modify an existing constraint, click the wrench icon next to the
      constraint and select <guimenu>Edit Constraint</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter a unique <guimenu>Constraint ID</guimenu>. When modifying
      existing constraints, the ID is already defined.
     </para>
    </step>
    <step>
     <para>
      Set a <guimenu>Loss Policy</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter the ID of the ticket that the resources should depend on.
     </para>
    </step>
    <step>
     <para>
      Select a resource from the list <guimenu>Add resource to
       constraint</guimenu>. The list shows the IDs of all resources and all
      resource templates configured for the cluster.
     </para>
    </step>
    <step>
     <para>
      To add the selected resource, click the plus icon next to the list. A
      new list appears beneath, showing the remaining resources. Add as many
      resources to the constraint as you would like to depend on the ticket.
     </para>
     <figure id="fig.hawk.ticket.dep.simple">
      <title>&hawk;&mdash;Example Ticket Dependency</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="hawk-ticket-dependency1.png" width="50%"
         format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="hawk-ticket-dependency1.png" width="50%"
         format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      <xref linkend="fig.hawk.ticket.dep.simple"/> shows a constraint with
      the ID <literal>rsc1-req-ticket-A</literal>. It defines that the
      resource <literal>rsc1</literal> depends on <literal>ticket-A</literal>
      and that the node running the resource should be fenced in case
      <literal>ticket-A</literal> is revoked.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Create Constraint</guimenu> to finish the
      configuration. A message at the top of the screen shows if the
      constraint was successfully created.
     </para>
    </step>
   </procedure>
   <!-- <para> If resource <literal>rsc1</literal> was not a primitive, but a special
    clone resource that can run in <literal>master</literal> or
    <literal>slave</literal> mode, you may want to configure that only
    <literal>rsc1</literal>'s master mode depends on
    <literal>ticket-A</literal>. With the configuration shown in <xref
    linkend="fig.hawk.ticket.dep.adv"/>, <literal>rsc1</literal> is
    automatically demoted to <literal>slave</literal> mode if
    <literal>ticket-A</literal> is revoked: </para>-->
   <!--taroth 2012-03-13: for some strange reasons, did not manage to create a 
    master-slave resource with Hawk, always got: sorry, but s.th. went wrong-->
   <!--<figure id="fig.hawk.ticket.dep.adv">
    <title>Example Ticket Dependency</title>
    <mediaobject>
    <imageobject role="fo">
    <imagedata fileref="hawk-ticket-dependency1.png" width="60%" format="PNG"
    />
    </imageobject>
    <imageobject role="html">
    <imagedata fileref="hawk-ticket-dependency1.png" width="50%" format="PNG"
    />
    </imageobject>
    </mediaobject>
    </figure>-->
 </sect2>
  </sect1>
 
  <sect1 id="sec.ha.geo.manage">
   <title>Managing &geo; Clusters</title>
   <para> Before booth can manage a certain ticket within the &geo;
    cluster, you initially need to grant it to a site manually.
    <remark>taroth 2013-12-13: work in progress, check
     if everything that can be done with CLI can also be done with Hawk (keep
     both sections in sync, if possible)</remark>
   </para>
   <sect2 id="sec.ha.geo.manage.cli">
 <title>From Command Line</title>
   
   <para>Use the <command>booth&nbsp;client</command> command line tool to grant, list, or
    revoke tickets as described in <xref linkend="vl.ha.booth.client.cmds"/>. The
    <command>booth&nbsp;client</command> commands can be run on any machine in the cluster, not
    only the ones having the &boothd; running. The <command>booth&nbsp;client</command>
    commands try to find the <quote>local</quote> cluster by looking at the booth configuration file
    and the locally defined IP addresses. If you do not specify a site which the booth client should
    connect to (using the <option>-s</option> option), it will always connect to the local site. </para>
    <note>
    <title>Syntax Changes</title>
    <para>The syntax of booth clients commands has been simplified since &productname; 11: For
     example, the <literal>client</literal> keyword can be omitted for <option>list</option>,
      <option>grant</option>, or <option>revoke</option> operations: <command>booth list</command>.
     Also, the <option>-t</option> option can be omitted when specifying a ticket. </para>
    <para>The former syntax is still supported. For detailed information, see the
      <literal>Synopsis</literal> section in the booth man page. However, the examples in this
     manual use the simplified syntax. </para>
    </note>

   <!--taroth 2013-04-24: information taken from bnc#752601, c#17-->
    <variablelist id="vl.ha.booth.client.cmds">
    <title>Overview of <command>booth client</command> Commands</title>
    <varlistentry>
     <term>Listing All Tickets</term>
     <listitem>
      <screen>&prompt.root; <command>booth</command> list
ticket: ticket-A, leader: none
ticket: ticket-B, leader: 10.2.12.101, expires: 2014-08-13 10:28:57
      </screen>
      <para>If you do not specify a certain site with <option>-s</option>, the information about the
       tickets will be requested from the local booth instance.</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Granting a Ticket to a Site</term>
     <listitem>
      <screen>&prompt.root; <command>booth</command> grant -s 147.2.207.14 ticket-A
booth[27891]: 2014/08/13_10:21:23 info: grant request sent, waiting for the result ...
booth[27891]: 2014/08/13_10:21:23 info: grant succeeded!</screen>
      <para> In this case, <literal>ticket-A</literal> will be granted to the site
        <literal>147.2.207.14</literal>. If you omit the <option>-s</option> option, booth will
       automatically connect to the current site (the site you are running the booth client on) and
       will request the <command>grant</command> operation. </para>
      <para> Before granting a ticket, the command will execute a sanity check. If the same ticket
       is already granted to another site, you will be warned about that and be prompted to revoke
       the ticket from the current site first. </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Revoking a Ticket From a Site</term>
     <listitem>
      <screen>&prompt.root; <command>booth</command> client revoke -s 147.2.207.14 ticket-A
booth[27900]: 2014/08/13_10:21:23 info: revoke succeeded!</screen>
      <para> In this case, <literal>ticket-A</literal> will be revoked from the
       site <literal>147.2.207.14</literal>. If you omit the <option>-s</option> option, booth will
       automatically connect to the current site (the site you are running the booth client
       on) and will request the <command>revoke</command> operation.  The revoke operation will be
       executed immediately. However, it might not be finished yet when the
       message above appears on the screen. Find the exact status in the log
       files. </para>
     </listitem>
    </varlistentry>
   </variablelist>

   <para>The <command>grant</command> and, under certain circumstances, <command>revoke</command>
    operations may take a while to return a definite operation's outcome. The client will wait for
    the result up to the ticket's <varname>timeout</varname> value before he gives up
    waiting&mdash;unless the <option>-w</option> option was used, in which case the client waits
    indefinitely. Find the exact status in the log files or with the
    <command>crm_ticket -L</command> command.</para>


   <warning>
    <title><command>crm_ticket</command> and
     <command>crm&nbsp;site&nbsp;ticket</command></title>
    <para> In case the booth service is not running for any reasons, you may
     also manage tickets manually with <command>crm_ticket</command> or
     <command>crm&nbsp;site&nbsp;ticket</command>. Both commands are
     only available on cluster nodes. In case of manual intervention, use them
     with great care as they <emphasis>cannot</emphasis> verify if the same
     ticket is already granted elsewhere. For basic information about the
     commands, refer to their man pages. </para>
    <para> As long as booth is up and running, only use
     <command>booth&nbsp;client</command> for manual intervention. </para>
   </warning>

   <para> After you have initially granted a ticket to a site, the booth
    mechanism will take over and manage the ticket automatically. If the site
    holding a ticket should be out of service, the ticket will automatically be
    revoked after the expiry time and granted to another site. The resources
    that depend on that ticket will fail over to the new site holding the
    ticket. The nodes that have run the resources before will be treated
    according to the <literal>loss-policy</literal> you set within the
    constraint. </para>
    <remark>Phil 2013-12: TODO-  Mostly booth should be used to avoid split-brains.
     Whether/how automatic failover should/does happen is TBD.
    </remark>

   <!--taroth 2013-04-24: fix for bnc#752601, c#10-->

   <procedure id="pro.ha.geo.manage.tickets">
    <title>Managing Tickets Manually</title>
    <para> Assuming that you want to manually move <literal>ticket-A</literal>
     from site <literal>147.2.207.14</literal> to
     <literal>&smbip;</literal>, proceed as follows: </para>
    <step>
     <para> Set <literal>ticket-A</literal> to standby with the following
      command: </para>
     <screen>&prompt.root;<command>crm_ticket</command> -t ticket-A -s</screen>
    </step>
    <step>
     <para> Wait for any resources that depend on <literal>ticket-A</literal> to
      be stopped or demoted cleanly. </para>
    </step>
    <step>
     <para> Revoke <literal>ticket-A</literal> from its current site with: </para>
     <screen>&prompt.root;<command>booth</command> revoke -s 147.2.207.14 ticket-A</screen>
    </step>
    
    <step>
     <para> After the ticket has been revoked from its original site, grant it
      to the new site with: </para>
     <screen>booth grant -s &smbip; ticket-A</screen>
    </step>
   </procedure>
    <remark>phil 2013-12: Will/Should get a cleaner, more integrated workflow - "booth client move -t ticket -s new-site"
     or something similar. - dmuhamedagic 2014-08-13: N/A</remark>
    </sect2>
  
  <sect2 id="sec.ha.geo.manage.hawk">
   <title>With the &haweb; (&hawk;)</title>
  <para>
    <remark>taroth 2013-12-13: work in progress, add introductory para and check
     if everything that can be done with CLI can also be done with Hawk (keep
     both sections in sync, if possible), todo - https://fate.suse.com/316119:
     hawk: display GEO setup (prio: important)</remark>
   </para>
   <sect3 id="sec.ha.config.hawk.geo.ticket">
    <title>Viewing Tickets</title>
    <procedure id="pro.ha.config.hawk.viewtickets">
     <title>Viewing Tickets with &hawk;</title>
     <para>
      Tickets are visible in &hawk; if they have been granted or revoked at
      least once or if they are referenced in a ticket dependency&mdash;see
      <xref
       linkend="sec.ha.config.hawk.geo.rsc.constraints"/>. In case
      a ticket is referenced in a ticket dependency, but has not been granted
      to any site yet, &hawk; displays it as <literal>revoked</literal>.
     </para>
     <step>
      <para>
       Start a Web browser and log in to the cluster.<!-- as described in
        <xref
        linkend="sec.ha.config.hawk.intro.connect"/>.-->
      </para>
     </step>
     <step>
      <para>
       In the left navigation bar, select <guimenu>Cluster Status</guimenu>.
      </para>
     </step>
     <step>
      <para>
       If the <guimenu>Summary View</guimenu> is not already active, click
       the respective view icon on the upper right-hand side. Along with
       information about cluster nodes and resources, &hawk; also displays a
       <guimenu>Ticket</guimenu> category.
      </para>
     </step>
     <step>
      <para>
       For more details, either click the title of the
       <guimenu>Ticket</guimenu> category or the individual ticket entries
       that are marked as links. &hawk; displays the ticket's name and, in a
       tooltip, the last time the ticket has been granted to the current
       site.
      </para>
      <figure>
       <title>&hawk; Cluster Status (Summary View)&mdash;Ticket Details</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk-summary-tickets.png" width="100%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk-summary-tickets.png" width="80%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
    </procedure>
    <note>
     <title>Managing Tickets</title>
     <para>
      To grant or revoke tickets, use the <command>booth client</command>
      command as described in <xref linkend="sec.ha.geo.manage.cli"/>. As
      managing tickets takes place on an <quote>inter-cluster</quote> layer,
      you cannot do so with &hawk;.
     </para>
    </note>
   </sect3>
   
   <sect3 id="sec.ha.config.hawk.geo.simulator">
    <title>Testing the Impact of Ticket Failover</title>
    <para>
     &hawk;'s <guimenu>Simulator</guimenu> allows you to explore failure
     scenarios before they happen. To explore if your resources that depend
     on a certain ticket behave as expected, you can also test the impact of
     granting or revoking tickets.
    </para>
    <procedure id="pro.ha.config.hawk.geo.simulator">
     <title>Simulating Granting and Revoking Tickets</title>
     <step>
      <para>
       Start a Web browser and log in to the cluster. <!--as described in
        <xref
        linkend="sec.ha.config.hawk.intro.connect"/>.-->
      </para>
     </step>
     <step>
      <para>
       Click the wrench icon next to the username in the top-level row, and
       select <guimenu>Simulator</guimenu>.
      </para>
      <para>
       &hawk;'s background changes color to indicate the simulator is active.
       A simulator dialog opens in the bottom right hand corner of the
       screen. Its title <guimenu>Simulator (initial state)</guimenu>
       indicates that <guimenu>Cluster Status</guimenu> screen still reflects
       the current state of the cluster.
      </para>
     </step>
     <step>
      <para>
       To simulate status change of a ticket:
      </para>
      <substeps>
       <step>
        <para>
         Click <guimenu>+Ticket</guimenu> in the simulator control dialog.
        </para>
       </step>
       <step>
        <para>
         Select the <guimenu>Action</guimenu> you want to simulate.
        </para>
       </step>
       <step>
        <para>
         Confirm your changes to add them to the queue of events listed in
         the controller dialog below <guimenu>Injected State</guimenu>.
        </para>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       To start the simulation, click <guimenu>Run</guimenu> in the simulator
       control dialog. The <guimenu>Cluster Status</guimenu> screen displays
       the impact of the simulated events. The simulator control dialog
       changes to <guimenu>Simulator (final state)</guimenu>.
      </para>
     </step>
     <step>
      <para>
       To exit the simulation mode, close the simulator control dialog. The
       <guimenu>Cluster Status</guimenu> screen switches back to its normal
       color and displays the current cluster state.
      </para>
     </step>
    </procedure>
    <figure>
     <title>&hawk;Simulator&mdash;Tickets</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk-simulator-tickets.png" width="100%" format="PNG"/>
      </imageobject>  
      <imageobject role="html">
       <imagedata fileref="hawk-simulator-tickets.png" width="80%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     For more information about &hawk;'s <guimenu>Simulator</guimenu> (and
     which other scenarios can be explored with it), refer to the <citetitle>&haguide;</citetitle> for
     &productname;, available from &suse-onlinedoc;. Refer to chapter
     <citetitle>Configuring and Managing Cluster Resources (Web Interface)</citetitle>, section
     <citetitle> Exploring Potential Failure Scenarios</citetitle>.</para>
   </sect3>
  </sect2>
</sect1>
<sect1 id="sec.ha.geo.trouble">
<!--taroth 2013-04-24: fix for bnc#753625-->

  <title>Troubleshooting</title>

  <para>
   Booth <!--logs to <filename>/var/log/messages</filename> and -->uses the same
   logging mechanism as the CRM. Thus, changing the log level will also take
   effect on booth logging. The booth log messages also contain information
   about any tickets.
  </para>

  <para>
   Both the booth log messages and the booth configuration file are included
   in the <command>hb_report</command> and <command>crm_report</command>.
  </para>

  <para> In case of unexpected booth behavior or any problems, check the logging
   data with <command>sudo journalctl -n</command> or create a detailed cluster
   report with either <command>hb_report</command> or
   <command>crm_report</command>. </para>
 </sect1>
</article>
