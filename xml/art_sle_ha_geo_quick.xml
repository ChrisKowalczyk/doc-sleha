<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml" 
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--taroth 2014-12-18: todo for next release: fix IDs in xincludes-->
<?provo dirname="geo_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art.ha.geo.quick"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <title>&geoquick;</title>
 <subtitle>&productname; &productnumber;</subtitle>
 <info>
  <productnumber>&productnumber;</productnumber>
   <productname>&productname;</productname>
   <date><?dbtimestamp?></date>
    <abstract>
    <para>
     &abstract-geoquick;
     This document guides you through the basic setup of a &geo; cluster,
     using the &geo; cluster bootstrap scripts provided by the
      <systemitem xmlns='http://docbook.org/ns/docbook'
        class='resource'>ha-cluster-bootstrap</systemitem> package.
      </para>
      <para>
        <remark>
          taroth 2017-06-22: todo - more details? restrictions?
        </remark>
      </para>
      </abstract>
      <dm:docmanager>
        <dm:bugtracker>
          <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
          <dm:product>SUSE Linux Enterprise High Availability Extension 12 SP3</dm:product>
          <dm:component>GEO Clustering</dm:component>
        </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
 </info>
  <sect1 xml:id="sec.ha.geo.quick.concept">
    <title>Conceptual Overview</title>
    <para>
      &geo; clusters based on &productname; can be considered
      <quote>overlay</quote> clusters where each cluster site corresponds to a
      cluster node in a traditional cluster. The overlay cluster is managed by
      the booth mechanism. Each of the parties involved in a &geo; cluster runs
      a service, the &boothd;. It connects to the booth daemons running at the
      other sites and exchanges connectivity details. For making cluster
      resources highly available across sites, booth relies on cluster objects
      called tickets. A ticket grants the right to run certain resources on a
      specific cluster site. Booth guarantees that every ticket is granted to no
      more than one site at a time.
    </para>
    <para>
    If the communication between two booth instances breaks down, it might be
    due to a network breakdown between the cluster sites <emphasis>or</emphasis>
    to an outage of one cluster site). In this case, you need an additional instance
    (an arbitrator) to reach consensus about decisions such as failover of resources
    across sites. Arbitrators are single machines (outside of the clusters) that
    run a booth instance in a special mode. Each &geo; cluster can have one or
    multiple arbitrators. They are especially important for &geo; cluster
    setups with an even number of sites.
    </para>
    <para>
      <remark>taroth 2017-06-27: todo: fine-tune graphic</remark>
    </para>
    <figure xml:id="fig.ha.geo.quick.example.geosetup">
      <title>Two-Site Cluster (2x2 Nodes + Arbitrator)</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="ha_geocluster.svg" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="ha_geocluster.png" width="85%" format="PNG"/>
        </imageobject>
      </mediaobject>
    </figure>
    <para>
      For more details on components and ticket management for &geo; clusters,
      see .<remark>FIXME - taroth 2017-06-27: add link to
      conceptual overview in Geo  Guide</remark>
    </para>
  </sect1>
  <sect1 xml:id="sec.ha.geo.quick.scenario">
    <title>Usage Scenario</title>
    <para>
      The procedures in this document will lead to an example setup of a
      &geo; cluster with the following properties:
    </para>
    <example>
      <title>Example Setup of a &geo; Cluster</title>
      <para> A &geo; cluster with two sites,
          <literal>&cluster1;</literal> and
          <literal>&cluster2;</literal>, including one arbitrator. </para>
    </example>
    <para>
      Currently, the &geo; cluster bootstrap script only takes care of setting up
      booth and of creating the minimum set of resources that booth needs.
      <remark>taroth 2017-06-26:
        add link to description of bootstrap scripts</remark>
    </para>
    <para>
      In addition, you need to take the following steps to get a functioning
      &geo; cluster:
      <remark>taroth 2017-06-26: @krig, could you help to provide a list of things that
        the admin needs to execute before/after running the init scripts (in order
        to get a functioning geo cluster)?</remark>
    </para>
  </sect1>
  <sect1 xml:base="sec.ha.geo.quick.req">
    <title>Requirements</title>
    <para>
      <remark>taroth 2017-06-27: todo - for future, use phrases entities like
      in Install Quick</remark>
    </para>
     <itemizedlist>
      <title>Software Requirements</title>
      <listitem>
        <para>
          All machines (cluster nodes and arbitrators) that will be part of the &geo; cluster must be based on
          &productname; &productnumber; and must have the following software installed:
        </para>
       <itemizedlist>
        <listitem>
        <para>
          &slsreg; &productnumber;
        </para>
        </listitem>
        <listitem>
         <para>
          &productname; &productnumber;
         </para>
        </listitem>
        <listitem>
          <para>
           &hageo; &productnumber;
           <!--<remark>taroth 2014-08-20: booth package would be enough (GEO pattern only consists of the booth
             package) but to make it clear where to get the package from, phrasing like this</remark>-->
          </para>
         </listitem>
       </itemizedlist>
   </listitem>
    </itemizedlist>
     <itemizedlist>
      <title>Network Requirements</title>
      <listitem>
        <para>
          Each cluster site has a private network routed to the other site:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              &cluster1;: <literal>192.168.201.x</literal>
            </para>
          </listitem>
          <listitem>
            <para>
              &cluster2;: <literal>192.168.202.x</literal>
            </para>
          </listitem>
        </itemizedlist>
      </listitem>
      <listitem>
        <para>
          The sites must be reachable on one UDP and TCP port per booth instance.
          That means any firewalls or IPsec tunnels in between must be configured
          accordingly.
        </para>
      </listitem>
      <listitem>
        <para>
          Other setup decision may require to open more ports (for example, for
          DRBD or database replication).
        </para>
      </listitem>
    </itemizedlist>
    
    <itemizedlist>
      <title>Other Requirements and Recommendations</title>
      <listitem>
        <para>
          All cluster nodes on all sites should synchronize to an NTP server
          outside the cluster. For more information, see the
          <citetitle>&admin;</citetitle> for &sls; &productnumber;,
          available at <link xlink:href="http://www.suse.com/documentation/"/>. Refer to the chapter <citetitle>Time
            Synchronization with NTP</citetitle>.
        </para>
        <para>
          If nodes are not synchronized, log files and cluster reports are very
          hard to analyze.
        </para>
      </listitem>
      <listitem>
        <para>The cluster on each site has a meaningful name, for example:
          <literal>&cluster1;</literal> and
          <literal>&cluster2;</literal>.
        </para>
        <para>
          The cluster names for each site are defined in the respective &corosync.conf; files:
        </para>
        <screen>totem {
      [...]
      cluster_name: &cluster1;
      }
        </screen>
        <para>Set the cluster name during the initial setup of a cluster (with
         <command>ha-cluster-init -n <replaceable>CLUSTERNAME</replaceable></command>), or manually
         (by editing &corosync.conf;). Alternatively, set it with the
          &yast; cluster module (by switching to the <guimenu>Communication Channels</guimenu>
          category and defining a <guimenu>Cluster Name</guimenu>).
          For the changes to take effect, restart the &pace; service afterwards.
        </para>
      </listitem>
    </itemizedlist>
  </sect1>
  <sect1 xml:id="sec.ha.geo.scripts">
    <title>Overview of the Bootstrap Scripts</title>
    <para>
      All commands from the <package>ha-cluster-bootstrap</package> package
      execute bootstrap scripts that require only a minimum of time and manual
      intervention.
    </para>
    <itemizedlist>
      <listitem>
        <para>
          With <command>ha-cluster-geo-init</command>, turn a cluster into the first
          site of a &geo; cluster. The script creates &booth.conf; (by taking
          some parameters like the names of the clusters, an arbitrator, and one or
          multiple ticket). On the first &geo; cluster site, it also configures
          the cluster resources needed for booth.
        </para>
       <para>
        For details, see <xref linkend="sec.ha.geo.quick.ha-cluster-geo-init"/>.
       </para>
      </listitem>
      <listitem>
        <para>
          With <command>ha-cluster-geo-join</command>, add the current cluster to an
          existing &geo; cluster. The script copies the booth configuration from an
          existing cluster site and writes it to &booth.conf;. On the current
          cluster site, it configures the cluster resources needed for booth.
        </para>
       <para>
        For details, see <xref linkend="sec.ha.geo.quick.ha-cluster-geo-join"/>.
       </para>
      </listitem>
      <listitem>
        <para>
          With <command>ha-cluster-geo-init-arbitrator</command>, turn the current
         machine into an arbitrator for the &geo; cluster. The script copies the
         booth configuration from an existing cluster site and writes it to &booth.conf;.
        </para>
       <para>
        For details, see <xref linkend="sec.ha.geo.quick.ha-cluster-geo-init-arbitrator"/>.
       </para>
      </listitem>
    </itemizedlist>
    <para>
      All bootstrap scripts log to <filename>/var/log/ha-cluster-bootstrap.log</filename>.
      Check this file for any details of the bootstrap process. Any options set
      during the bootstrap process can be modified later.
    </para>
    <!--taroth 2017-07-03: the following is not really true by now, the man pages
     are rather basic ATM:
     <para>Each script comes with a man page covering the range of functions, the
      script's options, and an overview of the files the script can create and modify.
    </para>-->
  </sect1>
  <sect1 xml:id="sec.ha.geo.quick.inst">
    <title>Installation as Extension</title>
    <para>
      For setting up a &geo; cluster you need the packages included in
      the following installation patterns:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <literal>&ha;</literal>
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>&geo; Clustering for &ha;</literal>
        </para>
      </listitem>
    </itemizedlist>
    <para>
      Both patterns are only available if you have registered your system at
      &scc; (or a local registration server) and have added the respective
      product channels or installation media as an extension. For information on how
      to install extensions, see the <citetitle>&sle; &productnumber;
        &deploy;</citetitle>, available at <link xlink:href="http://www.suse.com/documentation/"/>. Refer to
      chapter <citetitle>Installing Modules, Extensions, and Third Party Add-On Products</citetitle>.
      <!--taroth: need to use hard-coded link here as the target is not included in the same set-->
    </para>
    <procedure xml:id="pro.ha.geo.inst">
      <title>Installing the Packages</title>
      <step>
        <para>
          To install the packages from both patterns via command line, use
          Zypper:
        </para>
        <screen>sudo <command>zypper</command> in -t pattern ha_sles ha_geo</screen>
      </step>
      <step xml:id="step.ha.geo.inst.yast">
        <para>
          Alternatively, use &yast; for a graphical installation:
        </para>
        <substeps performance="required">
          <step>
            <para>
              Start &yast; as &rootuser; user and select <menuchoice>
                <guimenu>Software</guimenu> <guimenu>Software Management</guimenu>
              </menuchoice>.
            </para>
          </step>
          <step>
            <para>
              Click <menuchoice> <guimenu>View</guimenu>
                <guimenu>Patterns</guimenu> </menuchoice> and activate the following
              patterns:
            </para>
            <itemizedlist>
              <listitem>
                <para>
                  <literal>&ha;</literal>
                </para>
              </listitem>
              <listitem>
                <para>
                  <literal>&geo; Clustering for &ha;</literal>
                </para>
              </listitem>
            </itemizedlist>
          </step>
          <step>
            <para>
              Click <guimenu>Accept</guimenu> to start installing the packages.
            </para>
          </step>
        </substeps>
      </step>
    </procedure>
    <important>
      <title>Installing Software Packages on all Parties</title>
      <para>
        The software packages needed for &ha; and &geo; clusters are
        <emphasis>not</emphasis> automatically copied to the cluster nodes.
      </para>
      <itemizedlist>
        <listitem>
          <para>
            Install &sls; &productnumber; and the <literal>&ha;</literal>
            and <literal>&geo; Clustering for &ha;</literal> patterns on
            <emphasis>all</emphasis> machines that will be part of your &geo;
            cluster.
          </para>
        </listitem>
        <listitem>
          <para>
            Instead of manually installing the packages on all machines that
            will be part of your cluster, use &ay; to clone existing nodes.
            Find more information in the <citetitle>&haguide;</citetitle> for
            &productname; &productnumber;, available from
            <link xlink:href="http://www.suse.com/documentation/"/>. Refer to the
            chapter <citetitle>Installation the &hasi;</citetitle>, section
            <citetitle>Mass Installation and Deployment with &ay;</citetitle>.
          </para>
          <para>
           <remark>taroth 2017-06-29: krig, is the following still true?</remark>
            However, the &geo; clustering extension must be installed
           <emphasis>manually</emphasis> on all machines
            that will be part of the &geo; cluster. &ay; support for &hageo; is
            not yet available.
          </para>
        </listitem>
      </itemizedlist>
    </important>
  </sect1>
 <sect1 xml:id="sec.ha.geo.quick.ha-cluster-geo-init">
  <title>Setting Up the First Site of a &geo; Cluster</title>
  <para>
   Use the <command>ha-cluster-geo-init</command> script to turn an existing
   cluster into the first site of a &geo; cluster.
  </para>
  <itemizedlist>
   <title>Requirements</title>
   <listitem>
    <para>You have at least two existing clusters that you want to combine into
     a &geo; cluster.
    </para>
   </listitem>
   <listitem>
    <para>Each cluster has a meaningful <literal>cluster_name</literal> set in
     &corosync.conf;.</para>
   </listitem>
   <listitem>
    <para>You have installed a third machine outside of the existing clusters
     to be used as arbitrator.</para>
   </listitem>
  </itemizedlist>
  <procedure xml:id="pro.ha.geo.quick.ha-cluster-geo-init">
   <title>Setting Up the First Site (<systemitem>&cluster1;</systemitem>) with
    <command>ha-cluster-geo-init</command></title>
   <step>
    <para>
     Define a virtual IP per cluster site that can be used to access the
     site. You do not need to configure the virtual IPs as cluster resources yet.
     This will be done by the bootstrap scripts.</para>
   </step>
   <step>
    <para>
     Define the name of at least one ticket that will grant the right to run
     certain resources on a cluster site. The bootstrap scripts only need the
     ticket name&mdash;you can define the remaining details (ticket dependencies
     of the resources) later on. <!--FIXME: add link-->
    </para>
   </step>
   <step>
    <para>
     Log in to a node of an existing cluster, for example, on node <literal>&node1;</literal>,
     belonging to the cluster <literal>&cluster1;</literal>.
    </para>
   </step>
   <step xml:id="step.ha-cluster-geo-init">
    <para>
     Run the <command>ha-cluster-geo-init</command>command with the
     parameters <option>--clusters</option>, <option>--tickets</option>, and
     <option>--arbitrator</option>. For example:
    </para>
    <screen>ha-cluster-geo-init \
  --clusters<co xml:id="co.geo.init.cluster"/> "amsterdam=10.161.13.176 berlin=10.160.4.81" \
  --tickets<co xml:id="co.geo.init.ticket"/> ticket-nfs \
  --arbitrator<co xml:id="co.geo.init.arbitrator"/> cynthia</screen>
    <calloutlist>
     <callout arearefs="co.geo.init.cluster">
      <para>
       The names of the cluster sites (as defined in &corosync.conf;) and the virtual
       IP addresses you want to use for each cluster site. In this case, we have
       two cluster sites (<literal>&cluster1;</literal> and
       <literal>&cluster2;</literal> with a virtual IP address each).
      </para>
     </callout>
     <callout arearefs="co.geo.init.ticket">
      <para>
       The name of one or multiple tickets. In this case, the ticket's name is
       <literal>ticket-nfs</literal>.
      </para>
     </callout>
     <callout arearefs="co.geo.init.arbitrator">
     <para>
       The host name or IP address of a machine outside of the clusters. In this
       case, the arbitrator's host name is <literal>cynthia</literal>.
       </para>
     </callout>
    </calloutlist>
   </step>
  </procedure>
  <para>The bootstrap script creates the booth configuration file and synchronizes it
   across the cluster sites. It also creates the basic cluster
   resources needed for booth (see <xref linkend="sec.ha.geo.scripts"/>).
  For example, <xref linkend="step.ha-cluster-geo-init" xrefstyle="seletc:label"/>
   of <xref linkend="pro.ha.geo.quick.ha-cluster-geo-init" xrefstyle="select:label"/>
   results in the following booth configuration and cluster resources:</para>
  <example xml:id="ex.geo.init.booth.conf">
   <title>Booth Configuration Created By <command>ha-cluster-geo-init</command></title>
   <screen># The booth configuration file is "/etc/booth/booth.conf". You need to
# prepare the same booth configuration file on each arbitrator and
# each node in the cluster sites where the booth daemon can be launched.

# "transport" means which transport layer booth daemon will use.
# Currently only "UDP" is supported.
transport="UDP"
port="9929"

arbitrator="cynthia"
site="10.161.13.176"
site="10.160.4.81"
authfile="/etc/booth/authkey"
ticket="ticket-nfs"
expire="600"</screen>
  </example>
  <example xml:id="ex.geo.init.rsc.conf">
   <title>Cluster Resources Created By <command>ha-cluster-geo-init</command></title>
   <screen>primitive<co xml:id="co.geo.quick.rsc.booth-ip"/> booth-ip IPaddr2 \
  params rule #cluster-name eq amsterdam ip=10.161.13.176\
  params rule #cluster-name eq berlin ip=10.160.4.81
primitive<co xml:id="co.geo.quick.rsc.booth-site"/> booth-site ocf:pacemaker:booth-site \
  meta resource-stickiness=INFINITY \
  params config=booth \
  op monitor interval=10s
group<co xml:id="co.geo.quick.rsc.g-booth"/> g-booth booth-ip booth-site \
meta target-role=Stopped</screen>
   <calloutlist>
    <callout arearefs="co.geo.quick.rsc.booth-ip">
     <para>
      A primitive resource for the booth daemon. It communicates with the
      booth daemons on the other cluster sites.
     </para>
    </callout>
    <callout arearefs="co.geo.quick.rsc.booth-site">
     <para>A virtual IP address for each cluster site. It is required by the booth
      daemons who need a persistent IP address on each cluster site.
     </para>
    </callout>
    <callout arearefs="co.geo.quick.rsc.g-booth">
     <para>A cluster resource group for both primitives.</para>
    </callout>
   </calloutlist>
  </example>
 </sect1>
 <sect1 xml:id="sec.ha.geo.quick.ha-cluster-geo-join">
  <title>Adding Another Site to a &geo; Cluster</title>
  <para>After you have initialized the first site of your &geo; cluster, add the
   second cluster with the <literal>ha-cluster-geo-join</literal> bootstrap script,
   as described in <xref linkend="pro.ha.geo.quick.ha-cluster-geo-join"
   xrefstyle="select:label"/>. The script needs access to an already configured
   cluster site and will add the current cluster to the &geo; cluster.
  </para>
  <procedure xml:id="pro.ha.geo.quick.ha-cluster-geo-join">
   <title>Adding the Second Site (<literal>&cluster2;</literal>) with
    <command>ha-cluster-geo-join</command></title>
   <step>
    <para>
     Log in to a node of the cluster site you want to add, for example, on node
     <literal>&node3;</literal>, belonging to the cluster <literal>&cluster2;</literal>.
    </para>
   </step>
   <step xml:id="step.ha-cluster-geo-join">
    <para>
     Run the <command>ha-cluster-geo-init</command> command with the following
     parameters.
     The option <option>--cluster-node</option> specifies where to copy the booth
     configuration from. The option <option>--clusters</option> specifies the names
     of the cluster sites (as defined in &corosync.conf;) and the virtual IP
     addresses to be used for each cluster site (see also <xref
     linkend="pro.ha.geo.quick.ha-cluster-geo-init" xrefstyle="select:label"/>.
     For example:
    </para>
    <screen>ha-cluster-geo-init \
  --cluster-node<co xml:id="co.geo.join.cluster-node"/> 10.161.13.176\
  --clusters "amsterdam=10.161.13.176 berlin=10.160.4.81" \
     </screen>
    <calloutlist>
     <callout arearefs="co.geo.join.cluster-node">
      <para>
       The IP address or host name of a node in an already configured &geo;
       cluster site. Alternatively, use the IP address or host name of an already
       configured arbitrator for your &geo; cluster, or the virtual IP address of
       an already existing cluster site.
      </para>
     </callout>
    </calloutlist>
   </step>
  </procedure>
  <para>The <command>ha-cluster-geo-init</command> script copies the booth
  configuration from <xref linkend="co.geo.join.cluster-node" xrefstyle="select:label"/>, see
   <xref linkend="ex.geo.init.booth.conf"/>. In addition, it creates the
   cluster resources need for booth (see <xref linkend="ex.geo.init.rsc.conf"
   xrefstyle="select:label"/>).
  </para>
 </sect1>
 <sect1 xml:id="sec.ha.geo.quick.ha-cluster-geo-init-arbitrator">
  <title>Adding the Arbitrator</title>
  <para>After you have set up all sites of your &geo; cluster with
   <command>ha-cluster-geo-init</command> and <command>ha-cluster-geo-join</command>,
   set up the arbitrator with <command>ha-cluster-geo-init-arbitrator</command>.
  </para>
  <procedure xml:id="pro.ha.geo.quick.ha-cluster-geo-init-arbitrator">
   <title>Setting Up the Arbitrator with <command>ha-cluster-geo-init-arbitrator</command></title>
  <step>
   <para>Log in to the machine you want to use as arbitrator.</para>
  </step>
  <step>
   <para>
    Run the following command with the <option>--cluster-node</option> option.
    It specifies where to copy the booth configuration from. For example:
   </para>
   <screen>ha-cluster-geo-init-arbitrator --cluster-node<co
   xml:id="co.geo.init.arbitrator.cluster-node"/> 10.161.13.176</screen>
   <calloutlist>
    <callout arearefs="co.geo.init.arbitrator.cluster-node">
     <para>
      The IP address or host name of a node in an already configured &geo;
      cluster site. Alternatively, use the virtual IP address of an already
      existing cluster site.
     </para>
    </callout>
   </calloutlist>
  </step>
  </procedure>
  <para>The <command>ha-cluster-geo-init-arbitrator</command> script copies the booth
   configuration from <xref linkend="co.geo.init.arbitrator.cluster-node"/>, see
   <xref linkend="ex.geo.init.booth.conf" xrefstyle="select:label"/>. It also
   enables and starts the booth service on the arbitrator. Thus, the arbitrator
   is ready to communicate with the booth instances on the cluster sites as soon
   as the booth services are running there, too.</para>
 </sect1>
 <!--*********************************************************************** -->
  <sect1 xml:base="sec.ha.geo.quick.rsc">
    <title>Configuring Cluster Resources and Constraints</title>
    <para>
  Apart from the resources and constraints that you need to define for your
  specific cluster setup, &geo; clusters require additional resources and
  constraints as described below. You can either configure them with the
  &crmshell; (&crmsh;) as demonstrated in the examples below, or with
  the &haweb; (&hawk2;).
 </para>

 <para>
  This section focuses on tasks specific to &geo; clusters. For an
  introduction to your preferred cluster management tool and general
  instructions on how to configure resources and constraints with it, refer
  to one of the following chapters in the
  <citetitle>&haguide;</citetitle> for &productname;, available from
  <link xlink:href="http://www.suse.com/documentation/"/>:
 </para>

 <itemizedlist>
  <listitem>
   <para>
    &hawk2;: Chapter <citetitle>Configuring and Managing Cluster Resources
    (Web Interface)</citetitle>
   </para>
  </listitem>
  <listitem>
   <para>
    &crmsh;: Chapter <citetitle>Configuring and Managing Cluster
    Resources (Command Line)</citetitle>
   </para>
  </listitem>
 </itemizedlist>

 <important>
  <title>No CIB Synchronization Across Sites</title>
  <para>
   The CIB is <emphasis>not</emphasis> automatically synchronized across
   cluster sites of a &geo; cluster. This means you need to configure all
   resources that must be highly available across the &geo; cluster for
   each site accordingly.
  </para>
  <para>
   To simplify transfer of the configuration to other cluster sites, any
   resources with site-specific parameters can be configured in such a way
   that the parameters' values depend on the name of the cluster site where
   the resource is running.
  </para>
  <para>
   To make this work, the cluster names for each site must be defined in the
   respective &corosync.conf; files. For example, &corosync.conf; of
   site 1 (<literal>&cluster1;</literal>) must contain the following
   entry:
  </para>
<screen>totem {
   [...]
   cluster_name: &cluster1;
   }</screen>
  <para>
   After you have configured the resources on one site, you can tag the
   resources that are needed on all cluster sites, export them from the
   current CIB, and import them into the CIB of another cluster site. For
   details, see <xref linkend="sec.ha.geo.rsc.sync.cib"/>.
  </para>
 </important>

 <sect2 xml:id="sec.ha.geo.rsc.drbd">
  <title>Resources and Constraints for DRBD</title>
  <para>
   To complete the DRBD setup, you need to configure some resources and
   constraints as shown in
   <xref linkend="pro.ha.geo.rsc.drbd" xrefstyle="select:label"/> and
   transfer them to the other cluster sites as explained in
   <xref linkend="sec.ha.geo.rsc.sync.cib"/>.
  </para>
  <procedure xml:id="pro.ha.geo.rsc.drbd">
   <title>Configuring Resources for a DRBD Setup</title>
   <step>
    <para>
     On one of the nodes of cluster <literal>&cluster1;</literal>, start
     a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Configure the (site-dependent) service IP for NFS as a basic primitive:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip_nfs ocf:heartbeat:IPaddr2 \
  params iflabel="nfs" nic="eth1" cidr_netmask="24"
  params rule #cluster-name eq &cluster1; ip="192.168.201.151" \
  params rule #cluster-name eq &cluster2; ip="192.168.202.151" \
  op monitor interval=10</screen>
   </step>
   <step>
    <para>
     Configure a file system resource and a resource for the NFS server:
    </para>
    <screen>&prompt.crm.conf;<command>primitive</command> nfs_fs ocf:heartbeat:Filesystem \
  params device="/dev/drbd/by-res/nfs/0" directory="/mnt/nfs" \
  fstype="ext4"
&prompt.crm.conf;<command>primitive</command> nfs_service systemd:nfs-server</screen>
   </step>
   <step>
    <para>
     Configure the following primitives and multi-state resources for DRBD:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> drbd_nfs ocf:linbit:drbd \
  params drbd_resource="nfs-upper" \
  op monitor interval="31" role="Slave" \
  op monitor interval="30" role="Master"
&prompt.crm.conf;<command>primitive</command> drbd_nfs_lower ocf:linbit:drbd \
  params rule #cluster-name eq &cluster1; \
  drbd_resource="nfs-lower-&cluster1;" \
  params rule #cluster-name eq &cluster2; \
  drbd_resource="nfs-lower-&cluster2;" \                                
  op monitor interval="31" role="Slave" \
  op monitor interval="30" role="Master"
&prompt.crm.conf;<command>ms</command> ms_drbd_nfs drbd_nfs \
  meta master-max="1" master-node-max="1" \
  clone-max="1" clone-node-max="1" notify="true"
&prompt.crm.conf;<command>ms</command> ms_drbd_nfs_lower drbd_nfs_lower \
  meta master-max="1" master-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true"</screen>
   </step>
   <step>
    <para>
     Add a group with the following colocation and ordering constraints:
    </para>
<screen>&prompt.crm.conf;<command>group</command> g_nfs nfs_fs nfs_service
&prompt.crm.conf;<command>colocation</command> col_nfs_ip_with_lower \
   inf: ip_nfs:Started  ms_drbd_nfs_lower:Master
&prompt.crm.conf;<command>colocation</command> col_nfs_g_with_upper \
   inf: g_nfs:Started  ms_drbd_nfs:Master
&prompt.crm.conf;<command>colocation</command> col_nfs_upper_with_ip \
   inf: ms_drbd_nfs:Master  ip_nfs:Started
&prompt.crm.conf;<command>order</command> o_lower_drbd_before_ip_nfs \
   inf: ms_drbd_nfs_lower:promote  ip_nfs:start
&prompt.crm.conf;<command>order</command> o_ip_nfs_before_drbd \
   inf: ip_nfs:start  ms_drbd_nfs:promote
&prompt.crm.conf;<command>order</command> o_drbd_nfs_before_svc \
   inf: ms_drbd_nfs:promote  g_nfs:start</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec.ha.geo.rsc.booth">
  <title>Ticket Dependencies, Constraints and Resources for booth</title>
  <para>
   To complete the booth setup, you need to execute the following steps to
   configure the resources and constraints needed for booth and failover of
   resources:
  </para>
  <itemizedlist>
   <listitem>
    <para>
<!--Configuring Ticket Dependencies-->
     <xref linkend="pro.ha.geo.setup.rsc.constraints" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
<!--Configuring a Resource Group for <systemitem class="daemon"
      >boothd</systemitem>-->
     <xref linkend="pro.ha.geo.setup.rsc.boothd" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
<!--Adding an Ordering Constraint for <systemitem class="daemon"
       >boothd</systemitem> and the Resource Group-->
     <xref linkend="pro.ha.geo.setup.rsc.order" xrefstyle="select:title"/>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   The resource configurations need to be available on each of the cluster
   sites. Transfer them to the other sites as described in
   <xref linkend="sec.ha.geo.rsc.sync.cib"/>.
  </para>
  <procedure xml:id="pro.ha.geo.setup.rsc.constraints">
   <title>Configuring Ticket Dependencies of Resources</title>
   &ticket-dependency-loss-policy; 
   <step>
    <para>
     On one of the nodes of cluster &cluster1;, start a shell and log in
     as &rootuser; or equivalent.
<!--<remark>toms 2011-10-04: Shorten it to "Start a root shell on one
       of the cluster nodes"? I guess we deal with admin-minded people here</remark>
       taroth 2011-10-06: because of new ACL support of the crm shell, also
       non-root users may have the rights to do so, therefore phrased it like
       that -->
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step xml:id="step.ha.geo.setup.rsc.constraints">
    <para>
     Configure constraints that define which resources depend on a certain
     ticket. For example, we need the following constraint for the DRBD
     scenario outlined in <!--FIXME<xref linkend="sec.ha.geo.drbd.scenario"/>-->:
    </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> nfs-req-ticket-nfs ticket-nfs: \ 
   ms_drbd_nfs:Master loss-policy=demote</screen>
    <para>
     This command creates a constraint with the ID
     <literal>nfs-req-ticket-nfs</literal>. It defines that the multi-state
     resource <literal>ms_drbd_nfs</literal> depends on
     <literal>ticket-nfs</literal>. However, only the resource's master mode
     depends on the ticket. In case <literal>ticket-nfs</literal> is
     revoked, <literal>ms_drbd_nfs</literal> is automatically demoted to
     <literal>slave</literal> mode, which in return will put DRBD into
     <literal>Secondary</literal> mode. That way, it is ensured that DRBD
     replication is still running, even if a site does not have the ticket.
    </para>
   </step>
   <step>
    <para>
     If you want other resources to depend on further tickets, create as
     many constraints as necessary with <command>rsc_ticket</command>.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
  <example xml:id="ex.ha.geo.setup.rsc.ticket.dep">
   <title>Ticket Dependency for Primitives</title>
   <para>
    Here is another example for a constraint that makes a primitive resource
    <literal>rsc1</literal> depend on <literal>&ticket1;</literal>:
   </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-&ticket1; &ticket1;: \
   rsc1 loss-policy="fence"</screen>
   <para>
    In case <literal>&ticket1;</literal> is revoked, the node running the
    resource should be fenced.
   </para>
  </example>
  <procedure xml:id="pro.ha.geo.setup.rsc.boothd">
   <title>Configuring a Resource Group for <systemitem class="daemon">boothd</systemitem></title> 
   &boothd-resource-group; 
   <step>
    <para>
     On one of the nodes of cluster <literal>&cluster1;</literal>, start
     a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Enter the following to create both primitive resources and to add them
     to one group, <literal>g-booth</literal>:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip-booth ocf:heartbeat:IPaddr2 \
  params iflabel="ha" nic="eth1" cidr_netmask="24"
  params rule #cluster-name eq &cluster1; ip="192.168.201.151" \
  params rule #cluster-name eq &cluster2; ip="192.168.202.151" 
&prompt.crm.conf;<command>primitive</command> booth ocf:pacemaker:booth-site \
  meta resource-stickiness="INFINITY" \
  params config="nfs" op monitor interval="10s"
&prompt.crm.conf;<command>group</command> g-booth ip-booth booth</screen>
    <para>
     With this configuration, each booth daemon will be available at its
     individual IP address, independent of the node the daemon is running
     on.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
<!--taroth 2012-02-14: fix for bnc#746863-->
  <procedure xml:id="pro.ha.geo.setup.rsc.order">
   <title>Adding an Ordering Constraint</title> 
   &booth-order-constraint; 
   <step>
    <para>
     On one of the nodes of cluster &cluster1;, start a shell and log in
     as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Create an ordering constraint:
    </para>
<screen>&prompt.crm.conf;<command>order</command> o-booth-before-nfs inf: g-booth ms_drbd_nfs:promote</screen>
    <para>
     The ordering constraint <literal>o-booth-before-nfs</literal> defines
     that the resource <literal>ms_drbd_nfs</literal> can only be promoted
     to master mode after the <literal>g-booth</literal> resource group has
     started.
    </para>
   </step>
   <step>
    <para>
     For any other resources that depend on a certain ticket, define further
     ordering constraints.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
    <para>
     The configuration is saved to the CIB.
    </para>
   </step>
  </procedure>
  <example xml:id="ex.ha.geo.rsc.order">
   <title>Ordering Constraint for Primitives</title>
   <para>
    If the resource that depends on a certain ticket is not a multi-state
    resource, but a primitive, the ordering constraint would look like the
    following:
   </para>
<screen>&prompt.crm.conf;<command>order</command> o-booth-before-rsc1 inf: g-booth rsc1</screen>
   <para>
    It defines that <literal>rsc1</literal> (which depends on
    <literal>&ticket1;</literal>) can only be started after the
    <literal>g-booth</literal> resource group.
   </para>
  </example>
 </sect2>

 <sect2 xml:id="sec.ha.geo.rsc.sync.cib">
<!--taroth 2014-11-25: fate#316118: [BETA 7] CIB replication between sites-->
  <title>Transferring the Resource Configuration to Other Cluster Sites</title>
  <para>
   If you have configured resources for one cluster site as described in
   <xref linkend="sec.ha.geo.rsc.drbd" xrefstyle="select:label"/> and
   <xref linkend="sec.ha.geo.rsc.booth" xrefstyle="select:label"/>, you are
   not done yet. You need to transfer the resource configuration to the
   other sites of your &geo; cluster.
  </para>
  <para>
   To simplify the transfer, you can tag any resources that are needed on
   <emphasis>all</emphasis> cluster sites, export them from the current CIB,
   and import them into the CIB of another cluster site.
   <xref linkend="pro.ha.geo.rsc.sync.cib"/> gives an example of how to do
   so. It is based on the following prerequisites:
  </para>
  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     You have a &geo; cluster with two sites: cluster
     <literal>&cluster1;</literal> and cluster
     <literal>&cluster2;</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     The cluster names for each site are defined in the respective
     &corosync.conf; files:
    </para>
<screen>totem {
     [...]
     cluster_name: &cluster1;
     }</screen>
    <para>
     This can either be done manually (by editing &corosync.conf;) or
     with the &yast; cluster module as described in the
     <citetitle>&admin;</citetitle> for &productname;
     &productnumber;, available at <link xlink:href="http://www.suse.com/documentation/"/>. Refer to the
     chapter <citetitle>Installation and Basic Setup</citetitle>, procedure
     <citetitle>Defining the First Communication Channel</citetitle>.
    </para>
   </listitem>
   <listitem>
    <para>
     You have configured the necessary resources for DRBD and booth as
     described in <xref linkend="sec.ha.geo.rsc.drbd"/> and
     <xref linkend="sec.ha.geo.rsc.booth"/>.
    </para>
   </listitem>
  </itemizedlist>
  <procedure xml:id="pro.ha.geo.rsc.sync.cib">
   <title>Transferring the Resource Configuration to Other Cluster Sites</title>
   <step>
    <para>
     Log in to one of the nodes of cluster <literal>&cluster1;</literal>.
    </para>
   </step>
   <step>
    <para>
     Start the cluster with:
    </para>
<screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
   </step>
   <step>
    <para>
     Enter <command>crm configure</command> to switch to the interactive
     &crmshell;.
    </para>
   </step>
   <step>
    <para>
     Tag the resources and constraints that are needed across the &geo;
     cluster:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Review the current CIB configuration:
      </para>
<screen>&prompt.crm.conf;show</screen>
     </step>
     <step>
      <para>
       Enter the following command to group the &geo; cluster-related
       resources with the tag <literal>geo_resources</literal>:
      </para>
<screen>&prompt.crm.conf;<command>tag</command> geo_resources: \
  ip_nfs nfs_fs nfs_service drbd_nfs drbd_nfs_lower ms_drbd_nfs \
  ms_drbd_nfs_lower g_nfs <co xml:id="co.geo.rsc.drbd"/>\
  col_nfs_ip_with_lower  col_nfs_g_with_upper col_nfs_upper_with_ip <xref linkend="co.geo.rsc.drbd" xrefstyle="select:label"/>\
  o_lower_drbd_before_ip_nfs o_ip_nfs_before_drbd \
  o_drbd_nfs_before_svc <xref linkend="co.geo.rsc.drbd" xrefstyle="select:label"/>\
  nfs-req-ticket-nfs ip-booth booth g-booth o-booth-before-nfs <co xml:id="co.geo.rsc.booth"/>
  [...] <co xml:id="co.geo.rsc.any"/></screen>
      <para>
       Tagging does not create any colocation or ordering relationship
       between the resources.
      </para>
      <calloutlist>
       <callout arearefs="co.geo.rsc.drbd">
        <para>
         Resources and constraints for DRBD, see
         <xref linkend="sec.ha.geo.rsc.drbd"/>.
        </para>
       </callout>
       <callout arearefs="co.geo.rsc.booth">
        <para>
         Resources and constraints for boothd, see
         <xref linkend="sec.ha.geo.rsc.booth"/>.
        </para>
       </callout>
       <callout arearefs="co.geo.rsc.any">
        <para>
         Any other resources of your specific setup that you need on all
         sites of the &geo; cluster.
        </para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Review your changes with <command>show</command>.
      </para>
     </step>
     <step>
      <para>
       If the configuration is according to your wishes, submit your changes
       with <command>submit</command> and leave the crm live shell with
       <command>exit</command>.
      </para>
     </step>
    </substeps>
   </step>
   <step xml:id="st.ha.geo.rsc.sync.cib.export.start">
    <para>
     Export the tagged resources and constraints to a file named
     <filename>exported.cib</filename>:
    </para>
<screen>&prompt.root;<command>crm configure show</command> tag:geo_resources geo_resources &gt; exported.cib</screen>
    <para>
     The command <command>crm configure show tag:</command><replaceable>TAGNAME</replaceable> 
     shows all resources that belong to
     the tag <replaceable>TAGNAME</replaceable>.
    </para>
   </step>
   <step>
    <para>
     Log in to one of the nodes of cluster <literal>&cluster2;</literal>
     and proceed as follows:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Start the cluster with:
      </para>
<screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
     </step>
     <step>
      <para>
       Copy the file <filename>exported.cib</filename> from cluster
       <literal>&cluster1;</literal> to this node.
       <remark>taroth
        2014-11-26: alternatively, the CIB can be loaded from an URL - consider
        if to mention this, too</remark>
      </para>
     </step>
     <step>
      <para>
       Import the tagged resources and constraints from the file
       <filename>exported.cib</filename> into the CIB of cluster
       <literal>&cluster2;</literal>:
      </para>
<screen>&prompt.root;<command>crm configure load</command> update <replaceable>PATH_TO_FILE/exported.cib</replaceable></screen>
      <para>
       When using the <option>update</option> parameter for the <command>crm
       configure load</command> command, &crmsh; tries to integrate the
       contents of the file into the current CIB configuration (instead of
       replacing the current CIB with the file contents).
      </para>
     </step>
     <step xml:id="st.ha.geo.rsc.sync.cib.import.stop">
      <para>
       View the updated CIB configuration with the following command:
      </para>
<screen>&prompt.root;<command>crm configure show</command></screen>
      <para>
       The imported resources and constraints will appear in the CIB.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   This configuration will result in the following:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     When granting <literal>ticket-nfs</literal> to cluster
     <literal>&cluster1;</literal>, the node hosting the resource
     <literal>ip_nfs</literal> will get the IP address
     <literal>192.168.201.151</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     When granting <literal>ticket-nfs</literal> to cluster
     <literal>&cluster2;</literal>, the node hosting the resource
     <literal>ip_nfs</literal> will get the IP address
     <literal>192.168.202.151</literal>.
    </para>
   </listitem>
  </itemizedlist>
  <example xml:id="ex.ha.geo.rsc.refer.params">
   <title>Referencing Site-Dependent Parameters in Resources</title>
   <para>
    Based on the example in
    <xref linkend="pro.ha.geo.rsc.drbd" xrefstyle="select:label"/>, you can
    also create resources that reference site-specific parameters of another
    resource, for example, the IP parameters of <literal>ip_nfs</literal>.
    Proceed as follows:
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      On cluster <literal>&cluster1;</literal> create a dummy resource
      that references the IP parameters of <literal>ip_nfs</literal> and
      uses them as the value of its <literal>state</literal> parameter:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> dummy1 ocf:pacemaker:Dummy \
  params rule #cluster-name eq &cluster1; \
  @ip_nfs-instance_attributes-0-ip:state \
  params rule #cluster-name eq &cluster2; \
  @ip_nfs-instance_attributes-1-ip:state \
  op monitor interval=10</screen>
    </listitem>
    <listitem>
     <para>
      Add a constraint to make the <literal>dummy1</literal> resource depend
      on <literal>ticket-nfs</literal>, too:
     </para>
<screen>&prompt.crm.conf;<command>rsc_ticket</command> dummy1-dep-ticket-nfs \
  ticket-nfs: dummy1 loss-policy=stop</screen>
    </listitem>
    <listitem>
     <para>
      Tag the resource and the constraint:
     </para>
<screen>&prompt.crm.conf;<command>tag</command> geo_resources_2: dummy1 \
  dummy1-dep-ticket-nfs</screen>
    </listitem>
    <listitem>
     <para>
      Review your changes with <command>show</command>, submit your changes
      with <command>submit</command>, and leave the crm live shell with
      <command>exit</command>.
     </para>
    </listitem>
    <listitem>
     <para>
      Export the resources tagged with <literal>geo_resources_2</literal>
      from cluster <literal>&cluster1;</literal> and import them into the
      CIB of cluster <literal>&cluster2;</literal>, similar to
      <xref linkend="st.ha.geo.rsc.sync.cib.export.start"/> through
      <xref linkend="st.ha.geo.rsc.sync.cib.import.stop"/> of
      <xref linkend="pro.ha.geo.rsc.sync.cib" xrefstyle="select:label"/>.
     </para>
    </listitem>
   </orderedlist>
   <para>
    This configuration will result in the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      When granting <literal>ticket-nfs</literal> to cluster
      <literal>&cluster1;</literal>, the following file will be created
      on the node hosting the <literal>dummy</literal> resource:
      <filename>/var/lib/heartbeat/cores/192.168.201.151</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      When granting <literal>ticket-nfs</literal> to cluster
      <literal>&cluster2;</literal>, the following file will be created
      on the node hosting the <literal>dummy</literal> resource:
      <filename>/var/lib/heartbeat/cores/192.168.202.151</filename>.
     </para>
    </listitem>
   </itemizedlist>
  </example>
 </sect2>
</sect1>
<xi:include href="common_legal.xml"/>
</article>
