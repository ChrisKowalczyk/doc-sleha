<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:novdoc-profile.xsl"
  type="text/xml" 
  title="Profiling step"?>
<!DOCTYPE article PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
                      "novdocx.dtd"
[
<!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<!--taroth 2014-08-07: check screens (and add line-breaks)-->
<?provo dirname="geo_quick/"?>
<article lang="en" id="art.ha.geo.quick">
<?suse-quickstart columns="no" version="2"?>
 <title>&geoquick;</title>
<!-- <subtitle>GEO Clustering for &productname;
 </subtitle>-->
 <articleinfo><productname>GEO Clustering for &productname;</productname>
  <productnumber>&productnumber;</productnumber>
  <authorgroup>
   <author><firstname>Tanja</firstname><surname>Roth</surname>
   </author>
  </authorgroup>
 </articleinfo>
 <abstract>
  <para> Apart from local clusters and metro area clusters, &productnamereg;
   &productnumber; also supports &geo; clusters. That means you can have
   multiple, geographically dispersed sites with a local cluster each. Failover
   between these clusters is coordinated by a higher level entity: the booth
   daemon (&boothd;). Support for &geo; clusters is available as
   a separate extension to &hasi;, called &hageo;. </para>
  <para>
   <remark>taroth 2014-08-11: todo - https://fate.suse.com/316120: [docu]
    Quickstart guide for SLE HA GEO, for DRBD-related part, see also 
    https://mailman.suse.de/mailman/private/ha-devel/2014-February/003095.html
    for input)</remark>
   <remark>taroth 2014-08-11: todo - https://fate.suse.com/316114: Replicated
    storage solution based on DRBD (NEEDINFO, input probably 
    https://mailman.suse.de/mailman/private/ha-devel/2014-February/003095.html)</remark>
  </para>
 </abstract>
 <para>
  <remark>taroth 2014-08-11: https://fate.suse.com/316112: IP relocation via DNS
   update (prio: mandatory): check with devs about doc impact, where/in which
   scenario is it visible for the user? </remark>
  <remark>taroth 2014-08-11: https://fate.suse.com/316126: reconfiguration of boothd while running
   (prio: important): 
   #4: Lars Marowsky-Bree (2013-08-19 16:08) [reply] Currently, to add
   tickets or remove sites or change their IP address, the booth daemon
   has to be restarted, which affects all other tickets and connections
   as well - todo: not sure where to mention this...</remark></para>
 <sect1 id="sec.ha.geo.inst">
  <title>Installation as Add-on</title>
  <para>For using the &hasi; and &hageo;, you need the packages included
   in the following installation patterns:</para>
  
  <itemizedlist>
   <listitem>
    <para>
     <literal>&ha;</literal>
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>&geo; Clustering for &ha;</literal>
    </para>
   </listitem>
  </itemizedlist>
  
  <note>
   <title>Package Requirements for Arbitrators</title>
   <para>If your &geo; cluster setup includes one ore more arbitrators (see
    <xref linkend="vle.ha.geo.components.arbitrator"/>), those only need the
    pattern <literal>&geo; Clustering for &ha;</literal>. For instructions on
    how to install this pattern, see 
    <xref linkend="sec.ha.geo.inst.arbitrators"/>.</para>
  </note>
  
 <para>Both patterns are only available if you have registered your system at
   &scc; (or a local registration server) and have added the respective
   product channels or installation media as add-ons. For information on how to
   install add-on products, see the <citetitle>&sle; &productnumber;
   &deploy;</citetitle>, available at &suse-onlinedoc;. 
   Refer to chapter <citetitle>Installing Add-On Products</citetitle>.
   <!--taroth: need to use hard-coded link here as the target is not included in the same set-->
 </para>
  
  <sect2 id="sec.ha.geo.inst.nodes">
   <title>Installing the Packages on Cluster Nodes</title>

   <para>In case both &hasi; and &hageo; have been added as add-on
    products, but the packages are not installed yet, proceed as follows:</para>

   <procedure>
    <step>
     <para>To install the packages from both patterns via command line, use
      zypper:</para>
     <screen>sudo <command>zypper</command> in -t pattern ha_sles ha_geo</screen>
    </step>
    <step id="step.ha.geo.inst.yast">
     <para>Alternatively, use &yast; for a graphical installation:</para>
     <substeps>
      <step>
       <para> Start &yast; as &rootuser; user and select <menuchoice>
         <guimenu>Software</guimenu>
         <guimenu>Software Management</guimenu>
        </menuchoice>. </para>
      </step>
      <step>
       <para> Click <menuchoice>
         <guimenu>View</guimenu>
         <guimenu>Patterns</guimenu>
        </menuchoice> and activate the following patterns:</para>
       <itemizedlist>
        <listitem>
         <para>
          <literal>&ha;</literal>
         </para>
        </listitem>
        <listitem>
         <para>
          <literal>&geo; Clustering for &ha;</literal>
         </para>
        </listitem>
       </itemizedlist>
      </step>
      <step>
       <para> Click <guimenu>Accept</guimenu> to start installing the packages.
       </para>
      </step>
     </substeps>
    </step>
   </procedure>

   <important>
    <para> The software packages needed for &ha; and &geo; clusters are
      <emphasis>not</emphasis> automatically copied to the cluster nodes. </para>
    <itemizedlist>
     <listitem>
      <para> Install &sls; &productnumber; and the
        <literal>&ha;</literal> and <literal>&geo; Clustering for
        &ha;</literal> patterns on <emphasis>all</emphasis> machines that
       will be part of your &geo; cluster. </para>
     </listitem>
     <listitem>
      <para> If you do not want to install the packages manually on all nodes
       that will be part of your cluster, use &ay; to clone existing nodes.
       Find more information in the <citetitle>&haguide;</citetitle> for
       &productname; &productnumber;, available from
       &suse-onlinedoc;. Refer to chapter <citetitle>Installation and Basic
        Setup</citetitle>, section <citetitle>Mass Deployment with
        &ay;</citetitle>.</para>
     </listitem>
    </itemizedlist>
   </important>
  </sect2>
  <sect2 id="sec.ha.geo.inst.arbitrators">
   <title>Installing the Packages on Arbitrators</title>
   <procedure>
    <step>
     <para>Make sure that &hageo; has been added as add-on product to the
      machines to serve as arbitrators.</para>
    </step>
    <step>
     <para>Log in to each arbitrator and install the packages with the following
      command:</para>
     <screen>sudo <command>zypper</command> in -t pattern ha_geo</screen>
     <para>Alternatively, use &yast; to install the <literal>&geo;
       Clustering for &ha;</literal> pattern. </para>
    </step>
    </procedure>
  </sect2>
</sect1>
 
 
 <sect1 id="sec.ha.geo.challenges">
  <title>Challenges for &geo; Clusters</title>

  <para> Typically, &geo; environments are too far apart to support
   synchronous communication between the sites. That leads to the following
   challenges: </para>

  <itemizedlist>
   <listitem>
    <para>
     How to make sure that a cluster site is up and running?
    </para>
   </listitem>
   <listitem>
    <para>
     How to make sure that resources are only started once?
    </para>
   </listitem>
   <listitem>
    <para>
     How to make sure that quorum can be reached between the different sites
     and a split brain scenario can be avoided?
    </para>
   </listitem>
<!--<listitem>
    <para>How to keep the CIB up-to-date on all nodes and sites?</para>
   </listitem>
   -->
   <listitem>
    <para>
     How to manage failover between the sites?
    </para>
   </listitem>
   <listitem>
    <para>
     How to deal with high latency in case of resources that need to be
     stopped?
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In the following sections, learn how to meet these challenges with
   &productname;.
  </para>
 </sect1>
 <sect1 id="sec.ha.geo.concept">
  <title>Conceptual Overview</title>

  <para> &geo; clusters based on &productname; can be considered as <quote>overlay</quote>
   clusters where each cluster site corresponds to a cluster node in a traditional cluster. The
   overlay cluster is managed by the booth mechanism. It guarantees that the cluster resources will
   be highly available across different cluster sites. This is achieved by using so-called tickets
   that are treated as failover domain between cluster sites, in case a site should be down. Booth
   guarantees that every ticket is owned by only one site at the time.</para>

  <para>
   The following list explains the individual components and mechanisms that
   were introduced for &geo; clusters in more detail.
  </para>

  <variablelist id="vl.ha.geo.components">
   <title>Components and Ticket Management</title>
   <varlistentry id="vle.ha.geo.components.ticket">
    <term>Ticket</term>
    <listitem>
     <para>
      A ticket grants the right to run certain resources on a specific
      cluster site. A ticket can only be owned by one site at a time.
      Initially, none of the sites has a ticket&mdash;each ticket must be
      granted once by the cluster administrator. After that, tickets are
      managed by the booth for automatic failover of resources. But
      administrators may also intervene and grant or revoke tickets
      manually.
     </para>
     <para>After a ticket is administratively revoked, it is not managed by booth anymore. For booth
      to start managing the ticket again, the ticket must be again granted to a site.</para>
     <para>
      Resources can be bound to a certain ticket by dependencies. Only if
      the defined ticket is available at a site, the respective resources
      are started. Vice versa, if the ticket is removed, the resources
      depending on that ticket are automatically stopped.
     </para>
     <para>
      The presence or absence of tickets for a site is stored in the CIB as
      a cluster status. With regards to a certain ticket, there are only two
      states for a site: <literal>true</literal> (the site has the ticket)
      or <literal>false</literal> (the site does not have the ticket). The
      absence of a certain ticket (during the initial state of the
      &geo; cluster) is not treated differently from the situation
      after the ticket has been revoked: both are reflected by the value
      <literal>false</literal>.
     </para>
     <para>
      A ticket within an overlay cluster is similar to a resource in a
      traditional cluster. But in contrast to traditional clusters, tickets
      are the only type of resource in an overlay cluster. They are
      primitive resources that do not need to be configured nor cloned.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.booth">
    <term>Booth</term>
    <listitem>
     <para>
      Booth is the instance managing the ticket distribution and thus,
      the failover process between the sites of a &geo; cluster. Each
      of the participating clusters and arbitrators runs a service, the
      &boothd;. It connects
      to the booth daemons running at the other sites and exchanges
      connectivity details. Once a ticket is granted to a site, the booth
      mechanism can manage the ticket automatically: If the site which
      holds the ticket is out of service, the booth daemons will vote which
      of the other sites will get the ticket. To protect against brief
      connection failures, sites that lose the vote (either explicitly or
      implicitly by being disconnected from the voting body) need to
      relinquish the ticket after a time-out. Thus, it is made sure that a
      ticket will only be re-distributed after it has been relinquished by
      the previous site. See also
      <xref
       linkend="vle.ha.geo.components.deadman"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.arbitrator">
    <term>Arbitrator</term>
    <listitem>
     <para>
      Each site runs one booth instance that is responsible for
      communicating with the other sites. If you have a setup with an even
      number of sites, you need an additional instance to reach consensus
      about decisions such as failover of resources across sites. In this
      case, add one or more arbitrators running at additional sites.
      Arbitrators are single machines that run a booth instance in a special
      mode. As all booth instances communicate with each other, arbitrators
      help to make more reliable decisions about granting or revoking
      tickets. Arbitrators cannot hold any tickets.
     </para>
     <para>
      An arbitrator is especially important for a two-site scenario: For
      example, if site <literal>A</literal> can no longer communicate with
      site <literal>B</literal>, there are two possible causes for that:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        A network failure between <literal>A</literal> and
        <literal>B</literal>.
       </para>
      </listitem>
      <listitem>
       <para>
        Site <literal>B</literal> is down.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      However, if site <literal>C</literal> (the arbitrator) can still
      communicate with site <literal>B</literal>, site <literal>B</literal>
      must still be up and running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Ticket Failover</term>
    <listitem>
     <para>If the ticket gets lost, that means other boot instances do not hear from the ticket
      owner in a sufficiently long time, one of the remaining sites will acquire the ticket. This is
      what is called ticket failover. If the remaining members cannot form a majority, then the
      ticket cannot fail over. </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.deadman">
    <term>Dead Man Dependency (<literal>loss-policy="fence"</literal>)</term>
    <listitem>
     <para>
       After a ticket is revoked, it can take a long time until all resources
      depending on that ticket are stopped, especially in case of cascaded
      resources. To cut that process short, the cluster administrator can
      configure a <literal>loss-policy</literal> (together with the ticket
      dependencies) for the case that a ticket gets revoked from a site. If
      the loss-policy is set to <literal>fence</literal>, the nodes that are
      hosting dependent resources are fenced.  
     </para>
     <warning>
      <title>Potential Loss of Data</title>
      <para>On the one hand, <literal>loss-policy="fence"</literal> considerably
       speeds up the recovery process of the cluster and makes sure that
       resources can be migrated more quickly. </para>
      <para>On the other hand, it can lead to loss of all unwritten data, such
       as:</para>
      <itemizedlist>
       <listitem>
        <para>Data lying on shared storage (for example, DRBD).</para>
       </listitem>
       <listitem>
        <para>Data in a replicating database (for example, MariaDB or
         PostgreSQL) that has not yet reached the other site, due to a slow
         network link.</para>
       </listitem>
      </itemizedlist>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>

<!--taroth 201110-06: todo - ask eugene to redo the svg graphic 
  (shows some strange artefacts in PDF, maybe due to import from PDF)-->

  <figure>
   <title>Example Scenario: A Two-Site Cluster (4 Nodes + Arbitrator)</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_geocluster.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_geocluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>The most common scenario is probably a &geo; cluster with two sites
   and a single arbitrator on a third site. The upper limit is (currently) 16
   booth instances. </para>

  <para>
   As usual, the CIB is synchronized within each cluster, but it is not
   synchronized across cluster sites of a &geo; cluster. You have to
   configure the resources that will be highly available across the
   &geo; cluster for every site accordingly. <remark>taroth 2014-08-07: todo -
    https://fate.suse.com/316118:  CIB replication between sites (NEEDINFO, see
    fate c#21)</remark>
  </para>
 </sect1>
 <sect1 id="sec.ha.geo.req">
  <title>Requirements</title>

  <itemizedlist>
   <title>Software Requirements</title>
   <listitem>
    <para>
     All clusters that will be part of the &geo; cluster must be based
     on &productname; &productnumber;.
    </para>
   </listitem>
   <listitem>
    <para>
     &slsreg; &productnumber; must be installed on all arbitrators.
    </para>
   </listitem>
   <listitem>
    <para> The &hageo; add-on must be installed on all cluster nodes <emphasis>and</emphasis> on
     all arbitrators that will be part of the &geo; cluster. 
     <!--<remark>taroth 2014-08-20: booth package would be enough (GEO pattern only consists of the booth
      package) but to make it clear where to get the package from, phrasing like this</remark>--></para>
   </listitem>
  </itemizedlist>
  
  <itemizedlist>
   <title>Network Requirements</title>
   <listitem>
    <para>The sites must be reachable on one UDP and TCP port per booth
     instance. That means any firewalls or IPSec tunnels in between must be
     configured accordingly. </para>
   </listitem>
   <listitem>
    <para>Other setup decision may require to allow more open ports (for
     example, for DRBD or database replication). </para>
   </listitem>
  </itemizedlist>
  
  <itemizedlist>
   <title>Other Requirements and Recommendations</title>
   <listitem>
    <para>All cluster nodes on all sites should synchronize to an NTP server
     outside the cluster. For more information, see the
      <citetitle>&admin;</citetitle> for &sls; &productnumber;,
     available at &suse-onlinedoc;. Refer to the chapter <citetitle>Time
      Synchronization with NTP</citetitle>. </para>
      <para> If nodes are not synchronized, log files and cluster reports are
       very hard to analyze. </para>
     </listitem>
    </itemizedlist>
  

 
 </sect1>
 <sect1 id="sec.ha.geo.oview">
  <title>Basic Setup&mdash; Overview</title>

  <para>
   Configuring a &geo; cluster takes the following basic steps:
   </para>
  <variablelist>
    <varlistentry>
<!--Setting Up the Booth Services-->
    <term><xref linkend="sec.ha.geo.booth" xrefstyle="select:title"/>
    </term>
    <listitem>
     <orderedlist>
      <listitem>
<!--create booth config-->
       <para>
      <xref linkend="sec.ha.geo.booth" xrefstyle="select:title"/>
       </para>
      </listitem>
      <listitem>
       <!--sync booth config-->
       <para>
        <xref linkend="sec.ha.geo.booth.sync" xrefstyle="select:title"/>
       </para>
      </listitem>
      <listitem>
<!--start boothd-->
       <para>
        <xref linkend="sec.ha.geo.setup.booth.service" xrefstyle="select:title"
        />
       </para>
      </listitem>
     </orderedlist>
    </listitem>
    </varlistentry>
   <varlistentry>
    <!--Configuring Cluster Resources and Constraints-->
     <term>
      <xref linkend="sec.ha.geo.rsc" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>Use either &crmsh; or &hawk; for the following steps:</para>
     <orderedlist>
      <listitem>
       <para>Configuring Ticket Dependencies
        <!--<xref linkend="pro.ha.geo.setup.rsc.constraints"
         xrefstyle="select:title"/>-->
       </para>
      </listitem>
      <listitem>
       <para>Configuring a Resource Group for <systemitem class="daemon"
         >boothd</systemitem>
        <!--<xref linkend="pro.ha.geo.setup.rsc.boothd" xrefstyle="select:title"/>-->
       </para>
      </listitem>
      <listitem>
       <para>
        Adding an Ordering Constraint for <systemitem class="daemon"
         >boothd</systemitem> and the Resource Group
        <!--<xref linkend="pro.ha.geo.setup.rsc.order" xrefstyle="select:title"/>-->
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
  </variablelist>
  </sect1>

 <sect1 id="sec.ha.geo.booth">
  <title>Setting Up the Booth Services</title>
  <para>Usually, the booth configuration is defined in &booth.conf;. This
   file must be the same on all sites of your &geo; cluster, including the
   arbitrator or arbitrators. To keep the booth configuration synchronous across
   all sites and arbitrators, you can use &csync;. <remark>taroth
    2014-08-25: todo - describe how to do so and add xref to this
    section</remark></para>
  <para>&booth-multi-tenancy; For details on how to configure booth for
   multiple &geo; clusters, refer to <xref linkend="sec.ha.geo.booth.multi"
   />. </para>
  
  <sect2 id="sec.ha.geo.booth.default">
   <title>Default Booth Setup</title>
   <para>To configure all parameters needed for booth, either edit the booth
    configuration files manually or by using the &yast; <guimenu>Geo
     Cluster</guimenu> module. To access the &yast; module, start it from
    command line with <command>yast2
     geo-cluster</command> (or start
    &yast; and select <menuchoice>
     <guimenu>High Availability</guimenu>
     <guimenu>Geo Cluster</guimenu>
    </menuchoice>). </para>
   <example id="ex.ha.booth.conf.default">
    <title>A Booth Configuration File</title>
    <para>
     <remark>taroth 2014-08-21: not sure if it makes sense that all tickets
      configured here should have similar options and values or if we should
      rather show different options and values for individual tickets - dejan,
      please check!</remark>
    </para>
    <screen>transport = UDP <co id="co.ha.geo.booth.config.transport"/>
port = 9929 <co id="co.ha.geo.booth.config.port"/>
arbitrator = 147.2.207.14 <co id="co.ha.geo.booth.config.arbitrator"/>
site= 147.4.215.19 <co id="co.ha.geo.booth.config.site"/>
site= 147.18.2.1  <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket="ticketA" <co id="co.ha.geo.booth.config.ticket"/>
     expire = 600 <co id="co.ha.geo.booth.config.expiry"/>
     timeout = 10 <co id="co.ha.geo.booth.config.timeout"/>
     retries = 5 <co id="co.ha.geo.booth.config.retries"/>
     renewal-freq = 13 <co id="co.ha.geo.booth.config.renewal"/>
     before-acquire-handler = /usr/share/booth/service-runnable d-src1 <co id="co.ha.geo.booth.config.handler"/>
ticket="ticketB" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
     expire = 600 <xref linkend="co.ha.geo.booth.config.expiry" xrefstyle="select:label nopage"/>
     timeout = 10 <xref linkend="co.ha.geo.booth.config.timeout" xrefstyle="select:label nopage"/>
     retries = 5 <xref linkend="co.ha.geo.booth.config.retries" xrefstyle="select:label nopage"/>
     renewal-freq = 13 <xref linkend="co.ha.geo.booth.config.renewal" xrefstyle="select:label nopage"/>
     before-acquire-handler = /usr/share/booth/service-runnable d-src1 <xref linkend="co.ha.geo.booth.config.handler" xrefstyle="select:label nopage"/>
    </screen>
    <calloutlist>
     <callout arearefs="co.ha.geo.booth.config.transport">
      <para>&booth-transport; Currently, only UDP is 
       supported, other transport layers will follow.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.port">
      <para> &booth-port; When not using the default port (<literal>9929</literal>), choose a port that is not 
       already used for different services. Make sure to open the port in the
       nodes&apos; and arbitrators&apos; firewalls. The booth clients use TCP to communicate with the 
       &boothd;. Booth will always bind and listen to both UDP and TCP ports.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.arbitrator">
      <para>&booth-arbitrator; Add an entry for each 
       arbitrator you use in your &geo; cluster setup.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.site">
      <para> &booth-site; Add an entry for each site you use in your
       &geo; cluster setup. Make sure to insert the correct virtual IP
       addresses (<systemitem>IPaddr2</systemitem>) for each site, otherwise the
       booth mechanism will not work correctly. Booth works with both IPv4 and
       IPv6 addresses.
       <!--taroth 2014-08-21: https://fate.suse.com/316122: booth should support IPv6 in full 
        (prio: important--></para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.ticket">
      <para>&booth-ticket; For each ticket, add a
       <literal>ticket</literal> entry.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.expiry">
      <para> Optional parameter. &booth-ticket-expiry;</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.timeout">
      <para> Optional parameter. &booth-ticket-timeout;</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.retries">
      <para> Optional parameter. &booth-ticket-retries;</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.renewal">
      <para> Optional parameter. &booth-ticket-renewal; </para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.handler">
      <para> Optional parameter. &booth-ticket-handler-1;</para>
      &booth-ticket-handler-2; </callout>
    </calloutlist>
   </example>
   
   <procedure id="pro.ha.geo.setup.booth.config.edit">
    <title>Manually Editing The Booth Configuration File</title>
    <step>
     <para> Log in to a cluster node as &rootuser; or equivalent. </para>
    </step>
    <step>
     <para> Copy the example booth configuration file
       <filename>/etc/booth/booth.conf.example</filename> to
      &booth.conf;.</para>
    </step>
    <step>
     <para>Edit &booth.conf; according to <xref
       linkend="ex.ha.booth.conf.default"/>.</para>
    </step>
    <step>
     <para>Verify your changes and save the file. </para>
    </step>
    <step>
     <para>On all cluster nodes and arbitrators, open the port in the firewall
      that you have configured for booth. See <xref
       linkend="ex.ha.booth.conf.default"/>, position <xref
       linkend="co.ha.geo.booth.config.port"/>. </para>
    </step>
   </procedure>


   <procedure id="pro.ha.geo.setup.booth.yast">
    <title>Setting Up Booth with &yast;</title>
    <step>
     <para> Log in to a cluster node as &rootuser; or equivalent. </para>
    </step>
    <step>
     <para>Start the &yast; <guimenu>Geo Cluster</guimenu> module. </para>
    </step>
    <step>
     <para>Choose to <guimenu>Edit</guimenu> an existing booth configuration
      file or click <guimenu>Add</guimenu> to create a new booth configuration
      file:</para>
     <substeps>

      <step id="step.ha.booth.conf.params">
       <para>In the screen that appears configure the following
        parameters:</para>
       <itemizedlist>
        <listitem>
         <formalpara>
          <title>Configuration File</title>
          <para>A name for the booth configuration file. &yast; suggests
            <literal>booth</literal> by default. This results in the booth
           configuration being written to &booth.conf;. Only change this
           value if you need to set up multiple booth instances for different
           &geo; clusters.
           <!--FIXME: as described in <xref linkend=""/>--></para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara>
          <title>Transport</title>
          <para>&booth-transport; See also <xref
            linkend="ex.ha.booth.conf.default"/>, position <xref
            linkend="co.ha.geo.booth.config.transport"/>.</para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara>
          <title>Port</title>
          <para>&booth-port; See also <xref
            linkend="ex.ha.booth.conf.default"/>, position <xref
            linkend="co.ha.geo.booth.config.port"/>. </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara>
          <title>Arbitrator</title>
          <para>&booth-arbitrator; See also <xref
            linkend="ex.ha.booth.conf.default"/>, position <xref
            linkend="co.ha.geo.booth.config.arbitrator"/>.</para>
         </formalpara>
         <para>To specify an <guimenu>Arbitrator</guimenu>, click
           <guimenu>Add</guimenu>. In the dialog that opens, enter the IP
          address of your arbitrator and click <guimenu>OK</guimenu>. </para>
        </listitem>
        <listitem>
         <formalpara>
          <title>Site</title>
          <para>&booth-site; See also <xref
            linkend="ex.ha.booth.conf.default"/>, position <xref
            linkend="co.ha.geo.booth.config.site"/>.</para>
         </formalpara>
         <para>To specify a <guimenu>Site</guimenu> of your &geo; cluster,
          click <guimenu>Add</guimenu>. In the dialog that opens, enter the IP
          address of one site and click <guimenu>OK</guimenu>.</para>
        </listitem>
        <listitem>
         <formalpara>
          <title>Ticket</title>
          <para>&booth-ticket; See also <xref
            linkend="ex.ha.booth.conf.default"/>, position <xref
            linkend="co.ha.geo.booth.config.ticket"/>.</para>
         </formalpara>
         <para>To specify a <guimenu>Ticket</guimenu>, click
           <guimenu>Add</guimenu>. In the dialog that opens, enter a unique
           <guimenu>Ticket</guimenu> name. Additionally, you can specify
          optional parameters for your ticket. For an overview, see <xref
           linkend="ex.ha.booth.conf.default"/>, positions <xref
           linkend="co.ha.geo.booth.config.expiry"/> to <xref
           linkend="co.ha.geo.booth.config.handler"/>. Click
           <guimenu>OK</guimenu> to confirm your changes.</para>
        </listitem>
       </itemizedlist>

       <figure id="fig.yast2.ha.geo.booth">
        <title>Example Ticket Dependency</title>
        <!--taroth 2014-08-26: todo - update with more recent layout of yast,
         hopefully available in RC3  (https://bugzilla.novell.com/show_bug.cgi?id=892900) 
         and same values as in example booth file-->
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="yast2_geo_cluster_booth.png" width="60%"
           format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="yast2_geo_cluster_booth.png" width="50%"
           format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
      <step>
       <para>Click <guimenu>OK</guimenu> to close the current booth
        configuration screen. &yast; shows the name of the booth
        configuration file that you just defined. </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>Before closing the &yast; module, switch to the <guimenu>Firewall
       Configuration</guimenu> category.</para>
    </step>
    <step>
     <para>
      <remark>taroth 2014-08-26: DEVs, I guess the firewall setting is only
       applied to the current machine? so the ports on any other cluster
       nodes or arbitrators have to be opened manually, right?</remark>
      To open the port you have configured for booth, enable <guimenu>Open
      Port in Firewall</guimenu>. </para>
     <important>
      <title>Firewall Setting for Local Machine Only</title>
      <para>The firewall setting is only applied to the current machine. Make
       sure to open the respective port on all other cluster nodes and
       arbitrators of your &geo; cluster setup, too. </para></important>
    </step>
    <step>
     <para>Click <guimenu>Finish</guimenu> to confirm all settings and close the
      &yast; module. Depending on the <replaceable>NAME</replaceable> of the
       <guimenu>Configuration File </guimenu> specified in <xref
       linkend="step.ha.booth.conf.params"/>, the configuration is written to
        <filename>/etc/booth/<replaceable>NAME</replaceable>.conf</filename>.</para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.geo.booth.multi">
   <title>Booth Setup for Multiple Tenants</title>
   <!--taroth 2014-08-07:  https://fate.suse.com/316123:
    Multi-tenancy for booth (prio: important)-->
   <para>&booth-multi-tenancy;</para>
   <para>
    <remark>taroth 2014-08-26: DEVs, is the following a realistic setup? If not,
     please come up with a better example :)</remark> Let us assume you have two
    &geo; clusters, one in EMEA (Europe, the Middle East and Africa), and
    one in the Asia-Pacific region (APAC). </para>
   <para>To use the same arbitrator for both &geo; clusters, create two
    configuration files in the <filename>/etc/booth</filename> directory:
     <filename>/etc/booth/emea.conf</filename> and
     <filename>/etc/booth/apac.conf</filename>. Both must minimally differ in
    the following parameters:</para>
   <itemizedlist>
    <listitem>
     <para>The port used for the communication of the booth instances.</para>
    </listitem>
    <listitem>
     <para>The sites belonging to the different &geo; clusters that the
      arbitrator is used for.</para>
    </listitem>
   </itemizedlist>

   <example id="ex.ha.conf.booth.multi-1">
    <title>
     <filename>/etc/booth/apac.conf</filename>
    </title>
    <para><remark>taroth 2014-08-26: DEVs, can someone provide better IP
     addresses for the cluster sites below? I just noticed that the entities we have
    defined for our usual network examples in the docs are too similar for different cluster
    sites...</remark></para>
    <screen>transport = UDP 
port = 9133 <xref linkend="co.ha.geo.booth.config.port" xrefstyle="select:label nopage"/>
arbitrator = 147.2.207.14 <xref linkend="co.ha.geo.booth.config.arbitrator" xrefstyle="select:label nopage"/>
site= &slpip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
site= &proxyip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket="ticketC" <xref linkend="co.ha.geo.booth.config.ticket"/>
     expire = 600 
     timeout = 10 
     retries = 5 
     renewal-freq = 13 
     before-acquire-handler = /usr/share/booth/service-runnable d-src1
ticket="ticketD" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
     expire = 600 
     timeout = 10 
     retries = 5 
     renewal-freq = 13 
     before-acquire-handler = /usr/share/booth/service-runnable d-src1</screen>
   </example>

   <example id="ex.ha.conf.booth.multi-2">
    <title>
     <filename>/etc/booth/emea.conf</filename>
    </title>
    <para><remark>taroth 2014-08-26: DEVs, can someone provide better IP
     addresses for the cluster sites below? I just noticed that the entities we have
     defined for our usual network examples in the docs are too similar for different cluster
     sites...</remark></para>
    <screen>transport = UDP 
port = 9150 <xref linkend="co.ha.geo.booth.config.port" xrefstyle="select:label nopage"/>
arbitrator = 147.2.207.14 <xref linkend="co.ha.geo.booth.config.arbitrator" xrefstyle="select:label nopage"/>
site= &ldapip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
site= &nisip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
site= &ntpip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
site= &vpnip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>     
ticket="ticketE" <xref linkend="co.ha.geo.booth.config.ticket"/>
     expire = 600 
     timeout = 10 
     retries = 5 
     renewal-freq = 13 
     before-acquire-handler = /usr/share/booth/service-runnable d-src1
ticket="ticketF" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
     expire = 600 
     timeout = 10 
     retries = 5 
     renewal-freq = 13 
     before-acquire-handler = /usr/share/booth/service-runnable d-src1</screen>
   </example>

   <calloutlist>
    <callout arearefs="co.ha.geo.booth.config.port">
     <para>&booth-port; The configuration files use different ports to allow
      for start of multiple booth instances on the same arbitrator.</para>
    </callout>
    <callout arearefs="co.ha.geo.booth.config.arbitrator">
     <para>&booth-arbitrator; In the examples above, we use the same
      arbitrator for different &geo; clusters.</para>
    </callout>
    <callout arearefs="co.ha.geo.booth.config.site">
     <para>&booth-site; The sites defined in both booth configuration files
      are different, because they belong to two different &geo; clusters.
     </para>
    </callout>
    <callout arearefs="co.ha.geo.booth.config.ticket">
     <para>&booth-ticket; Theoretically the same ticket names can be defined
      in different booth configuration files&mdash; the tickets will not
      interfere because they are part of different &geo; clusters that are
      managed by different booth instances. However, (for better overview), we
      advise to use distinct ticket names for each &geo; cluster as shown in
      the examples above.</para>
    </callout>
   </calloutlist>

   <procedure>
    <title>Using the Same Arbitrator for Different &geo; Clusters</title>
    <step>
     <para>Create different booth configuration files in
       <filename>/etc/booth</filename> as shown in <xref
       linkend="ex.ha.conf.booth.multi-1"/> and <xref
       linkend="ex.ha.conf.booth.multi-2"/>. Do so either manually or with
      &yast;, as outlined in <xref linkend="pro.ha.geo.setup.booth.yast"/>.
     </para>
    </step>
    <step>
     <para>On the arbitrator, open the ports that are defined in any of the
      booth configuration files in <filename>/etc/booth</filename>.</para>
    </step>
    <step>
     <para>On the nodes belonging to the individual &geo; clusters that the
      arbitrator is used for, open the port that is used for the respective
      booth instance.</para>
    </step>
    <step>
     <para>Synchronize the respective booth configuration files across all
      cluster nodes and arbitrators that use the same booth configuration. For
      details, see <xref linkend="sec.ha.geo.booth.sync"/>.</para>
    </step>
    <step>
     <para>On the arbitrator, start the individual booth instances as described
      in <xref linkend="vle.ha.geo.setup.booth.service.arbitrator"/> for
      multi-tenancy setups.</para>
    </step>
    <step>
     <para>On the individual &geo; clusters, start the booth service as
      described in <xref linkend="vle.ha.geo.setup.booth.service.sites"
      />.</para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.geo.booth.sync">
   <title>Synchronizing the Booth Configuration Across All Sites and
    Arbitrators</title>
   <para>To make booth work correctly, all cluster nodes and arbitrators within
    one &geo; cluster must use the same booth configuration. In case of any
    booth configuration changes, make sure to update the configuration files accordingly on
    all parties and to restart the booth services as described in <xref
     linkend="sec.ha.geo.setup.booth.reconfig"/>. </para>

   <note>
    <title>Synchronize Booth Configuration to All Sites and Arbitrators</title>
    <para>
     <remark>taroth 2014-08-11: todo - https://fate.suse.com/316223: [docu] sync
      and change config files (prio: mandatory)</remark> All cluster nodes and
     arbitrators within the &geo; cluster must use the same booth
     configuration. While you may need to copy the files manually to the
     arbitrators and to one cluster node per site, you can use &csync;
     within each cluster site to synchronize the file to all nodes. </para>
   </note>

  </sect2>

  <sect2 id="sec.ha.geo.setup.booth.service">
   <title>Enabling and Starting the Booth Services</title>
   <para>Booth uses a <literal>systemd</literal> unit file, called
     <filename>booth@.service</filename>. </para>
   <variablelist>
    <varlistentry id="vle.ha.geo.setup.booth.service.sites">
     <term>Starting the Booth Services on Cluster Sites</term>
     <listitem>
      <para>The booth service for each cluster site is managed by the booth
       resource group configured in <xref linkend="pro.ha.geo.setup.rsc.boothd"
       />.
       <!--taroth 2014-08-25: FIXME: check and add link to Hawk config, too -->
       To start one instance of the booth service per site, start the respective
       booth resource group on each cluster site.</para>
     </listitem>
    </varlistentry>
    <varlistentry id="vle.ha.geo.setup.booth.service.arbitrator">
     <term>Starting the Booth Services on Arbitrators</term>
     <!--taroth 2014-08-11: https://bugzilla.novell.com/show_bug.cgi?id=877817: [Test
      Case 1378526] [316123] booth - Configuring startup 'complicated'-->
     <listitem>
      <para>To <emphasis>enable</emphasis> the booth service on an arbitrator,
       use the following command:</para>
      <screen>systemctl enable booth@booth service</screen>
      <para>After the service has been enabled from command line, &yast;
       &ycc_runlevel; can then be used to manage the service, as long as it
       is not disabled. In that case, it will disappear from the service list in
       &yast; next time systemd is restarted.</para>
      <para>However, the command to <emphasis>start</emphasis> the booth service
       depends on your booth setup: </para>
      <itemizedlist>
       <listitem>
        <para>If you are using the default setup as described in <xref
          linkend="sec.ha.geo.booth.default"/>, only
          <filename>/etc/booth/booth.conf</filename> is configured. In that
         case, log in to each arbitrator and use the following command: </para>
        <screen>&prompt.root;<command>systemctl</command> start booth@booth</screen>
       </listitem>
       <listitem>
        <para>If you are running booth in multi-tenancy mode as described in
          <xref linkend="sec.ha.geo.booth.multi"/>, you have configured multiple
         booth configuration files in <filename>/etc/booth</filename>. To start
         the services for the individual booth instances, use
         <command>systemctl&nbsp;start&nbsp;booth@<replaceable>NAME</replaceable></command>,
         where <replaceable>NAME</replaceable> stands for the name of the
         respective configuration file
           <filename>/etc/booth/<replaceable>NAME</replaceable>.conf</filename>.</para>
        <para>For example, if you have the booth configuration files
          <filename>/etc/booth/emea.conf</filename> and
          <filename>/etc/booth/apac.conf</filename>, log in to your
         arbitrator and execute the following commands:</para>
        <screen>&prompt.root;<command>systemctl</command> start booth@emea
&prompt.root;<command>systemctl</command> start booth@apac</screen>
        <para>
         <remark>taroth 2014-08-25: DEVs, is it also possible to start the two
          services in one go when using the "@" syntax? if yes, how is the exact
          command?</remark>
        </para>
       </listitem>
      </itemizedlist>
      <para> This starts the booth service in arbitrator mode. It can
       communicate with all other booth daemons but in contrast to the booth
       daemons running on the cluster sites, it cannot be granted a ticket.
       Booth arbitrators take part in elections only. Otherwise, they are
       dormant.</para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 
  <sect2 id="sec.ha.geo.setup.booth.reconfig">
   <title>Reconfiguring Booth While Running</title>
   <!--taroth 2014-08-26: https://fate.suse.com/316126: 
    reconfiguration of boothd while running (prio: important)-->
   <para>In case you need to change the booth configuration while the booth
    services are already running, proceed as follows:
    <remark>taroth 2014-05-26: dejan, is the order of the steps correct?
     anything missing?</remark></para>
   <procedure>
    <step>
     <para>Adjust the booth configuration files as desired.</para>
    </step>
    <step>
     <para>Synchronize the updated booth configuration files to all cluster
      nodes and arbitrators that are part of your &geo; cluster. For
      details, see <xref linkend="sec.ha.geo.booth.sync"/>.</para>
    </step>
    <step>
     <!--taroth 2014-08-26: https://bugzilla.novell.com/show_bug.cgi?id=891399-->
     <para>Restart the booth services on the arbitrators and cluster sites as
      described in <xref linkend="sec.ha.geo.setup.booth.service"/>. This does
      not have any effect on tickets that have already been granted to
      sites.</para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 
 <sect1 id="sec.ha.geo.rsc">
   <title>Configuring Cluster Resources and Constraints</title>
  <para> Apart from the resources and constraints that you need to define for
   your specific cluster setup, &geo; clusters require additional resources
   and constraints as described below. You can either configure them with the
   &crmshell; (&crmsh;), or with the &haweb; (&hawk;).</para>
     
   <sect2 id="sec.ha.geo.rsc.cli">
      <title>From Command Line</title>
    
   <para> This section focuses on tasks specific to &geo; clusters. For a an
    introduction to the &crmshell; and general instructions on how to
    configure resources and constraints with &crmsh;, refer to
    the <citetitle>&haguide;</citetitle> for &productname;, chapter
     <citetitle>Configuring and Managing Cluster Resources (Command Line)</citetitle>. 
   </para>
   
   <procedure id="pro.ha.geo.setup.rsc.constraints">
    <title>Configuring Ticket Dependencies</title>
     &ticket-dependency-loss-policy;
     <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
      <!--<remark>toms 2011-10-04: Shorten it to "Start a root shell on one
       of the cluster nodes"? I guess we deal with admin-minded people here</remark>
       taroth 2011-10-06: due to new ACL support of the crm shell, also
       non-root users may have the rights to do so, therefore phrased it like
       that -->
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      &crmshell;.
     </para>
    </step>
    <step id="step.ha.geo.setup.rsc.constraints">
     <para>
      Configure a constraint that defines which resources depend on a
      certain ticket. For example:
     </para>
     <screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-ticketA ticketA: rsc1 \
  loss-policy="fence"</screen>
     <para>
      This creates a constraint with the ID
      <literal>rsc1-req-ticketA</literal>. It defines that the resource
      <literal>rsc1</literal> depends on <literal>ticketA</literal> and that
      the node running the resource should be fenced in case
      <literal>ticketA</literal> is revoked.
     </para>
     <para> Alternatively, you can configure resource <literal>rsc1</literal>
      not as a primitive, but a multi-state resource that can run in
       <literal>master</literal> or <literal>slave</literal> mode. In that case,
      make only <literal>rsc1</literal>'s master mode depend on
       <literal>ticketA</literal>. With the following configuration,
       <literal>rsc1</literal> is automatically demoted to
       <literal>slave</literal> mode if <literal>ticketA</literal> is revoked:
     </para>
     <screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-ticketA ticketA: rsc1:Master \
  loss-policy="demote"</screen>
    </step>
    <step>
     <para>
      If you want other resources to depend on further tickets, create as
      many constraints as necessary with <command>rsc_ticket</command>.
     </para>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
     <para>
      The constraints are saved to the CIB.
     </para>
     <!--For example, the command in
      <xref linkend="step.ha.geo.setup.rsc.constraints" xreflabel="nopage"/> would result in
      the following constraint configuration in the CIB:
      <screen>&lt;rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" role="Master" ticket="ticketA" loss-policy="fence"/></screen>-->
    </step>
   </procedure>
   <procedure id="pro.ha.geo.setup.rsc.boothd">
    <title>Configuring a Resource Group for <systemitem class="daemon"
     >boothd</systemitem></title>
    <para>
     Each site needs to run one instance of
     <systemitem class="daemon"
      >boothd</systemitem> that communicates
     with the other booth daemons. The daemon can be started on any node,
     therefore it should be configured as primitive resource. To make the
     <systemitem>boothd</systemitem> resource stay on the same node, if
     possible, add resource stickiness to the configuration. As each daemon
     needs a persistent IP address, configure another primitive with a
     virtual IP address. Group booth primitives:
    </para>
    <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      &crmshell;.
     </para>
    </step>
    <step>
     <para>
      Enter the following to create both primitive resources and to add them to one group,
      <literal>g-booth</literal>:
     </para>
     <screen>&prompt.crm.conf;<command>primitive</command> booth-ip ocf:heartbeat:IPaddr2 \
  params ip="<replaceable>IP_ADDRESS</replaceable>"
&prompt.crm.conf;<command>primitive</command> booth ocf:pacemaker:booth-site \
  meta resource-stickiness="INFINITY" \
  op monitor interval="10s" timeout="20s"
  group g-booth booth-ip booth</screen>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
    </step>
    <step>
     <para>
      Repeat the resource group configuration on the other cluster sites,
      using a different IP address for each <literal>boothd</literal>
      resource group.
     </para>
     <para>
      With this configuration, each booth daemon will be available at its
      individual IP address, independent of the node the daemon is running
      on.
     </para>
    </step>
   </procedure>
   <!--taroth 2012-02-14: fix for bnc#746863-->
   <procedure id="pro.ha.geo.setup.rsc.order">
    <title>Adding an Ordering Constraint</title>
    <para>
     If a ticket has been granted to a site but all nodes of that site
     should fail to host the <systemitem class="daemon">boothd</systemitem>
     resource group for any reason, a <quote>split-brain</quote> situation
     among the geographically dispersed sites could occur. In that case, no
     <systemitem class="daemon">boothd</systemitem> instance would be
     available to safely manage fail-over of the ticket to another site. To
     avoid a potential concurrency violation of the ticket (the ticket is
     granted to multiple sites simultaneously), add an ordering constraint:
    </para>
    <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      &crmshell;.
     </para>
    </step>
    <step>
     <para>
      Create an ordering constraint:
     </para>
     <screen>&prompt.crm.conf;<command>order</command> order-booth-rsc1 inf: g-booth rsc1</screen>
     <para>
      This defines that <literal>rsc1</literal> (that depends on
      <literal>ticketA</literal>) can only be started after the
      <literal>g-booth</literal> resource group.
     </para>
     <para>
      In case <literal>rsc1</literal> is not a primitive, but a special
      clone resource and configured as described in
      <xref
       linkend="step.ha.geo.setup.rsc.constraints"/> of 
      <xref linkend="pro.ha.geo.setup.rsc.constraints"/>, the
      ordering constraint should be configured as follows:
     </para>
     <screen>&prompt.crm.conf;<command>order</command> order-booth-rsc1 inf: g-booth rsc1:promote</screen>
     <para>
      This defines that <literal>rsc1</literal> can only be promoted to
      master mode after the <literal>g-booth</literal> resource group has
      started.
     </para>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      For any other resources that depend on a certain ticket, define
      further ordering constraints.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
    </step>
   </procedure>
   </sect2>
  
   <sect2 id="sec.ha.geo.rsc.hawk">
  <title>With the &haweb; (&hawk;)</title>
   <para> This section focuses on tasks specific to &geo; clusters. For an
    introduction to &hawk; and general instructions on how to configure
    resources and constraints with &hawk;, refer to the
     <citetitle>&haguide;</citetitle> for &productname;, chapter
     <citetitle>Configuring and Managing Cluster Resources (Web
     Interface)</citetitle>. </para>
   
    <procedure id="sec.ha.config.hawk.geo.rsc.constraints">
    <title>Configuring Ticket Dependencies</title>
    &ticket-dependency-loss-policy;
    <para>The following example shows two alternatives to configure the
     constraint: One with the resource being a primitive and
      <literal>loss-policy="fence"</literal>, the other one with the resource
     being a multi-state resource that can run in <literal>master</literal> or
      <literal>slave</literal> mode and with
      <literal>loss-policy="demote"</literal>.</para>
    <step>
     <para>
      Start a Web browser and log in to &hawk;.
     </para>
    </step>
    <step>
     <para>
      In the left navigation bar, select <guimenu>Constraints</guimenu>. The
      <guimenu>Constraints</guimenu> screen shows categories for all types
      of constraints and lists all defined constraints.
     </para>
    </step>
    <step>
     <para>
      To add a new ticket dependency, click the plus icon in the
      <guimenu>Ticket</guimenu> category.
     </para>
     <para>
      To modify an existing constraint, click the wrench icon next to the
      constraint and select <guimenu>Edit Constraint</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter a unique <guimenu>Constraint ID</guimenu>. When modifying
      existing constraints, the ID is already defined.
     </para>
    </step>
    <step>
     <para>
      Set a <guimenu>Loss Policy</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter the ID of the ticket that the resources should depend on.
     </para>
    </step>
    <step>
     <para>
      Select a resource from the list <guimenu>Add resource to
       constraint</guimenu>. The list shows the IDs of all resources and all
      resource templates configured for the cluster.
     </para>
    </step>
    <step>
     <para> To add the selected resource, click the plus icon next to the list.
      A new list appears beneath, showing the remaining resources. Add as many
      resources to the constraint as you would like to depend on the ticket. </para>
     <figure id="fig.hawk.ticket.dep.simple">
      <title>&hawk;&mdash;Ticket Dependency with <literal>loss-policy="fence"</literal></title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="hawk-ticket-dependency1.png" width="50%"
         format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="hawk-ticket-dependency1.png" width="50%"
         format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      <xref linkend="fig.hawk.ticket.dep.simple"/> shows a constraint with the
      ID <literal>rsc1-req-ticketA</literal>. It defines that the resource
       <literal>rsc1</literal> depends on <literal>ticketA</literal> and that
      the node running the resource should be fenced in case
       <literal>ticketA</literal> is revoked. </para>
     <para>If resource <literal>rsc1</literal> was not a primitive, but a
      multi-state resource, define that only
       <literal>rsc1</literal>'s master mode depends on
       <literal>ticketA</literal>. With the configuration shown in <xref
       linkend="fig.hawk.ticket.dep.adv"/>, <literal>rsc1</literal> is
      automatically demoted to <literal>slave</literal> mode if
       <literal>ticketA</literal> is revoked: </para>
     <figure id="fig.hawk.ticket.dep.adv">
      <title>&hawk;&mdash;Ticket Dependency with
        <literal>loss-policy="demote"</literal></title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="hawk-ticket-dependency2.png" width="60%"
         format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="hawk-ticket-dependency2.png" width="50%"
         format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Click <guimenu>Create Constraint</guimenu> to finish the
      configuration. A message at the top of the screen shows if the
      constraint was successfully created.
     </para>
    </step>
   </procedure>
  
 </sect2>
  </sect1>
 
  <sect1 id="sec.ha.geo.manage">
   <title>Managing &geo; Clusters</title>
   <para> Before booth can manage a certain ticket within the &geo;
    cluster, you initially need to grant it to a site manually.
    <remark>taroth 2013-12-13: work in progress, check
     if everything that can be done with CLI can also be done with Hawk (keep
     both sections in sync, if possible)</remark>
   </para>
   <sect2 id="sec.ha.geo.manage.cli">
 <title>From Command Line</title>
   
   <para>Use the <command>booth&nbsp;client</command> command line tool to grant, list, or
    revoke tickets as described in <xref linkend="vl.ha.booth.client.cmds"/>. The
    <command>booth&nbsp;client</command> commands can be run on any machine in the cluster, not
    only the ones having the &boothd; running. The <command>booth&nbsp;client</command>
    commands try to find the <quote>local</quote> cluster by looking at the booth configuration file
    and the locally defined IP addresses. If you do not specify a site which the booth client should
    connect to (using the <option>-s</option> option), it will always connect to the local site. </para>
    <note>
    <title>Syntax Changes</title>
    <para>The syntax of booth clients commands has been simplified since &productname; 11: For
     example, the <literal>client</literal> keyword can be omitted for <option>list</option>,
      <option>grant</option>, or <option>revoke</option> operations: <command>booth list</command>.
     Also, the <option>-t</option> option can be omitted when specifying a ticket. </para>
    <para>The former syntax is still supported. For detailed information, see the
      <literal>Synopsis</literal> section in the booth man page. However, the examples in this
     manual use the simplified syntax. </para>
    </note>

   <!--taroth 2013-04-24: information taken from bnc#752601, c#17-->
    <variablelist id="vl.ha.booth.client.cmds">
    <title>Overview of <command>booth client</command> Commands</title>
    <varlistentry>
     <term>Listing All Tickets</term>
     <listitem>
      <screen>&prompt.root; <command>booth</command> list
ticket: ticketA, leader: none
ticket: ticketB, leader: 10.2.12.101, expires: 2014-08-13 10:28:57
      </screen>
      <para>If you do not specify a certain site with <option>-s</option>, the information about the
       tickets will be requested from the local booth instance.</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Granting a Ticket to a Site</term>
     <listitem>
      <screen>&prompt.root; <command>booth</command> grant -s 147.2.207.14 ticketA
booth[27891]: 2014/08/13_10:21:23 info: grant request sent, waiting for the result ...
booth[27891]: 2014/08/13_10:21:23 info: grant succeeded!</screen>
      <para> In this case, <literal>ticketA</literal> will be granted to the site
        <literal>147.2.207.14</literal>. If you omit the <option>-s</option> option, booth will
       automatically connect to the current site (the site you are running the booth client on) and
       will request the <command>grant</command> operation. </para>
      <para> Before granting a ticket, the command will execute a sanity check. If the same ticket
       is already granted to another site, you will be warned about that and be prompted to revoke
       the ticket from the current site first. </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Revoking a Ticket From a Site</term>
     <listitem>
      <screen>&prompt.root; <command>booth</command> client revoke -s 147.2.207.14 ticketA
booth[27900]: 2014/08/13_10:21:23 info: revoke succeeded!</screen>
      <para> In this case, <literal>ticketA</literal> will be revoked from the
       site <literal>147.2.207.14</literal>. If you omit the <option>-s</option> option, booth will
       automatically connect to the current site (the site you are running the booth client
       on) and will request the <command>revoke</command> operation.  The revoke operation will be
       executed immediately. However, it might not be finished yet when the
       message above appears on the screen. Find the exact status in the log
       files. </para>
     </listitem>
    </varlistentry>
   </variablelist>

   <para>The <command>grant</command> and, under certain circumstances, <command>revoke</command>
    operations may take a while to return a definite operation's outcome. The client will wait for
    the result up to the ticket's <varname>timeout</varname> value before he gives up
    waiting&mdash;unless the <option>-w</option> option was used, in which case the client waits
    indefinitely. Find the exact status in the log files or with the
    <command>crm_ticket -L</command> command.</para>


   <warning>
    <title><command>crm_ticket</command> and
     <command>crm&nbsp;site&nbsp;ticket</command></title>
    <para> In case the booth service is not running for any reasons, you may
     also manage tickets manually with <command>crm_ticket</command> or
     <command>crm&nbsp;site&nbsp;ticket</command>. Both commands are
     only available on cluster nodes. In case of manual intervention, use them
     with great care as they <emphasis>cannot</emphasis> verify if the same
     ticket is already granted elsewhere. For basic information about the
     commands, refer to their man pages. </para>
    <para> As long as booth is up and running, only use
     <command>booth&nbsp;client</command> for manual intervention. </para>
   </warning>

   <para> After you have initially granted a ticket to a site, the booth
    mechanism will take over and manage the ticket automatically. If the site
    holding a ticket should be out of service, the ticket will automatically be
    revoked after the expiry time and granted to another site. The resources
    that depend on that ticket will fail over to the new site holding the
    ticket. The nodes that have run the resources before will be treated
    according to the <literal>loss-policy</literal> you set within the
    constraint. </para>
    <remark>Phil 2013-12: TODO-  Mostly booth should be used to avoid split-brains.
     Whether/how automatic failover should/does happen is TBD.
    </remark>

   <!--taroth 2013-04-24: fix for bnc#752601, c#10-->

   <procedure id="pro.ha.geo.manage.tickets">
    <title>Managing Tickets Manually</title>
    <para> Assuming that you want to manually move <literal>ticketA</literal>
     from site <literal>147.2.207.14</literal> to
     <literal>&smbip;</literal>, proceed as follows: </para>
    <step>
     <para> Set <literal>ticketA</literal> to standby with the following
      command: </para>
     <screen>&prompt.root;<command>crm_ticket</command> -t ticketA -s</screen>
    </step>
    <step>
     <para> Wait for any resources that depend on <literal>ticketA</literal> to
      be stopped or demoted cleanly. </para>
    </step>
    <step>
     <para> Revoke <literal>ticketA</literal> from its current site with: </para>
     <screen>&prompt.root;<command>booth</command> revoke -s 147.2.207.14 ticketA</screen>
    </step>
    
    <step>
     <para> After the ticket has been revoked from its original site, grant it
      to the new site with: </para>
     <screen>booth grant -s &smbip; ticketA</screen>
    </step>
   </procedure>
    <remark>phil 2013-12: Will/Should get a cleaner, more integrated workflow - "booth client move -t ticket -s new-site"
     or something similar. - dmuhamedagic 2014-08-13: N/A</remark>
    </sect2>
  
  <sect2 id="sec.ha.geo.manage.hawk">
   <title>With the &haweb; (&hawk;)</title>
  <para>
    <remark>taroth 2013-12-13: work in progress, add introductory para and check
     if everything that can be done with CLI can also be done with Hawk (keep
     both sections in sync, if possible), todo - https://fate.suse.com/316119:
     hawk: display GEO setup (prio: important)</remark>
   </para>
   <sect3 id="sec.ha.config.hawk.geo.ticket">
    <title>Viewing Tickets</title>
    <procedure id="pro.ha.config.hawk.viewtickets">
     <title>Viewing Tickets with &hawk;</title>
     <para>
      Tickets are visible in &hawk; if they have been granted or revoked at
      least once or if they are referenced in a ticket dependency&mdash;see
      <xref
       linkend="sec.ha.config.hawk.geo.rsc.constraints"/>. In case
      a ticket is referenced in a ticket dependency, but has not been granted
      to any site yet, &hawk; displays it as <literal>revoked</literal>.
     </para>
     <step>
      <para>
       Start a Web browser and log in to the cluster.<!-- as described in
        <xref
        linkend="sec.ha.config.hawk.intro.connect"/>.-->
      </para>
     </step>
     <step>
      <para>
       In the left navigation bar, select <guimenu>Cluster Status</guimenu>.
      </para>
     </step>
     <step>
      <para>
       If the <guimenu>Summary View</guimenu> is not already active, click
       the respective view icon on the upper right-hand side. Along with
       information about cluster nodes and resources, &hawk; also displays a
       <guimenu>Ticket</guimenu> category.
      </para>
     </step>
     <step>
      <para>
       For more details, either click the title of the
       <guimenu>Ticket</guimenu> category or the individual ticket entries
       that are marked as links. &hawk; displays the ticket's name and, in a
       tooltip, the last time the ticket has been granted to the current
       site.
      </para>
      <figure>
       <title>&hawk; Cluster Status (Summary View)&mdash;Ticket Details</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk-summary-tickets.png" width="100%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk-summary-tickets.png" width="80%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
    </procedure>
    <note>
     <title>Managing Tickets</title>
     <para>
      To grant or revoke tickets, use the <command>booth client</command>
      command as described in <xref linkend="sec.ha.geo.manage.cli"/>. As
      managing tickets takes place on an <quote>inter-cluster</quote> layer,
      you cannot do so with &hawk;.
     </para>
    </note>
   </sect3>
   
   <sect3 id="sec.ha.config.hawk.geo.simulator">
    <title>Testing the Impact of Ticket Failover</title>
    <para>
     &hawk;'s <guimenu>Simulator</guimenu> allows you to explore failure
     scenarios before they happen. To explore if your resources that depend
     on a certain ticket behave as expected, you can also test the impact of
     granting or revoking tickets.
    </para>
    <procedure id="pro.ha.config.hawk.geo.simulator">
     <title>Simulating Granting and Revoking Tickets</title>
     <step>
      <para>
       Start a Web browser and log in to the cluster. <!--as described in
        <xref
        linkend="sec.ha.config.hawk.intro.connect"/>.-->
      </para>
     </step>
     <step>
      <para>
       Click the wrench icon next to the username in the top-level row, and
       select <guimenu>Simulator</guimenu>.
      </para>
      <para>
       &hawk;'s background changes color to indicate the simulator is active.
       A simulator dialog opens in the bottom right hand corner of the
       screen. Its title <guimenu>Simulator (initial state)</guimenu>
       indicates that <guimenu>Cluster Status</guimenu> screen still reflects
       the current state of the cluster.
      </para>
     </step>
     <step>
      <para>
       To simulate status change of a ticket:
      </para>
      <substeps>
       <step>
        <para>
         Click <guimenu>+Ticket</guimenu> in the simulator control dialog.
        </para>
       </step>
       <step>
        <para>
         Select the <guimenu>Action</guimenu> you want to simulate.
        </para>
       </step>
       <step>
        <para>
         Confirm your changes to add them to the queue of events listed in
         the controller dialog below <guimenu>Injected State</guimenu>.
        </para>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       To start the simulation, click <guimenu>Run</guimenu> in the simulator
       control dialog. The <guimenu>Cluster Status</guimenu> screen displays
       the impact of the simulated events. The simulator control dialog
       changes to <guimenu>Simulator (final state)</guimenu>.
      </para>
     </step>
     <step>
      <para>
       To exit the simulation mode, close the simulator control dialog. The
       <guimenu>Cluster Status</guimenu> screen switches back to its normal
       color and displays the current cluster state.
      </para>
     </step>
    </procedure>
    <figure>
     <title>&hawk;Simulator&mdash;Tickets</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk-simulator-tickets.png" width="100%" format="PNG"/>
      </imageobject>  
      <imageobject role="html">
       <imagedata fileref="hawk-simulator-tickets.png" width="80%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     For more information about &hawk;'s <guimenu>Simulator</guimenu> (and
     which other scenarios can be explored with it), refer to the <citetitle>&haguide;</citetitle> for
     &productname;, available from &suse-onlinedoc;. Refer to chapter
     <citetitle>Configuring and Managing Cluster Resources (Web Interface)</citetitle>, section
     <citetitle> Exploring Potential Failure Scenarios</citetitle>.</para>
   </sect3>
  </sect2>
</sect1>
<sect1 id="sec.ha.geo.trouble">
<!--taroth 2013-04-24: fix for bnc#753625-->

  <title>Troubleshooting</title>

  <para>
   Booth <!--logs to <filename>/var/log/messages</filename> and -->uses the same
   logging mechanism as the CRM. Thus, changing the log level will also take
   effect on booth logging. The booth log messages also contain information
   about any tickets.
  </para>

  <para>
   Both the booth log messages and the booth configuration file are included
   in the <command>hb_report</command> and <command>crm_report</command>.
  </para>

  <para> In case of unexpected booth behavior or any problems, check the logging
   data with <command>sudo journalctl -n</command> or create a detailed cluster
   report with either <command>hb_report</command> or
   <command>crm_report</command>. </para>
 </sect1>
 <sect1 id="sec.ha.geo.upgrade">
 <title>Upgrading to the Latest Product Version</title>
  <para>For general instructions on how to upgrade a cluster, see the
    <citetitle>&haguide;</citetitle> for &productname;
   &productnumber;. It is available at &suse-onlinedoc;. The chapter
    <citetitle>Upgrading Your Cluster and Updating Software
   Packages</citetitle> also describes which preparations to take care of
   before starting the upgrade process.</para>
  <sect2 id="sec.ha.geo.upgrade.sle12">
   <title>Upgrading from SLE&nbsp;HA&nbsp;11 SP3 to
    SLE&nbsp;HA&nbsp;12</title>
   <para>
    <remark>taroth 2014-08-26: dejan, I studied your upgrade instructions for
     GEO at
     https://mailman.suse.de/mailman/private/ha-devel/2014-August/004140.html.
     But because the cluster stack needs to be stopped on all nodes anyway
     before upgrading from SLE* 11 to SLE*12 (rolling upgrade is *not* supported
     in this case), I'm not sure in how far the steps for removing dependencies
     (and re-adding them) later on apply here... Please advise and let me know
     if to resort, add or remove any steps.</remark></para>
   <procedure>
    <step>
     <para>For an upgrade of the cluster nodes from &productname; 11 SP3 to
      &productname; 12, follow the instructions in the
       <citetitle>&haguide;</citetitle> for &productname;
      &productnumber;, section <citetitle> Upgrading from SLE HA 11 SP3 to
       SLE HA 12</citetitle>. </para>
    </step>
    <step>
     <para>If you use arbitrators outside of the cluster sites:</para>
     <substeps>
      <step>
       <para>Upgrade them from &sls; 11&nbsp;SP3 to &sls; 12,
       too.</para>
      </step>
      <step>
       <para>Add the &hageo; add-on and install the packages as described in
         <xref linkend="sec.ha.geo.inst.arbitrators"/>.
       </para>
      </step>
     </substeps>
     </step>
    <step>
     <para>Because the syntax and the consensus algorithm for booth has changed,
      you need to update the booth configuration files to match the latest requirements. See
       <xref linkend="sec.ha.geo.booth"/> for details.</para>
    </step>  
    <step>
     <para>Synchronize the updated booth configuration files across all cluster sites
     and arbitrators.</para>
    </step>
    <step>
     <para>Start the booth service on the cluster sites and the arbitrators as
      described in <xref linkend="sec.ha.geo.setup.booth.service"/>. </para>
    </step>
   </procedure>
   
   
   
 </sect2>
 
  <!--taroth 2014-08-26: 
  https://mailman.suse.de/mailman/private/ha-devel/2014-August/004140.html:
  
  The SLE12 booth most probably cannot talk to the SLE11 booth,
  since the previous consensus algorythm (paxos) got replaced.
  
  Hence, it is more involved. The best way would be to take the
  booth completely out of picture during upgrade.
  
  The backup site can be upgraded without any special extra steps
  for GEO. The only thing not to forget is to convert the booth
  configuration.
  
  Of course, particular attention needs the active site, i.e. the
  site where the protected resources are currently running. I'd
  propose the following procedure for the active site:
  
  ==========
  
  1. Remove the dependency on the booth resource and on the
  ticket.
  
  Our GEO quick guide recommends adding an order constraint
  between the booth group and the protected resource:
  
  order order-booth-rsc1 inf: g-booth rsc1:promote
  
  That constraint should be removed. All rsc_ticket constraints
  should be removed too.
  
  The protected resource will continue to run uninterrupted.
  
  2. Upgrade booth on all arbitrators and modify the booth
  configuration to match the new format. Start booth on all
  arbitrators and make sure that it's running.
  
  3. Update the booth configuration files on all nodes at this
  site.
  
  4. Update the cluster stack and booth.
  
  5. Once the cluster stack and all resources are running, use
  booth list to verify that the ticket is still granted to this
  site. If it is not, then grant the ticket. Also verify that the
  arbitrators have booth list output consistent with the site.
  
  6. Add back the order and rsc_ticket constraints removed in
  step 1.
  
  ==========
  
  On the inactive site, we'll have just the booth configuration
  file modifications. The rest shouldn't matter. -->
</sect1>
 
 
</article>
