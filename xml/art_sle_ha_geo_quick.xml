<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:novdoc-profile.xsl"
  type="text/xml" 
  title="Profiling step"?>
<!DOCTYPE article PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN"
                      "novdocx.dtd"
[
<!ENTITY % NOVDOC.DEACTIVATE.IDREF "IGNORE">
<!ENTITY % entities SYSTEM "entity-decl.ent">
%entities;
]>
<!--taroth 2014-08-07: check screens (and add line-breaks)-->
<?provo dirname="geo_quick/"?>
<article lang="en" id="art.ha.geo.quick">
<?suse-quickstart columns="no" version="2"?>
 <title>&geoquick;</title>
<!-- <subtitle>GEO Clustering for &productname;
 </subtitle>-->
 <articleinfo><productname>GEO Clustering for &productname;</productname>
  <productnumber>&productnumber;</productnumber>
  <authorgroup>
   <author><firstname>Tanja</firstname><surname>Roth</surname>
   </author>
  </authorgroup>
 </articleinfo>
 <abstract>
  <para> Apart from local clusters and metro area clusters, &productnamereg;
   &productnumber; also supports &geo; clusters. That means you can have
   multiple, geographically dispersed sites with a local cluster each. Failover
   between these clusters is coordinated by a higher level entity: the booth
   daemon (&boothd;). Support for &geo; clusters is available as
   a separate extension to &hasi;, called &hageo;. </para>
  <para>
   <remark>taroth 2014-08-11: todo - https://fate.suse.com/316120: [docu]
    Quickstart guide for SLE HA GEO, for DRBD-related part, see also 
    https://mailman.suse.de/mailman/private/ha-devel/2014-February/003095.html
    for input)</remark>
   <remark>taroth 2014-08-11: todo - https://fate.suse.com/316114: Replicated
    storage solution based on DRBD (NEEDINFO, input probably 
    https://mailman.suse.de/mailman/private/ha-devel/2014-February/003095.html)</remark>
  </para>
 </abstract>
 <para>
  <remark>taroth 2014-08-11: https://fate.suse.com/316112: IP relocation via DNS
   update (prio: mandatory): NEEDINFO - check with devs about doc impact, where/in which
   scenario is it visible for the user? </remark>
 </para>
 <sect1 id="sec.ha.geo.inst">
  <title>Installation as Add-on</title>
  <para>For using the &hasi; and &hageo;, you need the packages included
   in the following installation patterns:</para>
  
  <itemizedlist>
   <listitem>
    <para>
     <literal>&ha;</literal>
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>&geo; Clustering for &ha;</literal>
    </para>
   </listitem>
  </itemizedlist>
  
  <note>
   <title>Package Requirements for Arbitrators</title>
   <para>If your &geo; cluster setup includes one ore more arbitrators (see
    <xref linkend="vle.ha.geo.components.arbitrator"/>), those only need the
    pattern <literal>&geo; Clustering for &ha;</literal>. For instructions on
    how to install this pattern, see 
    <xref linkend="sec.ha.geo.inst.arbitrators"/>.</para>
  </note>
  
 <para>Both patterns are only available if you have registered your system at
   &scc; (or a local registration server) and have added the respective
   product channels or installation media as add-ons. For information on how to
   install add-on products, see the <citetitle>&sle; &productnumber;
   &deploy;</citetitle>, available at &suse-onlinedoc;. 
   Refer to chapter <citetitle>Installing Add-On Products</citetitle>.
   <!--taroth: need to use hard-coded link here as the target is not included in the same set-->
 </para>
  
  <sect2 id="sec.ha.geo.inst.nodes">
   <title>Installing the Packages on Cluster Nodes</title>

   <para>In case both &hasi; and &hageo; have been added as add-on
    products, but the packages are not installed yet, proceed as follows:</para>

   <procedure>
    <step>
     <para>To install the packages from both patterns via command line, use
      zypper:</para>
     <screen>sudo <command>zypper</command> in -t pattern ha_sles ha_geo</screen>
    </step>
    <step id="step.ha.geo.inst.yast">
     <para>Alternatively, use &yast; for a graphical installation:</para>
     <substeps>
      <step>
       <para> Start &yast; as &rootuser; user and select <menuchoice>
         <guimenu>Software</guimenu>
         <guimenu>Software Management</guimenu>
        </menuchoice>. </para>
      </step>
      <step>
       <para> Click <menuchoice>
         <guimenu>View</guimenu>
         <guimenu>Patterns</guimenu>
        </menuchoice> and activate the following patterns:</para>
       <itemizedlist>
        <listitem>
         <para>
          <literal>&ha;</literal>
         </para>
        </listitem>
        <listitem>
         <para>
          <literal>&geo; Clustering for &ha;</literal>
         </para>
        </listitem>
       </itemizedlist>
      </step>
      <step>
       <para> Click <guimenu>Accept</guimenu> to start installing the packages.
       </para>
      </step>
     </substeps>
    </step>
   </procedure>

   <important>
    <para> The software packages needed for &ha; and &geo; clusters are
      <emphasis>not</emphasis> automatically copied to the cluster nodes. </para>
    <itemizedlist>
     <listitem>
      <para> Install &sls; &productnumber; and the
        <literal>&ha;</literal> and <literal>&geo; Clustering for
        &ha;</literal> patterns on <emphasis>all</emphasis> machines that
       will be part of your &geo; cluster. </para>
     </listitem>
     <listitem>
      <para> If you do not want to install the packages manually on all nodes
       that will be part of your cluster, use &ay; to clone existing nodes.
       Find more information in the <citetitle>&haguide;</citetitle> for
       &productname; &productnumber;, available from
       &suse-onlinedoc;. Refer to chapter <citetitle>Installation and Basic
        Setup</citetitle>, section <citetitle>Mass Deployment with
        &ay;</citetitle>. </para>
      <para>For all machines that need the &hageo; add-on, you currently need to install the
       packages for &geo; clusters manually as &ay; support for &hageo; is not yet available.
      </para>
     </listitem>
    </itemizedlist>
   </important>
  </sect2>
  <sect2 id="sec.ha.geo.inst.arbitrators">
   <title>Installing the Packages on Arbitrators</title>
   <procedure>
    <step>
     <para>Make sure that &hageo; has been added as add-on product to the
      machines to serve as arbitrators.</para>
    </step>
    <step>
     <para>Log in to each arbitrator and install the packages with the following
      command:</para>
     <screen>sudo <command>zypper</command> in -t pattern ha_geo</screen>
     <para>Alternatively, use &yast; to install the <literal>&geo;
       Clustering for &ha;</literal> pattern. </para>
    </step>
    </procedure>
  </sect2>
</sect1>
 
 
 <sect1 id="sec.ha.geo.challenges">
  <title>Challenges for &geo; Clusters</title>

  <para> Typically, &geo; environments are too far apart to support
   synchronous communication between the sites. That leads to the following
   challenges: </para>

  <itemizedlist>
   <listitem>
    <para>
     How to make sure that a cluster site is up and running?
    </para>
   </listitem>
   <listitem>
    <para>
     How to make sure that resources are only started once?
    </para>
   </listitem>
   <listitem>
    <para>
     How to make sure that quorum can be reached between the different sites
     and a split brain scenario can be avoided?
    </para>
   </listitem>
<!--<listitem>
    <para>How to keep the CIB up-to-date on all nodes and sites?</para>
   </listitem>
   -->
   <listitem>
    <para>
     How to manage failover between the sites?
    </para>
   </listitem>
   <listitem>
    <para>
     How to deal with high latency in case of resources that need to be
     stopped?
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In the following sections, learn how to meet these challenges with
   &productname;.
  </para>
 </sect1>
 <sect1 id="sec.ha.geo.concept">
  <title>Conceptual Overview</title>

  <para> &geo; clusters based on &productname; can be considered as <quote>overlay</quote>
   clusters where each cluster site corresponds to a cluster node in a traditional cluster. The
   overlay cluster is managed by the booth mechanism. It guarantees that the cluster resources will
   be highly available across different cluster sites. This is achieved by using cluster objects called tickets 
   that are treated as failover domain between cluster sites, in case a site should be down. Booth
   guarantees that every ticket is owned by only one site at the time.</para>

  <para>
   The following list explains the individual components and mechanisms that
   were introduced for &geo; clusters in more detail.
  </para>

  <variablelist id="vl.ha.geo.components">
   <title>Components and Ticket Management</title>
   <varlistentry id="vle.ha.geo.components.ticket">
    <term>Ticket</term>
    <listitem>
     <para>
      A ticket grants the right to run certain resources on a specific
      cluster site. A ticket can only be owned by one site at a time.
      Initially, none of the sites has a ticket&mdash;each ticket must be
      granted once by the cluster administrator. After that, tickets are
      managed by the booth for automatic failover of resources. But
      administrators may also intervene and grant or revoke tickets
      manually.
     </para>
     <para>After a ticket is administratively revoked, it is not managed by booth anymore. For booth
      to start managing the ticket again, the ticket must be again granted to a site.</para>
     <para>
      Resources can be bound to a certain ticket by dependencies. Only if
      the defined ticket is available at a site, the respective resources
      are started. Vice versa, if the ticket is removed, the resources
      depending on that ticket are automatically stopped.
     </para>
     <para>
      The presence or absence of tickets for a site is stored in the CIB as
      a cluster status. With regards to a certain ticket, there are only two
      states for a site: <literal>true</literal> (the site has the ticket)
      or <literal>false</literal> (the site does not have the ticket). The
      absence of a certain ticket (during the initial state of the
      &geo; cluster) is not treated differently from the situation
      after the ticket has been revoked: both are reflected by the value
      <literal>false</literal>.
     </para>
     <para>
      A ticket within an overlay cluster is similar to a resource in a
      traditional cluster. But in contrast to traditional clusters, tickets
      are the only type of resource in an overlay cluster. They are
      primitive resources that do not need to be configured nor cloned.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.booth">
    <term>Booth</term>
    <listitem>
     <para>
      Booth is the instance managing the ticket distribution and thus,
      the failover process between the sites of a &geo; cluster. Each
      of the participating clusters and arbitrators runs a service, the
      &boothd;. It connects
      to the booth daemons running at the other sites and exchanges
      connectivity details. Once a ticket is granted to a site, the booth
      mechanism can manage the ticket automatically: If the site which
      holds the ticket is out of service, the booth daemons will vote which
      of the other sites will get the ticket. To protect against brief
      connection failures, sites that lose the vote (either explicitly or
      implicitly by being disconnected from the voting body) need to
      relinquish the ticket after a time-out. Thus, it is made sure that a
      ticket will only be re-distributed after it has been relinquished by
      the previous site. See also
      <xref
       linkend="vle.ha.geo.components.deadman"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.arbitrator">
    <term>Arbitrator</term>
    <listitem>
     <para>
      Each site runs one booth instance that is responsible for
      communicating with the other sites. If you have a setup with an even
      number of sites, you need an additional instance to reach consensus
      about decisions such as failover of resources across sites. In this
      case, add one or more arbitrators running at additional sites.
      Arbitrators are single machines that run a booth instance in a special
      mode. As all booth instances communicate with each other, arbitrators
      help to make more reliable decisions about granting or revoking
      tickets. Arbitrators cannot hold any tickets.
     </para>
     <para>
      An arbitrator is especially important for a two-site scenario: For
      example, if site <literal>A</literal> can no longer communicate with
      site <literal>B</literal>, there are two possible causes for that:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        A network failure between <literal>A</literal> and
        <literal>B</literal>.
       </para>
      </listitem>
      <listitem>
       <para>
        Site <literal>B</literal> is down.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      However, if site <literal>C</literal> (the arbitrator) can still
      communicate with site <literal>B</literal>, site <literal>B</literal>
      must still be up and running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Ticket Failover</term>
    <listitem>
     <para>If the ticket gets lost, that means other boot instances do not hear from the ticket
      owner in a sufficiently long time, one of the remaining sites will acquire the ticket. This is
      what is called ticket failover. If the remaining members cannot form a majority, then the
      ticket cannot fail over. </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.deadman">
    <term>Dead Man Dependency (<literal>loss-policy="fence"</literal>)</term>
    <listitem>
     <para>
       After a ticket is revoked, it can take a long time until all resources
      depending on that ticket are stopped, especially in case of cascaded
      resources. To cut that process short, the cluster administrator can
      configure a <literal>loss-policy</literal> (together with the ticket
      dependencies) for the case that a ticket gets revoked from a site. If
      the loss-policy is set to <literal>fence</literal>, the nodes that are
      hosting dependent resources are fenced.  
     </para>
     <warning>
      <title>Potential Loss of Data</title>
      <para>On the one hand, <literal>loss-policy="fence"</literal> considerably
       speeds up the recovery process of the cluster and makes sure that
       resources can be migrated more quickly. </para>
      <para>On the other hand, it can lead to loss of all unwritten data, such
       as:</para>
      <itemizedlist>
       <listitem>
        <para>Data lying on shared storage (for example, DRBD).</para>
       </listitem>
       <listitem>
        <para>Data in a replicating database (for example, MariaDB or
         PostgreSQL) that has not yet reached the other site, due to a slow
         network link.</para>
       </listitem>
      </itemizedlist>
     </warning>
    </listitem>
   </varlistentry>
  </variablelist>

<!--taroth 201110-06: todo - ask eugene to redo the svg graphic 
  (shows some strange artefacts in PDF, maybe due to import from PDF)-->

  <figure>
   <title>Example Scenario: A Two-Site Cluster (4 Nodes + Arbitrator)</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_geocluster.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_geocluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>The most common scenario is probably a &geo; cluster with two sites
   and a single arbitrator on a third site. The upper limit is (currently) 16
   booth instances. </para>

  <para>
   As usual, the CIB is synchronized within each cluster, but it is not
   synchronized across cluster sites of a &geo; cluster. You have to
   configure the resources that will be highly available across the
   &geo; cluster for every site accordingly. <remark>taroth 2014-08-07: todo -
    https://fate.suse.com/316118:  CIB replication between sites (NEEDINFO, see
    fate c#21)</remark>
  </para>
 </sect1>
 <sect1 id="sec.ha.geo.req">
  <title>Requirements</title>

  <itemizedlist>
   <title>Software Requirements</title>
   <listitem>
    <para>
     All clusters that will be part of the &geo; cluster must be based
     on &productname; &productnumber;.
    </para>
   </listitem>
   <listitem>
    <para>
     &slsreg; &productnumber; must be installed on all arbitrators.
    </para>
   </listitem>
   <listitem>
    <para> The &hageo; add-on must be installed on all cluster nodes <emphasis>and</emphasis> on
     all arbitrators that will be part of the &geo; cluster. 
     <!--<remark>taroth 2014-08-20: booth package would be enough (GEO pattern only consists of the booth
      package) but to make it clear where to get the package from, phrasing like this</remark>--></para>
   </listitem>
  </itemizedlist>
  
  <itemizedlist>
   <title>Network Requirements</title>
   <listitem>
    <para>The sites must be reachable on one UDP and TCP port per booth
     instance. That means any firewalls or IPSec tunnels in between must be
     configured accordingly. </para>
   </listitem>
   <listitem>
    <para>Other setup decision may require to allow more open ports (for
     example, for DRBD or database replication). </para>
   </listitem>
  </itemizedlist>
  
  <itemizedlist>
   <title>Other Requirements and Recommendations</title>
   <listitem>
    <para>All cluster nodes on all sites should synchronize to an NTP server
     outside the cluster. For more information, see the
      <citetitle>&admin;</citetitle> for &sls; &productnumber;,
     available at &suse-onlinedoc;. Refer to the chapter <citetitle>Time
      Synchronization with NTP</citetitle>. </para>
      <para> If nodes are not synchronized, log files and cluster reports are
       very hard to analyze. </para>
     </listitem>
    </itemizedlist>
  

 
 </sect1>
 <sect1 id="sec.ha.geo.oview">
  <title>Basic Setup&mdash; Overview</title>

  <para>
   Configuring a &geo; cluster takes the following basic steps:
   </para>
  <variablelist>
    <varlistentry>
<!--Setting Up the Booth Services-->
    <term><xref linkend="sec.ha.geo.booth" xrefstyle="select:title"/>
    </term>
    <listitem>
     <orderedlist>
      <listitem>
<!--create booth config-->
       <para>
      <xref linkend="sec.ha.geo.booth" xrefstyle="select:title"/>
       </para>
      </listitem>
      <listitem>
       <!--sync booth config-->
       <para>
        <xref linkend="sec.ha.geo.booth.sync" xrefstyle="select:title"/>
       </para>
      </listitem>
      <listitem>
<!--start boothd-->
       <para>
        <xref linkend="sec.ha.geo.setup.booth.service" xrefstyle="select:title"
        />
       </para>
      </listitem>
     </orderedlist>
    </listitem>
    </varlistentry>
   <varlistentry>
    <!--Configuring Cluster Resources and Constraints-->
     <term>
      <xref linkend="sec.ha.geo.rsc" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>Use either &crmsh; or &hawk; for the following steps:</para>
     <orderedlist>
      <listitem>
       <para>Configuring Ticket Dependencies
        <!--<xref linkend="pro.ha.geo.setup.rsc.constraints"
         xrefstyle="select:title"/>-->
       </para>
      </listitem>
      <listitem>
       <para>Configuring a Resource Group for <systemitem class="daemon"
         >boothd</systemitem>
        <!--<xref linkend="pro.ha.geo.setup.rsc.boothd" xrefstyle="select:title"/>-->
       </para>
      </listitem>
      <listitem>
       <para>
        Adding an Ordering Constraint for <systemitem class="daemon"
         >boothd</systemitem> and the Resource Group
        <!--<xref linkend="pro.ha.geo.setup.rsc.order" xrefstyle="select:title"/>-->
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
  </variablelist>
  </sect1>

 <sect1 id="sec.ha.geo.booth">
  <title>Setting Up the Booth Services</title>
  <para>The default booth configuration is &booth.conf;. This
   file must be the same on all sites of your &geo; cluster, including the
   arbitrator or arbitrators. To keep the booth configuration synchronous across
   all sites and arbitrators, you can use &csync;. <remark>taroth
    2014-08-25: todo - describe how to do so and add xref to this
    section</remark></para>
  <para>&booth-multi-tenancy; For details on how to configure booth for
   multiple &geo; clusters, refer to <xref linkend="sec.ha.geo.booth.multi"
   />. </para>
  
  <sect2 id="sec.ha.geo.booth.default">
   <title>Default Booth Setup</title>
   <para>To configure all parameters needed for booth, either edit the booth
    configuration files manually or by using the &yast; <guimenu>Geo
     Cluster</guimenu> module. To access the &yast; module, start it from
    command line with <command>yast2
     geo-cluster</command> (or start
    &yast; and select <menuchoice>
     <guimenu>High Availability</guimenu>
     <guimenu>Geo Cluster</guimenu>
    </menuchoice>). </para>
   <example id="ex.ha.booth.conf.default">
    <title>A Booth Configuration File</title>
      <!--taroth 2014-08-21: not sure if it makes sense that all tickets
      configured here should have similar options and values or if we should
      rather show different options and values for individual tickets - dejan,
      please check! - dejan (bnc#896673): It makes sense for the network parameters 
      to be shared between tickets as the parties communicating are the same 
      (parameters 6-9)-->
<screen>transport = UDP <co id="co.ha.geo.booth.config.transport"/>
port = 9929 <co id="co.ha.geo.booth.config.port"/>
arbitrator = 147.2.207.14 <co id="co.ha.geo.booth.config.arbitrator"/>
site= 147.4.215.19 <co id="co.ha.geo.booth.config.site"/>
site= 147.18.2.1  <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket="ticketA" <co id="co.ha.geo.booth.config.ticket"/>
     expire = 600 <co id="co.ha.geo.booth.config.expiry"/>
     timeout = 10 <co id="co.ha.geo.booth.config.timeout"/>
     retries = 5 <co id="co.ha.geo.booth.config.retries"/>
     renewal-freq = 30 <co id="co.ha.geo.booth.config.renewal"/>
     before-acquire-handler<co id="co.ha.geo.booth.config.handler"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<co 
     id="co.ha.geo.booth.config.script"/>&nbsp;db-1<co  id="co.ha.geo.booth.config.rsc"/>
     acquire-after = 60 <co id="co.ha.geo.booth.config.acquire-after"/>
ticket="ticketB" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
     expire = 600 <xref linkend="co.ha.geo.booth.config.expiry" xrefstyle="select:label nopage"/>
     timeout = 10 <xref linkend="co.ha.geo.booth.config.timeout" xrefstyle="select:label nopage"/>
     retries = 5 <xref linkend="co.ha.geo.booth.config.retries" xrefstyle="select:label nopage"/>
     renewal-freq = 30 <xref linkend="co.ha.geo.booth.config.renewal" xrefstyle="select:label nopage"/>
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
     xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
     linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;db-8 <xref
     linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/>
     acquire-after = 60 <xref linkend="co.ha.geo.booth.config.acquire-after" xrefstyle="select:label nopage"/>
    </screen>
    <calloutlist>
     <callout arearefs="co.ha.geo.booth.config.transport">
      <para>&booth-transport; Only UDP is supported, but other transport layers will follow in
       the future. Currently, this parameter can therefore be omitted.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.port">
      <para> &booth-port; When not using the default port (<literal>9929</literal>), choose a port that is not 
       already used for different services. Make sure to open the port in the
       nodes&apos; and arbitrators&apos; firewalls. The booth clients use TCP to communicate with the 
       &boothd;. Booth will always bind and listen to both UDP and TCP ports.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.arbitrator">
      <para>&booth-arbitrator; Add an entry for each 
       arbitrator you use in your &geo; cluster setup.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.site">
      <para> &booth-site; Add an entry for each site you use in your
       &geo; cluster setup. Make sure to insert the correct virtual IP
       addresses (<systemitem>IPaddr2</systemitem>) for each site, otherwise the
       booth mechanism will not work correctly. Booth works with both IPv4 and
       IPv6 addresses.
       <!--taroth 2014-08-21: https://fate.suse.com/316122: booth should support IPv6 in full 
        (prio: important--></para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.ticket">
      <para>&booth-ticket; For each ticket, add a
       <literal>ticket</literal> entry.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.expiry">
      <para> Optional parameter. Defines the ticket&apos;s expiry time in seconds. A site that
       has been granted a ticket will renew the ticket regularly. If booth does not receive any
       information about renewal of the ticket within the defined expiry time, the ticket will be
       revoked and granted to another site. If no expiry time is specified, the ticket will expire
       after <literal>600</literal> seconds by default. The parameter should not be set to a value
       less than 120 seconds.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.timeout">
      <para> Optional parameter. Defines a timeout period in seconds. After that time, booth will
       resend packets if it did not receive a reply within this period. The timeout defined should
       be long enough to allow packets to reach other booth members (all arbitrators and
       sites).</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.retries">
      <para> Optional parameter. Defines how many times booth retries sending packets before giving
       up waiting for confirmation by other sites. Values smaller than <literal>3</literal> are
       invalid and will prevent booth from starting.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.renewal">
      <para> Optional parameter. Sets the ticket renewal frequency period. Ticket renewal occurs
       every half expiry time by default. If the network reliability is often reduced over prolonged
       periods, it is advisable to renew more often. Before every renewal the
        <literal>before-acquire-handler</literal> is run. </para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.handler">
      <para> Optional parameter. If set, the specified command will be called before &boothd;
       tries to acquire or renew a ticket. On exit code other than <literal>0</literal>,
       &boothd; relinquishes the ticket.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.script">
      <para>The <filename>service-runnable</filename> script referenced here is included in the
       product as an example. It is a simple script based on <command>crm_simulate</command>. It can
       be used to test if a particular cluster resource <emphasis>can</emphasis> be run on the
       current cluster site (if the cluster is healthy enough to run the resource, if all resource
       dependencies are fulfilled etc.). For instance, if a service in the dependency-chain has a
       failcount of <literal>INFINITY</literal> on all available nodes, the service cannot be run on
       that site. In that case, it is of no use to claim the ticket.</para>
      </callout>
    </calloutlist>
    <calloutlist>
     <callout arearefs="co.ha.geo.booth.config.rsc">
      <para>The resource to be tested by the <literal>before-acquire-handler</literal> (in this
       case, by the <filename>service-runnable</filename> script). You need to reference the
       resource which is protected by the respective ticket. In this example, resource
        <literal>db-1</literal> is protected by <literal>ticketA</literal> whereas
        <literal>db-8</literal> is protected by <literal>ticketB</literal>.</para>
     </callout>
     <callout arearefs="co.ha.geo.booth.config.acquire-after">
      <para>Optional parameter. After a ticket is lost, booth will wait this time in addition before
       acquiring the ticket. This is to allow for the site that lost the ticket to relinquish the
       resources, by either stopping them or fencing a node. A typical delay might be
        <literal>60</literal> seconds, but ultimately it depends on the protected resources and the
       fencing configuration. The default value is <literal>0</literal>.</para>
      <para>If you are unsure how long stopping or demoting the resources or fencing a node may take
       (depending on the <literal>loss-policy</literal>), use this parameter to prevent resources
       from running on two sites at the same time.</para>
     </callout>
    </calloutlist>
    
   </example>
   
   <procedure id="pro.ha.geo.setup.booth.config.edit">
    <title>Manually Editing The Booth Configuration File</title>
    <step>
     <para> Log in to a cluster node as &rootuser; or equivalent. </para>
    </step>
    <step>
     <para> Copy the example booth configuration file
       <filename>/etc/booth/booth.conf.example</filename> to
      &booth.conf;.</para>
    </step>
    <step>
     <para>Edit &booth.conf; according to <xref
       linkend="ex.ha.booth.conf.default"/>.</para>
    </step>
    <step>
     <para>Verify your changes and save the file. </para>
    </step>
    <step>
     <para>On all cluster nodes and arbitrators, open the port in the firewall
      that you have configured for booth. See <xref
       linkend="ex.ha.booth.conf.default"/>, position <xref
       linkend="co.ha.geo.booth.config.port"/>. </para>
    </step>
   </procedure>


   <procedure id="pro.ha.geo.setup.booth.yast">
    <title>Setting Up Booth with &yast;</title>
    <step>
     <para> Log in to a cluster node as &rootuser; or equivalent. </para>
    </step>
    <step>
     <para>Start the &yast; <guimenu>Geo Cluster</guimenu> module. </para>
    </step>
    <step>
     <para>Choose to <guimenu>Edit</guimenu> an existing booth configuration
      file or click <guimenu>Add</guimenu> to create a new booth configuration
      file:</para>
     <substeps>

      <step id="step.ha.booth.conf.params">
       <para>In the screen that appears configure the following parameters:</para>
       <itemizedlist>
        <listitem>
         <formalpara>
          <title>Configuration File</title>
          <para>A name for the booth configuration file. &yast; suggests
            <literal>booth</literal> by default. This results in the booth configuration being
           written to &booth.conf;. Only change this value if you need to set up multiple booth
           instances for different &geo; clusters as described in <xref
            linkend="sec.ha.geo.booth.multi"/>.</para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara>
          <title>Transport</title>
          <para>&booth-transport; See also <xref linkend="ex.ha.booth.conf.default"/>, position
            <xref linkend="co.ha.geo.booth.config.transport"/>.</para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara>
          <title>Port</title>
          <para>&booth-port; See also <xref linkend="ex.ha.booth.conf.default"/>, position <xref
            linkend="co.ha.geo.booth.config.port"/>. </para>
         </formalpara>
        </listitem>
        <listitem>
         <formalpara>
          <title>Arbitrator</title>
          <para>&booth-arbitrator; See also <xref linkend="ex.ha.booth.conf.default"/>, position
            <xref linkend="co.ha.geo.booth.config.arbitrator"/>.</para>
         </formalpara>
         <para>To specify an <guimenu>Arbitrator</guimenu>, click <guimenu>Add</guimenu>. In the
          dialog that opens, enter the IP address of your arbitrator and click
          <guimenu>OK</guimenu>. </para>
        </listitem>
        <listitem>
         <formalpara>
          <title>Site</title>
          <para>&booth-site; See also <xref linkend="ex.ha.booth.conf.default"/>, position <xref
            linkend="co.ha.geo.booth.config.site"/>.</para>
         </formalpara>
         <para>To specify a <guimenu>Site</guimenu> of your &geo; cluster, click
           <guimenu>Add</guimenu>. In the dialog that opens, enter the IP address of one site and
          click <guimenu>OK</guimenu>.</para>
        </listitem>
        <listitem>
         <formalpara>
          <title>Ticket</title>
          <para>&booth-ticket; See also <xref linkend="ex.ha.booth.conf.default"/>, position
            <xref linkend="co.ha.geo.booth.config.ticket"/>.</para>
         </formalpara>
         <para>To specify a <guimenu>Ticket</guimenu>, click <guimenu>Add</guimenu>. In the dialog
          that opens, enter a unique <guimenu>Ticket</guimenu> name. Additionally, you can specify
          optional parameters for your ticket. For an overview, see <xref
           linkend="ex.ha.booth.conf.default"/>, positions <xref
           linkend="co.ha.geo.booth.config.expiry"/> to <xref
            linkend="co.ha.geo.booth.config.acquire-after"/>. Click <guimenu>OK</guimenu> to confirm your
          changes.</para>
        </listitem>
       </itemizedlist>

       <figure id="fig.yast2.ha.geo.booth">
        <title>Example Ticket Dependency</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="yast2_geo_cluster_booth.png" width="80%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="yast2_geo_cluster_booth.png" width="50%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>

       <remark>taroth 2014-08-26: todo - update with more recent layout of yast, hopefully available
        in RC3 (https://bugzilla.novell.com/show_bug.cgi?id=892900) and same values as in example
        booth file</remark>

      </step>
      <step>
       <para>Click <guimenu>OK</guimenu> to close the current booth
        configuration screen. &yast; shows the name of the booth
        configuration file that you just defined. </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>Before closing the &yast; module, switch to the <guimenu>Firewall
       Configuration</guimenu> category.</para>
    </step>
    <step>
     <para>
      <remark>taroth 2014-08-26: DEVs, I guess the firewall setting is only
       applied to the current machine? so the ports on any other cluster
       nodes or arbitrators have to be opened manually, right?</remark>
      To open the port you have configured for booth, enable <guimenu>Open
      Port in Firewall</guimenu>. </para>
     <important>
      <title>Firewall Setting for Local Machine Only</title>
      <para>The firewall setting is only applied to the current machine. Make
       sure to open the respective port on all other cluster nodes and
       arbitrators of your &geo; cluster setup, too. </para></important>
    </step>
    <step>
     <para>Click <guimenu>Finish</guimenu> to confirm all settings and close the
      &yast; module. Depending on the <replaceable>NAME</replaceable> of the
       <guimenu>Configuration File </guimenu> specified in <xref
       linkend="step.ha.booth.conf.params"/>, the configuration is written to
        <filename>/etc/booth/<replaceable>NAME</replaceable>.conf</filename>.</para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.geo.booth.multi">
   <title>Booth Setup for Multiple Tenants</title>
   <!--taroth 2014-08-07:  https://fate.suse.com/316123:
    Multi-tenancy for booth (prio: important)-->
   <para>&booth-multi-tenancy;</para>
   <para>
    Let us assume you have two
    &geo; clusters, one in EMEA (Europe, the Middle East and Africa), and
    one in the Asia-Pacific region (APAC). </para>
   <para>To use the same arbitrator for both &geo; clusters, create two
    configuration files in the <filename>/etc/booth</filename> directory:
     <filename>/etc/booth/emea.conf</filename> and
     <filename>/etc/booth/apac.conf</filename>. Both must minimally differ in
    the following parameters:</para>
   <itemizedlist>
    <listitem>
     <para>The port used for the communication of the booth instances.</para>
    </listitem>
    <listitem>
     <para>The sites belonging to the different &geo; clusters that the
      arbitrator is used for.</para>
    </listitem>
   </itemizedlist>

   <example id="ex.ha.conf.booth.multi-1">
    <title>
     <filename>/etc/booth/apac.conf</filename>
    </title>
    <para><remark>taroth 2014-08-26: DEVs, can someone provide better IP
     addresses for the cluster sites below? I just noticed that the entities we have
    defined for our usual network examples in the docs are too similar for different cluster
    sites...</remark></para>
    <screen><?dbsuse-fo font-size="0.75em"?>port = 9133 <xref linkend="co.ha.geo.booth.config.port" xrefstyle="select:label nopage"/>
arbitrator = 147.2.207.14 <xref linkend="co.ha.geo.booth.config.arbitrator" xrefstyle="select:label nopage"/>
site= &slpip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
site= &proxyip; <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket="tkt-db-apac-intern" <xref linkend="co.ha.geo.booth.config.ticket"/>
     timeout = 10 
     retries = 5 
     renewal-freq = 60 
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
      xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
       linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;db-apac-intern <xref
        linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/> 
ticket="tkt-db-apac-cust" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
     timeout = 10 
     retries = 5 
     renewal-freq = 60 
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
      xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
       linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;db-apac-cust <xref
        linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/></screen>
   </example>

   <example id="ex.ha.conf.booth.multi-2">
    <title>
     <filename>/etc/booth/emea.conf</filename>
    </title>
    <para><remark>taroth 2014-08-26: DEVs, can someone provide better IP
     addresses for the cluster sites below? I just noticed that the entities we have
     defined for our usual network examples in the docs are too similar for different cluster
     sites...</remark></para>
    <screen><?dbsuse-fo font-size="0.75em"?>port = 9150 <xref linkend="co.ha.geo.booth.config.port" xrefstyle="select:label nopage"/>
arbitrator = 147.2.207.14 <xref linkend="co.ha.geo.booth.config.arbitrator" xrefstyle="select:label nopage"/>
site= 192.168.4.113 <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
site=  192.168.6.113<xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket="tkt-sap-crm" <xref linkend="co.ha.geo.booth.config.ticket"/>
     expire = 900 
     renewal-freq = 60 
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
      xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
       linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;sap-crm <xref
        linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/>
ticket="tkt-sap-prod" <xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>
     expire = 600 
     renewal-freq = 60 
     before-acquire-handler<xref linkend="co.ha.geo.booth.config.handler" 
      xrefstyle="select:label nopage"/>&nbsp;=&nbsp;/usr/share/booth/service-runnable<xref
       linkend="co.ha.geo.booth.config.script" xrefstyle="select:label nopage"/>&nbsp;sap-prod <xref
        linkend="co.ha.geo.booth.config.rsc" xrefstyle="select:label nopage"/></screen>
   </example>

   <calloutlist>
    <callout arearefs="co.ha.geo.booth.config.port">
     <para>&booth-port; The configuration files use different ports to allow
      for start of multiple booth instances on the same arbitrator.</para>
    </callout>
    <callout arearefs="co.ha.geo.booth.config.arbitrator">
     <para>&booth-arbitrator; In the examples above, we use the same
      arbitrator for different &geo; clusters.</para>
    </callout>
    <callout arearefs="co.ha.geo.booth.config.site">
     <para>&booth-site; The sites defined in both booth configuration files
      are different, because they belong to two different &geo; clusters.
     </para>
    </callout>
    <callout arearefs="co.ha.geo.booth.config.ticket">
     <para>&booth-ticket; Theoretically the same ticket names can be defined
      in different booth configuration files&mdash; the tickets will not
      interfere because they are part of different &geo; clusters that are
      managed by different booth instances. However, (for better overview), we
      advise to use distinct ticket names for each &geo; cluster as shown in
      the examples above.</para>
    </callout>
   </calloutlist>

   <procedure>
    <title>Using the Same Arbitrator for Different &geo; Clusters</title>
    <step>
     <para>Create different booth configuration files in
       <filename>/etc/booth</filename> as shown in <xref
       linkend="ex.ha.conf.booth.multi-1"/> and <xref
       linkend="ex.ha.conf.booth.multi-2"/>. Do so either manually or with
      &yast;, as outlined in <xref linkend="pro.ha.geo.setup.booth.yast"/>.
     </para>
    </step>
    <step>
     <para>On the arbitrator, open the ports that are defined in any of the
      booth configuration files in <filename>/etc/booth</filename>.</para>
    </step>
    <step>
     <para>On the nodes belonging to the individual &geo; clusters that the
      arbitrator is used for, open the port that is used for the respective
      booth instance.</para>
    </step>
    <step>
     <para>Synchronize the respective booth configuration files across all
      cluster nodes and arbitrators that use the same booth configuration. For
      details, see <xref linkend="sec.ha.geo.booth.sync"/>.</para>
    </step>
    <step>
     <para>On the arbitrator, start the individual booth instances as described
      in <xref linkend="vle.ha.geo.setup.booth.service.arbitrator"/> for
      multi-tenancy setups.</para>
    </step>
    <step>
     <para>On the individual &geo; clusters, start the booth service as
      described in <xref linkend="vle.ha.geo.setup.booth.service.sites"
      />.</para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.geo.booth.sync">
   <title>Synchronizing the Booth Configuration Across All Sites and
    Arbitrators</title>
   <para>To make booth work correctly, all cluster nodes and arbitrators within
    one &geo; cluster must use the same booth configuration. In case of any
    booth configuration changes, make sure to update the configuration files accordingly on
    all parties and to restart the booth services as described in <xref
     linkend="sec.ha.geo.setup.booth.reconfig"/>. </para>

   <note>
    <title>Synchronize Booth Configuration to All Sites and Arbitrators</title>
    <para>
     <remark>taroth 2014-08-11: todo - https://fate.suse.com/316223: [docu] sync
      and change config files (prio: mandatory)</remark> All cluster nodes and
     arbitrators within the &geo; cluster must use the same booth
     configuration. While you may need to copy the files manually to the
     arbitrators and to one cluster node per site, you can use &csync;
     within each cluster site to synchronize the file to all nodes. </para>
   </note>

  </sect2>

  <sect2 id="sec.ha.geo.setup.booth.service">
   <title>Enabling and Starting the Booth Services</title>
  
   <variablelist>
    <varlistentry id="vle.ha.geo.setup.booth.service.sites">
     <term>Starting the Booth Services on Cluster Sites</term>
     <listitem>
      <para>The booth service for each cluster site is managed by the booth
       resource group configured in <xref linkend="pro.ha.geo.setup.rsc.boothd"
       />.
       <!--taroth 2014-08-25: FIXME: check and add link to Hawk config, too -->
       To start one instance of the booth service per site, start the respective
       booth resource group on each cluster site.</para>
     </listitem>
    </varlistentry>
    <varlistentry id="vle.ha.geo.setup.booth.service.arbitrator">
     <term>Starting the Booth Services on Arbitrators</term>
     <!--taroth 2014-08-11: https://bugzilla.novell.com/show_bug.cgi?id=877817: [Test
      Case 1378526] [316123] booth - Configuring startup 'complicated'-->
     <listitem>
      <para>Starting with &sle; 12, booth arbitrators are managed with systemd. The unit file is
       named <filename>booth@.service</filename>. The <literal>@</literal> denotes the possibility
       to run the service with a parameter, which is in this case the name of the configuration
       file.</para>
      <para>To <emphasis>enable</emphasis> the booth service on an arbitrator, use the following
       command:</para>
      <screen>systemctl enable booth@booth</screen>
      <para>After the service has been enabled from command line, &yast; &ycc_runlevel; can
       then be used to manage the service, as long as it is not disabled. In that case, it will
       disappear from the service list in &yast; next time systemd is restarted.</para>
      <para>However, the command to <emphasis>start</emphasis> the booth service depends on your
       booth setup: </para>
      <itemizedlist>
       <listitem>
        <para>If you are using the default setup as described in <xref
          linkend="sec.ha.geo.booth.default"/>, only <filename>/etc/booth/booth.conf</filename> is
         configured. In that case, log in to each arbitrator and use the following command: </para>
        <screen>&prompt.root;<command>systemctl</command> start booth@booth</screen>
       </listitem>
       <listitem>
        <para>If you are running booth in multi-tenancy mode as described in <xref
          linkend="sec.ha.geo.booth.multi"/>, you have configured multiple booth configuration files
         in <filename>/etc/booth</filename>. To start the services for the individual booth
         instances, use
         <command>systemctl&nbsp;start&nbsp;booth@<replaceable>NAME</replaceable></command>,
         where <replaceable>NAME</replaceable> stands for the name of the respective configuration
         file <filename>/etc/booth/<replaceable>NAME</replaceable>.conf</filename>.</para>
        <para>For example, if you have the booth configuration files
          <filename>/etc/booth/emea.conf</filename> and <filename>/etc/booth/apac.conf</filename>,
         log in to your arbitrator and execute the following commands:</para>
        <screen>&prompt.root;<command>systemctl</command> start booth@emea
&prompt.root;<command>systemctl</command> start booth@apac</screen>
        <para>
         <!-- taroth 2014-08-25: is it also possible to start the two
          services in one go when using the "@" syntax? if yes, how is the exact
          command? - dejan 2014-09-15: No, don't think so.-->
        </para>
       </listitem>
      </itemizedlist>
      <para> This starts the booth service in arbitrator mode. It can communicate with all other
       booth daemons but in contrast to the booth daemons running on the cluster sites, it cannot be
       granted a ticket. Booth arbitrators take part in elections only. Otherwise, they are
       dormant.</para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 
  <sect2 id="sec.ha.geo.setup.booth.reconfig">
   <title>Reconfiguring Booth While Running</title>
   <!--taroth 2014-08-26: https://fate.suse.com/316126: 
    reconfiguration of boothd while running (prio: important)-->
   <para>In case you need to change the booth configuration while the booth
    services are already running, proceed as follows:
  </para>
   <procedure>
    <step>
     <para>Adjust the booth configuration files as desired.</para>
    </step>
    <step>
     <para>Synchronize the updated booth configuration files to all cluster
      nodes and arbitrators that are part of your &geo; cluster. For
      details, see <xref linkend="sec.ha.geo.booth.sync"/>.</para>
    </step>
    <step>
     <!--taroth 2014-08-26: https://bugzilla.novell.com/show_bug.cgi?id=891399-->
     <para>Restart the booth services on the arbitrators and cluster sites as
      described in <xref linkend="sec.ha.geo.setup.booth.service"/>. This does
      not have any effect on tickets that have already been granted to
      sites.</para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 
 <sect1 id="sec.ha.geo.rsc">
   <title>Configuring Cluster Resources and Constraints</title>
  <para> Apart from the resources and constraints that you need to define for
   your specific cluster setup, &geo; clusters require additional resources
   and constraints as described below. You can either configure them with the
   &crmshell; (&crmsh;), or with the &haweb; (&hawk;).</para>
     
   <sect2 id="sec.ha.geo.rsc.cli">
      <title>From Command Line</title>
    
   <para> This section focuses on tasks specific to &geo; clusters. For a an
    introduction to the &crmshell; and general instructions on how to
    configure resources and constraints with &crmsh;, refer to
    the <citetitle>&haguide;</citetitle> for &productname;, chapter
     <citetitle>Configuring and Managing Cluster Resources (Command Line)</citetitle>. 
   </para>
   
   <procedure id="pro.ha.geo.setup.rsc.constraints">
    <title>Configuring Ticket Dependencies</title>
     &ticket-dependency-loss-policy;
     <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
      <!--<remark>toms 2011-10-04: Shorten it to "Start a root shell on one
       of the cluster nodes"? I guess we deal with admin-minded people here</remark>
       taroth 2011-10-06: due to new ACL support of the crm shell, also
       non-root users may have the rights to do so, therefore phrased it like
       that -->
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      &crmshell;.
     </para>
    </step>
    <step id="step.ha.geo.setup.rsc.constraints">
     <para>
      Configure a constraint that defines which resources depend on a
      certain ticket. For example:
     </para>
     <screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-ticketA ticketA: rsc1 \
  loss-policy="fence"</screen>
     <para>
      This creates a constraint with the ID
      <literal>rsc1-req-ticketA</literal>. It defines that the resource
      <literal>rsc1</literal> depends on <literal>ticketA</literal> and that
      the node running the resource should be fenced in case
      <literal>ticketA</literal> is revoked.
     </para>
     <para> Alternatively, you can configure resource <literal>rsc1</literal>
      not as a primitive, but a multi-state resource that can run in
       <literal>master</literal> or <literal>slave</literal> mode. In that case,
      make only <literal>rsc1</literal>'s master mode depend on
       <literal>ticketA</literal>. With the following configuration,
       <literal>rsc1</literal> is automatically demoted to
       <literal>slave</literal> mode if <literal>ticketA</literal> is revoked:
     </para>
     <screen>&prompt.crm.conf;<command>rsc_ticket</command> rsc1-req-ticketA ticketA: rsc1:Master \
  loss-policy="demote"</screen>
    </step>
    <step>
     <para>
      If you want other resources to depend on further tickets, create as
      many constraints as necessary with <command>rsc_ticket</command>.
     </para>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
     <para>
      The constraints are saved to the CIB.
     </para>
     <!--For example, the command in
      <xref linkend="step.ha.geo.setup.rsc.constraints" xreflabel="nopage"/> would result in
      the following constraint configuration in the CIB:
      <screen>&lt;rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" role="Master" ticket="ticketA" loss-policy="fence"/></screen>-->
    </step>
   </procedure>
   <procedure id="pro.ha.geo.setup.rsc.boothd">
    <title>Configuring a Resource Group for <systemitem class="daemon"
     >boothd</systemitem></title>
     &boothd-resource-group;
    <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      &crmshell;.
     </para>
    </step>
    <step>
     <para>
      Enter the following to create both primitive resources and to add them to one group,
      <literal>g-booth</literal>:
     </para>
     <screen>&prompt.crm.conf;<command>primitive</command> booth-ip ocf:heartbeat:IPaddr2 \
  params ip="<replaceable>IP_ADDRESS</replaceable>"
&prompt.crm.conf;<command>primitive</command> booth ocf:pacemaker:booth-site \
  meta resource-stickiness="INFINITY" \
  op monitor interval="10s"
  group g-booth booth-ip booth</screen>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
    </step>
    <step>
     <para>
      Repeat the resource group configuration on the other cluster sites,
      using a different IP address for each <literal>boothd</literal>
      resource group.
     </para>
     <para>
      With this configuration, each booth daemon will be available at its
      individual IP address, independent of the node the daemon is running
      on.
     </para>
    </step>
   </procedure>
   <!--taroth 2012-02-14: fix for bnc#746863-->
   <procedure id="pro.ha.geo.setup.rsc.order">
    <title>Adding an Ordering Constraint</title>
    &booth-order-constraint;
    <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      &crmshell;.
     </para>
    </step>
    <step>
     <para>
      Create an ordering constraint:
     </para>
     <screen>&prompt.crm.conf;<command>order</command> order-booth-rsc1 inf: g-booth rsc1</screen>
     <para>
      This defines that <literal>rsc1</literal> (that depends on
      <literal>ticketA</literal>) can only be started after the
      <literal>g-booth</literal> resource group.
     </para>
     <para>
      In case <literal>rsc1</literal> is not a primitive, but a special
      clone resource and configured as described in
      <xref
       linkend="step.ha.geo.setup.rsc.constraints"/> of 
      <xref linkend="pro.ha.geo.setup.rsc.constraints"/>, the
      ordering constraint should be configured as follows:
     </para>
     <screen>&prompt.crm.conf;<command>order</command> order-booth-rsc1 inf: g-booth rsc1:promote</screen>
     <para>
      This defines that <literal>rsc1</literal> can only be promoted to
      master mode after the <literal>g-booth</literal> resource group has
      started.
     </para>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      For any other resources that depend on a certain ticket, define
      further ordering constraints.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
    </step>
   </procedure>
   </sect2>
  
  <sect2 id="sec.ha.geo.rsc.hawk">
   <title>With the &haweb; (&hawk;)</title>
   <para> This section focuses on tasks specific to &geo; clusters. For an
    introduction to &hawk; and general instructions on how to configure
    resources and constraints with &hawk;, refer to the
     <citetitle>&haguide;</citetitle> for &productname;, chapter
     <citetitle>Configuring and Managing Cluster Resources (Web
     Interface)</citetitle>. </para>

   <procedure id="pro.ha.config.hawk.geo.rsc.constraints">
    <title>Configuring Ticket Dependencies</title>
    &ticket-dependency-loss-policy; <para>The following example shows two
     alternatives to configure the constraint: One with the resource being a
     primitive and <literal>loss-policy="fence"</literal>, the other one with
     the resource being a multi-state resource that can run in
      <literal>master</literal> or <literal>slave</literal> mode and with
      <literal>loss-policy="demote"</literal>.</para>
    <step>
     <para> Start a Web browser and log in to &hawk;. </para>
    </step>
    <step>
     <para> In the left navigation bar, select <guimenu>Constraints</guimenu>.
      The <guimenu>Constraints</guimenu> screen shows categories for all types
      of constraints and lists all defined constraints. </para>
    </step>
    <step>
     <para> To add a new ticket dependency, click the plus icon in the
       <guimenu>Ticket</guimenu> category. </para>
     <para> To modify an existing constraint, click the wrench icon next to the
      constraint and select <guimenu>Edit Constraint</guimenu>. </para>
    </step>
    <step>
     <para> Enter a unique <guimenu>Constraint ID</guimenu>. When modifying
      existing constraints, the ID is already defined. </para>
    </step>
    <step>
     <para> Set a <guimenu>Loss Policy</guimenu>. </para>
    </step>
    <step>
     <para> Enter the ID of the ticket that the resources should depend on.
     </para>
    </step>
    <step>
     <para> Select a resource from the list <guimenu>Add resource to
       constraint</guimenu>. The list shows the IDs of all resources and all
      resource templates configured for the cluster. </para>
    </step>
    <step id="step.geo.ticket.dependency.clone">
     <para> To add the selected resource, click the plus icon next to the list.
      A new list appears beneath, showing the remaining resources. Add as many
      resources to the constraint as you would like to depend on the ticket. </para>
     <figure id="fig.hawk.ticket.dep.simple">
      <title>&hawk;&mdash;Ticket Dependency with
        <literal>loss-policy="fence"</literal></title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="hawk-geo-ticket-dependency1.png" width="50%"
         format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="hawk-geo-ticket-dependency1.png" width="50%"
         format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      <xref linkend="fig.hawk.ticket.dep.simple"/> shows a constraint with the
      ID <literal>rsc1-req-ticketA</literal>. It defines that the resource
       <literal>rsc1</literal> depends on <literal>ticketA</literal> and that
      the node running the resource should be fenced in case
       <literal>ticketA</literal> is revoked. </para>
     <para>If resource <literal>rsc1</literal> was not a primitive, but a
      multi-state resource, define that only <literal>rsc1</literal>'s master
      mode depends on <literal>ticketA</literal>. With the configuration shown
      in <xref linkend="fig.hawk.ticket.dep.adv"/>, <literal>rsc1</literal> is
      automatically demoted to <literal>slave</literal> mode if
       <literal>ticketA</literal> is revoked: </para>
     <figure id="fig.hawk.ticket.dep.adv">
      <title>&hawk;&mdash;Ticket Dependency with
        <literal>loss-policy="demote"</literal></title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="hawk-geo-ticket-dependency2.png" width="60%"
         format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="hawk-geo-ticket-dependency2.png" width="50%"
         format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para> Click <guimenu>Create Constraint</guimenu> to finish the
      configuration. A message at the top of the screen shows if the constraint
      was successfully created. </para>
    </step>
   </procedure>

   <procedure id="pro.ha.geo.rsc.hawk.group">
    <title>Configuring a Resource Group for <systemitem class="daemon"
      >boothd</systemitem></title> &boothd-resource-group; <step>
     <para> Start a Web browser and log in to &hawk;. </para>
    </step>
    <step>
     <para> In the left navigation bar, select <guimenu>Resources</guimenu>. The
       <guimenu>Resources</guimenu> screen shows categories for all types of
      resources. It lists any resources that are already defined. </para>
    </step>
    <step>
     <para> Select the <guimenu>Primitive</guimenu> category and click the plus
      icon. </para>
    </step>
    <step>
     <para> To specify the resource for boothd: </para>
     <substeps>
      <step>
       <para> Enter a unique <guimenu>Resource ID</guimenu>, for example:
         <literal>booth-ip</literal>. </para>
      </step>
      <step>
       <para> Set <guimenu>Class</guimenu> to <literal>ocf</literal>,
         <guimenu>Provider</guimenu> to <literal>heartbeat</literal> and
         <guimenu>Type</guimenu> to <literal>IPaddr2</literal>. </para>
       <para> &hawk; automatically shows any required parameters for the
        resource plus an empty drop-down list that you can use to specify
        additional parameters. </para>
      </step>
      <step>
       <para> Define the following <guimenu>Parameters</guimenu> (instance
        attributes) for the resource and enter values for them: </para>
       <itemizedlist>
        <listitem>
         <para>ip</para>
        </listitem>
        <listitem>
         <para>cidr_netmask</para>
        </listitem>
       </itemizedlist>
      </step>
      <step>
       <para> Click <guimenu>Create Resource</guimenu> to finish the
        configuration. A message at the top of the screen shows if the resource
        was successfully created or not. </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>Click <guimenu>Back</guimenu> to return to the list of configured
      resources.</para>
    </step>
    <step>
     <para> Select the <guimenu>Primitive</guimenu> category and click the plus
      icon. </para>
    </step>
    <step>
     <para> To specify the resource for boothd: </para>
     <substeps>
      <step>
       <para> Enter a unique <guimenu>Resource ID</guimenu>, for example:
         <literal>booth</literal>. </para>
      </step>
      <step>
       <para> Set <guimenu>Class</guimenu> to <literal>ocf</literal>,
         <guimenu>Provider</guimenu> to <literal>pacemaker</literal> and
         <guimenu>Type</guimenu> to <literal>booth-site</literal>. </para>
       <para> &hawk; automatically shows any required parameters for the
        resource plus an empty drop-down list that you can use to specify
        additional parameters. </para>
      </step>
      <step>
       <para>In the <guimenu>Operations</guimenu> category, select
         <literal>monitor</literal>. &hawk; proposes a timeout value of 20
        and an interval of 10 seconds. Keep the proposed values and add this
        monitoring operation by clicking the plus icon next to it. </para>
      </step>
      <step>
       <para>In the <guimenu>Meta-Attributes</guimenu> category, select
         <literal>resource-stickiness</literal> and enter
         <literal>INFINITY</literal> as value. Click the plus icons next to the
        value to add this meta attribute.</para>
      </step>
      <step>
       <para> Click <guimenu>Create Resource</guimenu> to finish the
        configuration. A message at the top of the screen shows if the resource
        was successfully created or not. </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>Click <guimenu>Back</guimenu> to return to the list of configured
      resources.</para>
    </step>
    <step>
     <para>To create the group and add booth primitives to it:</para>
     <substeps>
      <step>
       <para> Select the <guimenu>Group</guimenu> category and click the plus
        icon. </para>
      </step>
      <step>
       <para> Enter a unique <guimenu>Group ID</guimenu>, for example:
         <literal>g-booth</literal>. </para>
      </step>
      <step>
       <para>To define the group members, select <literal>booth-ip</literal> and
         <literal>booth</literal> in the list of <guimenu>Available
         Primitives</guimenu> and click the &lt; icon to add them to the
         <guimenu>Group Children</guimenu> list. To define the order of the
        group members, you currently need to add and remove them in the order
        you desire. </para>
      </step>
      <step>
       <para> &hawk; automatically proposes the meta attribute
         <literal>target-role</literal>. Set its value to
         <literal>Started</literal>. </para>
       <figure id="fig.hawk.geo.booth.group">
        <title>&hawk;&mdash;Resource Group for boothd</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="hawk-geo-booth-group.png" width="60%" format="PNG"
          />
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="hawk-geo-booth-group.png" width="50%" format="PNG"
          />
         </imageobject>
        </mediaobject>
       </figure>
      </step>
      <step>
       <para> Click <guimenu>Create Group</guimenu> to finish the configuration.
        A message at the top of the screen shows if the group was successfully
        created. </para>
      </step>
     </substeps>
    </step>
    <step>
     <para> Repeat the resource group configuration on the other cluster sites,
      using a different IP address for each <literal>boothd</literal> resource
      group. </para>
     <para> With this configuration, each booth daemon will be available at its
      individual IP address, independent of the node the daemon is running on.
     </para>
    </step>
   </procedure>


   <procedure id="pro.ha.geo.rsc.hawk.order">
    <title>Adding an Ordering Constraint</title> &booth-order-constraint; <step>
     <para> Start a Web browser and log in to &hawk;. </para>
    </step>
    <step>
     <para> In the left navigation bar, select <guimenu>Constraints</guimenu>.
      The <guimenu>Constraints</guimenu> screen shows categories for all types
      of constraints and lists all defined constraints. </para>
    </step>
    <step>
     <para> Select the <guimenu>Order</guimenu> category and click the plus icon
      to create a new ordering constraint. </para>
    </step>
    <step>
     <para> Enter a unique <guimenu>Constraint ID</guimenu>, for example
       <literal>order-booth-rsc1</literal>. </para>
    </step>
    <step>
     <para> Set the <guimenu>Score</guimenu> to <literal>INFINITY</literal>. </para>
     <para> For colocation constraints, the score determines the location
      relationship between the resources. Setting the score to
       <literal>INFINITY</literal> forces the resources to run on the same node.
      For order constraints, the constraint is mandatory if the score is greater
      than zero, otherwise it is only a suggestion. The default value is
       <literal>INFINITY</literal>. </para>
    </step>
    <step>
     <para> Keep the option <guimenu>Symmetrical</guimenu> enabled. This
      specifies that resources are stopped in reverse order. </para>
    </step>
    <step>
     <para> To define the resources for the constraint: </para>
     <substeps>
      <step>
       <para> Select the resource group <literal>g-booth</literal> from the list
         <guimenu>Add resource to constraint</guimenu> and click the plus icon
        next to the list to add the resource to the ordering constraint. </para>

       <figure id="fig.hawk.geo.booth.order">
        <title>&hawk;&mdash;Ordering Constraint with Multi-state
         Resource</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="hawk-geo-order-constraint-clone.png" width="60%"
           format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="hawk-geo-order-constraint-clone.png" width="50%"
           format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
      <step>
       <para> Select the resource <literal>rsc1</literal> from the list
         <guimenu>Add resource to constraint</guimenu> and click the plus icon
        next to the list to add the resource to the ordering constraint. </para>
       <para> Now you have both resources in a dependency chain. The topmost
         (<literal>g-booth</literal>) will start first, then the next one
         (<literal>rsc1</literal>). Usually the resources will be stopped in
        reverse order. </para>
      </step>
      <step>
       <para> In case <literal>rsc1</literal> is not a primitive, but a
        multi-state resource and configured as described in <xref
         linkend="step.geo.ticket.dependency.clone"/> of <xref
         linkend="pro.ha.config.hawk.geo.rsc.constraints"/>, select the
        following entry from the empty drop-down box next to
         <literal>rsc1</literal>: <literal>promote</literal>. This defines that
         <literal>rsc1</literal> can only be promoted to master mode after the
         <literal>g-booth</literal> resource group has started. </para>
      </step>
      <step>
       <para>Click <guimenu>Create Constraint</guimenu>.</para>
       <para> A message at the top of the screen shows if the constraint was
        successfully created. </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>Click <guimenu>Back</guimenu> to return to the list of
      constraints.</para>
    </step>
    <step>
     <para> For any other resources that depend on a certain ticket, define
      further ordering constraints. </para>
    </step>
   </procedure>
  </sect2>
  </sect1>
 
  <sect1 id="sec.ha.geo.manage">
   <title>Managing &geo; Clusters</title>
   <para> Before booth can manage a certain ticket within the &geo;
    cluster, you initially need to grant it to a site manually.
   
   </para>
   <sect2 id="sec.ha.geo.manage.cli">
 <title>From Command Line</title>
   
   <para>Use the <command>booth&nbsp;client</command> command line tool to grant, list, or
    revoke tickets as described in <xref linkend="vl.ha.booth.client.cmds"/>. The
    <command>booth&nbsp;client</command> commands can be run on any machine in the cluster, not
    only the ones having the &boothd; running. The <command>booth&nbsp;client</command>
    commands try to find the <quote>local</quote> cluster by looking at the booth configuration file
    and the locally defined IP addresses. If you do not specify a site which the booth client should
    connect to (using the <option>-s</option> option), it will always connect to the local site. </para>
    <note>
    <title>Syntax Changes</title>
    <para>The syntax of booth clients commands has been simplified since &productname; 11: For
     example, the <literal>client</literal> keyword can be omitted for <option>list</option>,
      <option>grant</option>, or <option>revoke</option> operations: <command>booth list</command>.
     Also, the <option>-t</option> option can be omitted when specifying a ticket. </para>
    <para>The former syntax is still supported. For detailed information, see the
      <literal>Synopsis</literal> section in the booth man page. However, the examples in this
     manual use the simplified syntax. </para>
    </note>

   <!--taroth 2013-04-24: information taken from bnc#752601, c#17-->
    <variablelist id="vl.ha.booth.client.cmds">
    <title>Overview of <command>booth client</command> Commands</title>
    <varlistentry>
     <term>Listing All Tickets</term>
     <listitem>
      <screen>&prompt.root; <command>booth</command> list
ticket: ticketA, leader: none
ticket: ticketB, leader: 10.2.12.101, expires: 2014-08-13 10:28:57
      </screen>
      <para>If you do not specify a certain site with <option>-s</option>, the information about the
       tickets will be requested from the local booth instance.</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Granting a Ticket to a Site</term>
     <listitem>
      <screen>&prompt.root; <command>booth</command> grant -s 147.2.207.14 ticketA
booth[27891]: 2014/08/13_10:21:23 info: grant request sent, waiting for the result ...
booth[27891]: 2014/08/13_10:21:23 info: grant succeeded!</screen>
      <para> In this case, <literal>ticketA</literal> will be granted to the site
        <literal>147.2.207.14</literal>. If you omit the <option>-s</option> option, booth will
       automatically connect to the current site (the site you are running the booth client on) and
       will request the <command>grant</command> operation. </para>
      <para> Before granting a ticket, the command will execute a sanity check. If the same ticket
       is already granted to another site, you will be warned about that and be prompted to revoke
       the ticket from the current site first. </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Revoking a Ticket From a Site</term>
     <listitem>
      <screen>&prompt.root; <command>booth</command> revoke ticketA
booth[27900]: 2014/08/13_10:21:23 info: revoke succeeded!</screen>
      <para>Booth will check to which site the ticket is currently granted and will request the
       <command>revoke</command> operation for <literal>ticketA</literal>. The revoke operation will
       be executed immediately.</para>
     </listitem>
    </varlistentry>
   </variablelist>

   <para>The <command>grant</command> and, under certain circumstances, <command>revoke</command>
    operations may take a while to return a definite operation's outcome. The client will wait for
    the result up to the ticket's <varname>timeout</varname> value before it gives up
    waiting&mdash;unless the <option>-w</option> option was used, in which case the client waits
    indefinitely. Find the exact status in the log files or with the
    <command>crm_ticket -L</command> command.</para>


   <warning>
    <title><command>crm_ticket</command> and
     <command>crm&nbsp;site&nbsp;ticket</command></title>
    <para> In case the booth service is not running for any reasons, you may
     also manage tickets manually with <command>crm_ticket</command> or
     <command>crm&nbsp;site&nbsp;ticket</command>. Both commands are
     only available on cluster nodes. In case of intervention, use them
     with great care as they <emphasis>cannot</emphasis> verify if the same
     ticket is already granted elsewhere. For more information, read the man pages. 
    </para>
    <para> As long as booth is up and running, only use
     <command>booth&nbsp;client</command> for manual intervention. </para>
   </warning>

   <para> After you have initially granted a ticket to a site, the booth
    mechanism will take over and manage the ticket automatically. If the site
    holding a ticket should be out of service, the ticket will automatically be
    revoked after the expiry time and granted to another site. The resources
    that depend on that ticket will fail over to the new site holding the
    ticket. The nodes that have run the resources before will be treated
    according to the <literal>loss-policy</literal> you set within the
    constraint. </para>
  
   <!--taroth 2013-04-24: fix for bnc#752601, c#10-->

   <procedure id="pro.ha.geo.manage.tickets">
    <title>Managing Tickets Manually</title>
    <para> Assuming that you want to manually move <literal>ticketA</literal>
     from site <literal>147.2.207.14</literal> to
     <literal>&smbip;</literal>, proceed as follows: </para>
    <step>
     <para> Set <literal>ticketA</literal> to standby with the following
      command: </para>
     <screen>&prompt.root;<command>crm_ticket</command> -t ticketA -s</screen>
    </step>
    <step>
     <para> Wait for any resources that depend on <literal>ticketA</literal> to
      be stopped or demoted cleanly. </para>
    </step>
    <step>
     <para> Revoke <literal>ticketA</literal> from its current site with: </para>
     <screen>&prompt.root;<command>booth</command> revoke -s 147.2.207.14 ticketA</screen>
    </step>
    
    <step>
     <para> After the ticket has been revoked from its original site, grant it
      to the new site with: </para>
     <screen>booth grant -s &smbip; ticketA</screen>
    </step>
   </procedure>
    <remark>phil 2013-12: Will/Should get a cleaner, more integrated workflow - "booth client move -t ticket -s new-site"
     or something similar. - dmuhamedagic 2014-08-13: N/A</remark>
    </sect2>
  
  <sect2 id="sec.ha.geo.manage.hawk">
   <title>With the &haweb; (&hawk;)</title>
   <para> You can use &hawk; as a single point of administration for
    monitoring multiple clusters. &hawk;'s <guimenu>Cluster
     Dashboard</guimenu> allows you to view a summary of multiple clusters, with
    each summary listing the number of nodes, resources, tickets, and their
    state. The summary also shows if any failures have appeared in the
    respective cluster. </para>

   <para>To manage cluster site tickets and to test the impact of ticket
    failover with the <guimenu>Simulator</guimenu>, you can easily switch from
    the <guimenu>Cluster Dashboard</guimenu> to the other &hawk; functions
    that are available after logging in to an individual cluster. &hawk;
    allows you to grant or revoke tickets, to view ticket details, and to test
    the impact of ticket failover with the <guimenu>Simulator</guimenu>. </para>



   <sect3 id="sec.ha.geo.manage.hawk.dashboard">
<!--taroth 2014-08-27: CAVE - the following is copied from ha_config_hawk.xml,
 find a better solution next time-->
    <title>Monitoring Multiple Clusters with the Cluster Dashboard</title>

    <para> You can use &hawk; as a single point of administration for
     monitoring multiple clusters. &hawk;'s <guimenu>Cluster
      Dashboard</guimenu> allows you to view a summary of multiple clusters,
     with each summary listing the number of nodes, resources, tickets (if you
     use &geo; clusters), and their state. The summary also shows if any
     failures have appeared in the respective cluster. </para>

    <para> The cluster information displayed in the <guimenu>Cluster
      Dashboard</guimenu> is stored in a persistent cookie. This means you need
     to decide which &hawk; instance you want to view the <guimenu>Cluster
      Dashboard</guimenu> on, and always use that one. The machine you are
     running &hawk; on does not even have to be part of any cluster for that
     purpose&mdash;it can be a separate, unrelated system. </para>

    <procedure id="pro.ha.config.hawk.geo.dashboard">
     <title>Monitoring Multiple Clusters with &hawk;</title>
     <itemizedlist>
      <title>Prerequisites</title>
      <listitem>
       <para> All clusters to be monitored from &hawk;'s <guimenu>Cluster
         Dashboard</guimenu> must be running &productname;
        &productnumber;. It is not possible to monitor clusters that are
        running earlier versions of &productname;. </para>
      </listitem>
      <listitem>
       <para> If you did not replace the self-signed certificate for &hawk;
        on every cluster node with your own certificate (or a certificate signed
        by an official Certificate Authority), you must log in to &hawk; on
         <emphasis>every</emphasis> node in <emphasis>every</emphasis> cluster
        at least once. Verify the certificate (and add an exception in the
        browser to bypass the warning). </para>
      </listitem>
      <listitem>
       <para> If you are using Mozilla Firefox, you must change its preferences
        to <guimenu>Accept third-party cookies</guimenu>. Otherwise cookies from
        monitored clusters will not be set, thus preventing login to the
        clusters you are trying to monitor. </para>
      </listitem>
     </itemizedlist>
     <step>
      <para> Start the &hawk; Web service on a machine you want to use for
       monitoring multiple clusters. </para>
     </step>
     <step>
      <para> Start a Web browser and as URL enter the IP address or hostname of
       the machine that runs &hawk;: </para>
      <screen>https://<replaceable>IPaddress</replaceable>:7630/</screen>
     </step>
     <step>
      <para> On the &hawk; login screen, click the
        <guimenu>Dashboard</guimenu> link in the right upper corner. </para>
      <para> The <guimenu>Add Cluster</guimenu> dialog appears. </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk-dashboard-add-cluster.png" width="100%"
          format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk-dashboard-add-cluster.png" width="80%"
          format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para> Enter a custom <guimenu>Cluster Name</guimenu> with which to
       identify the cluster the <guimenu>Cluster Dashboard</guimenu>. </para>
     </step>
     <step>
      <para> Enter the <guimenu>Host Name</guimenu> of one of the cluster nodes
       and confirm your changes. </para>
      <para> The <guimenu>Cluster Dashboard</guimenu> opens and shows a summary
       of the cluster you just added. </para>
     </step>
     <step>
      <para> To add more clusters to the dashboard, click the plus icon and
       enter the details for the next cluster. </para>
      <figure>
       <title>&hawk;&mdash;Cluster Dashboard</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk-dashboard-multiple-clusters.png" width="100%"
          format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk-dashboard-multiple-clusters.png" width="80%"
          format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
     <step>
      <para> To remove a cluster from the dashboard, click the
        <literal>x</literal> icon next to the cluster's summary. </para>
     </step>
     <step>
      <para> To view more details about a cluster, click somewhere into the
       cluster's box on the dashboard. </para>
      <para> This opens a new browser window or new browser tab. If you are not
       currently logged in to the cluster, this takes you to the &hawk;
       login screen. After having logged in, &hawk; shows the
        <guimenu>Cluster Status</guimenu> of that cluster in the summary view.
       From here, you can administrate the cluster with &hawk; as usual.
      </para>
     </step>
     <step>
      <para> As the <guimenu>Cluster Dashboard</guimenu> stays open in a
       separate browser window or tab, you can easily switch between the
       dashboard and the administration of individual clusters in &hawk;.
      </para>
     </step>
    </procedure>

    <para> Any status changes for nodes or resources are reflected almost
     immediately within the <guimenu>Cluster Dashboard</guimenu>. </para>


    <!--from bnc#808703:  
     Hawk's status screen is for viewing detailed status of one cluster (which is
     why you can't add other clusters to monitor when you're on the status screen).
     
     The dashboard is for showing summary details of many clusters, any of which you
     can click to get to the (more detailed) status screen showing that cluster
     only.
     
     You don't need to log in to hawk to visit the dashboard screen.  But you do
     need to be logged in to any clusters that are being viewed in the dashboard
     (since beta3 the dashboard will prompt for login details for each monitored
     cluster if you're not already logged in).  This is probably where the confusion
     starts.  That and the fact that any Hawk instance can also show a dashboard...
     
     It might help to look at it this way.  Assume you have two clusters (c0 and
     c1), and happen to have a separate unrelated system running Hawk, which you
     only use for its dashboard functionality (in fact there doesn't even need to be
     a cluster running on that system at all).  If you view the dashboard on that
     system, you don't need to log into hawk on that node, as you're not interested
     in that system itself - you just want to use its dashboard to monitor clusters
     c0 and c1.-->
   </sect3>

   <sect3>
    <title>Managing Tickets with &hawk;</title>

    <note>
     <title>Granting Tickets to Current Site</title>
     <para>
      <remark>taroth 2014-08-27: it is not explicitly mentioned in fate#316119,
       therefore I'm wondering if the same is true for any revoke operations? or
       does that work differently?</remark>Though you can view tickets for all
      sites with &hawk;, any grant operations triggered by &hawk; only
      apply to the current site, that means on the site of the cluster node that
      you are currently connected to with &hawk;. To grant a ticket to
      another site of your &geo; cluster, start &hawk; on one of the
      cluster nodes belonging to the respective site.</para>
    </note>


    <procedure id="pro.ha.config.hawk.viewtickets">
     <title>Granting, Revoking and Viewing with &hawk;</title>
     <para><remark>taroth 2014-0827: DEVs, is the following still true?</remark>
      Tickets are visible in &hawk; if they have been granted or revoked at
      least once or if they are referenced in a ticket dependency&mdash;see
       <xref linkend="pro.ha.config.hawk.geo.rsc.constraints"/>. In case a
      ticket is referenced in a ticket dependency, but has not been granted to
      any site yet, &hawk; displays it as <literal>revoked</literal>. </para>
     <step>
      <para> Start a Web browser and log in to the
       cluster.<!-- as described in
        <xref
        linkend="sec.ha.config.hawk.intro.connect"/>.-->
      </para>
     </step>
     <step>
      <para> In the left navigation bar, select <guimenu>Cluster
        Status</guimenu>. </para>
     </step>
     <step>
      <para>Switch to the <guimenu>Summary View</guimenu> or the <guimenu>Tree
        View</guimenu> to view tickets. Along with information about cluster
       nodes and resources, &hawk; also displays a
        <guimenu>Tickets</guimenu> category. </para>
      <para>It shows the following information:<remark>tarot 2014-08-27: DEVs,
       is the following correct?</remark></para>
      <itemizedlist>
       <listitem>
        <para>
         <guimenu>Granted</guimenu>: Tickets that are granted to the current
         site.</para>
       </listitem>
       <listitem>
        <para>
         <guimenu>Elsewhere</guimenu>: Tickets that are granted to another
         site.</para>
       </listitem>
       <listitem>
        <para><guimenu>Revoked</guimenu>: Tickets that have been revoked.</para>
       </listitem>
      </itemizedlist>
      <figure>
       <title>&hawk; Cluster Status (Summary View)&mdash;Ticket
        Overview</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk-geo-status-tickets.png" width="100%"
          format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk-geo-status-tickets.png" width="80%"
          format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
     <step>
      <para>To view more details, either click the title of the
        <guimenu>Tickets</guimenu> category or the individual ticket entries
       that are marked as links. Hover the cursor over the information icon next
       to the ticket to display the following information: time when the ticket
       has been last granted, <remark>taroth 2014-08-27: DEVs, not sure what
        leader means here? does it display the IP address of the site that holds
        the ticket? (=the virtual IP configured for boothd at that
        site)?</remark>the leader, and the ticket expiry date.</para>
      <figure>
       <title>&hawk; Cluster Status (Summary View)&mdash;Ticket
        Details</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk-geo-ticket-details.png" width="100%"
          format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk-geo-ticket-details.png" width="80%"
          format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
     <step>
      <para>To revoke a ticket, click the wrench icon next to the ticket and
       select <guimenu>Revoke</guimenu>. Confirm your choice when &hawk;
       prompts for a confirmation. </para>
      <para>If the ticket cannot be revoked for any reasons, &hawk; shows an
       error message. After the ticket has been successfully revoked, &hawk;
       will update the ticket status in the <guimenu>Tickets</guimenu>
       category.</para>
     </step>
     <step>
      <para>You can only grant tickets that are not already given to any site.
       To grant a ticket to the current site:</para>
      <substeps>
       <step>
        <para>Click the wrench icon next to a ticket with the current status
          <guimenu>Revoked</guimenu> and select <guimenu>Grant</guimenu>.</para>
       </step>
       <step>
        <para>Confirm your choice when &hawk; prompts for a confirmation. </para>
        <para>If the ticket cannot be granted for any reasons, &hawk; shows
         an error message. After the ticket has been successfully granted,
         &hawk; will update the ticket status in the
          <guimenu>Tickets</guimenu> category.</para>
       </step>
      </substeps>
     </step>
    </procedure>


    <procedure id="pro.ha.config.hawk.geo.simulator">
     <title>Simulating Granting and Revoking Tickets</title>
     <para> &hawk;'s <guimenu>Simulator</guimenu> allows you to explore
      failure scenarios before they happen. To explore if your resources that
      depend on a certain ticket behave as expected, you can also test the
      impact of granting or revoking tickets. </para>
     <step>
      <para> Start a Web browser and log in to &hawk;. </para>
     </step>
     <step>
      <para> Click the wrench icon next to the username in the top-level row,
       and select <guimenu>Simulator</guimenu>. </para>
      <para> &hawk;'s background changes color to indicate the simulator is
       active. A simulator dialog opens in the bottom right hand corner of the
       screen. Its title <guimenu>Simulator (initial state)</guimenu> indicates
       that <guimenu>Cluster Status</guimenu> screen still reflects the current
       state of the cluster. </para>
     </step>
     <step>
      <para> To simulate status change of a ticket: </para>
      <substeps>
       <step>
        <para> Click <guimenu>+Ticket</guimenu> in the simulator control dialog.
        </para>
       </step>
       <step>
        <para> Select the <guimenu>Action</guimenu> you want to simulate.
        </para>
       </step>
       <step>
        <para> Confirm your changes to add them to the queue of events listed in
         the controller dialog below <guimenu>Injected State</guimenu>. </para>
       </step>
      </substeps>
     </step>
     <step>
      <para> To start the simulation, click <guimenu>Run</guimenu> in the
       simulator control dialog. The <guimenu>Cluster Status</guimenu> screen
       displays the impact of the simulated events. The simulator control dialog
       changes to <guimenu>Simulator (final state)</guimenu>. </para>
     </step>
     <step>
      <para> To exit the simulation mode, close the simulator control dialog.
       The <guimenu>Cluster Status</guimenu> screen switches back to its normal
       color and displays the current cluster state. </para>
     </step>
    </procedure>
    <figure>
     <title>&hawk;Simulator&mdash;Tickets</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk-geo-simulator-tickets.png" width="100%"
        format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk-geo-simulator-tickets.png" width="80%"
        format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para> For more information about &hawk;'s <guimenu>Simulator</guimenu>
     (and which other scenarios can be explored with it), refer to the
      <citetitle>&haguide;</citetitle> for &productname;, available from
     &suse-onlinedoc;. Refer to chapter <citetitle>Configuring and Managing
      Cluster Resources (Web Interface)</citetitle>, section <citetitle>
      Exploring Potential Failure Scenarios</citetitle>.</para>
   </sect3>

  </sect2>
</sect1>
 <sect1 id="sec.ha.geo.trouble">
  <!--taroth 2013-04-24: fix for bnc#753625-->

  <title>Troubleshooting</title>

  <para> Booth <!--logs to <filename>/var/log/messages</filename> and -->uses the same logging
   mechanism as the CRM. Thus, changing the log level will also take effect on booth logging. The
   booth log messages also contain information about any tickets. </para>

  <para> Both the booth log messages and the booth configuration file are included in the
   <command>hb_report</command> and <command>crm_report</command>. </para>

  <para> In case of unexpected booth behavior or any problems, check the logging data with
   <command>sudo journalctl -n</command> or create a detailed cluster report with either
   <command>hb_report</command> or <command>crm_report</command>. </para>

  <para>In case you can access the cluster nodes on all sites (plus the arbitrators) from one single
   host via SSH, it is possible to collect log files from all of them within the same
    <literal>hb_report</literal>. When calling <command>hb_report</command> with the
    <option>-n</option> option, it gets the log files from all hosts that you specify with
    <option>-n</option> (instead of trying to obtain the list of nodes from the respective cluster).
   For example, to create a single <literal>hb_report</literal> including the log files from two
   two-node clusters (<literal>192.168.2.190</literal>|<literal>192.168.2.191</literal> and
    <literal>192.168.1.90</literal>|<literal>192.168.1.91</literal>) and an arbitrator
    (<literal>147.2.207.14</literal>), use the following command:</para>
  <screen>&prompt.root; hb_report -n "147.2.207.14 192.168.2.190 192.168.1.90 192.168.2.191
  192.168.1.91" -f 10:00 -t 11:00 db-incident</screen>
  <para>If the issue is about booth only and you know on which cluster nodes (within a site)
   booth is running, then specify only those two nodes plus the arbitrator.</para>
  <para>If there is no way to access all sites from one host, you need to run
   <command>hb_report</command> individually on the arbitrator and on the cluster nodes of the
   individual sites, specifying the same
   period of time. To collect the logs on an arbitrator, you must use the <option>-S</option> option
   for single node operation: </para>
  <screen>site1# hb_report -f 10:00 -t 11:00 db-incident-site1
site2# hb_report -f 10:00 -t 11:00 db-incident-site2
arbitrator# hb_report -S -f 10:00 -t 11:00 db-incident-arb</screen>
  <para>However, it is preferable to produce one single <literal>hb_report</literal> for all
   machines that you need log files from. </para>
 </sect1>
 <sect1 id="sec.ha.geo.upgrade">
 <title>Upgrading to the Latest Product Version</title>
  <para>For general instructions on how to upgrade a cluster, see the
    <citetitle>&haguide;</citetitle> for &productname;
   &productnumber;. It is available at &suse-onlinedoc;. The chapter
    <citetitle>Upgrading Your Cluster and Updating Software
   Packages</citetitle> also describes which preparations to take care of
   before starting the upgrade process.</para>
  <sect2 id="sec.ha.geo.upgrade.sle12">
   <title>Upgrading from SLE&nbsp;HA&nbsp;11 SP3 to
    SLE&nbsp;HA&nbsp;12</title>
   <para>The former booth version (v0.1) was based on the Paxos algorithm. The
    current booth version (v0.2) is loosely based on raft and incompatible
    with the one running v0.1. Therefore, rolling upgrades are not
    possible. Due to the new multi-tenancy feature, the new arbitrator init
    script cannot stop nor test the status of the Paxos v0.1 arbitrator.
    On upgrade to v0.2, the arbitrator will be stopped, if running.
    The OCF resource-agent 
    <literal>ocf:pacemaker:booth-site</literal> is capable of
    stopping and monitoring the booth v0.1 site daemon.</para>
   <procedure>
    <step>
     <para>For an upgrade of the cluster nodes from &productname; 11 SP3 to
      &productname; 12, follow the instructions in the
       <citetitle>&haguide;</citetitle> for &productname;
      &productnumber;, section <citetitle> Upgrading from SLE HA 11 SP3 to
       SLE HA 12</citetitle>. </para>
    </step>
    <step>
     <para>If you use arbitrators outside of the cluster sites:</para>
     <substeps>
      <step>
       <para>Upgrade them from &sls; 11&nbsp;SP3 to &sls; 12,
       too.</para>
      </step>
      <step>
       <para>Add the &hageo; add-on and install the packages as described in
         <xref linkend="sec.ha.geo.inst.arbitrators"/>.
       </para>
      </step>
     </substeps>
     </step>
    <step>
     <para>Because the syntax and the consensus algorithm for booth has changed, you need to update
      the booth configuration files to match the latest requirements. Whereas previously the
      optional expiry time and weights could be specified by appending them to the ticket name with
      a semicolon (<literal>;</literal>) as separator, the new syntax has separate tokens for all
      ticket options. See <xref linkend="sec.ha.geo.booth"/> for details. If you did not specify
      expiry time or weights different from the defaults and do not want to make use of the
      multi-tenancy feature, you can still use the old
      &booth.conf;.</para>
    </step>  
    <step>
     <para>Synchronize the updated booth configuration files across all cluster sites
     and arbitrators.</para>
    </step>
    <step>
     <para>Start the booth service on the cluster sites and the arbitrators as
      described in <xref linkend="sec.ha.geo.setup.booth.service"/>. </para>
    </step>
   </procedure>
   
   
   
 </sect2>
 
  <!--taroth 2014-08-26: 
  https://mailman.suse.de/mailman/private/ha-devel/2014-August/004140.html:
  
  The SLE12 booth most probably cannot talk to the SLE11 booth,
  since the previous consensus algorythm (paxos) got replaced.
  
  Hence, it is more involved. The best way would be to take the
  booth completely out of picture during upgrade.
  
  The backup site can be upgraded without any special extra steps
  for GEO. The only thing not to forget is to convert the booth
  configuration.
  
  Of course, particular attention needs the active site, i.e. the
  site where the protected resources are currently running. I'd
  propose the following procedure for the active site:
  
  ==========
  
  1. Remove the dependency on the booth resource and on the
  ticket.
  
  Our GEO quick guide recommends adding an order constraint
  between the booth group and the protected resource:
  
  order order-booth-rsc1 inf: g-booth rsc1:promote
  
  That constraint should be removed. All rsc_ticket constraints
  should be removed too.
  
  The protected resource will continue to run uninterrupted.
  
  2. Upgrade booth on all arbitrators and modify the booth
  configuration to match the new format. Start booth on all
  arbitrators and make sure that it's running.
  
  3. Update the booth configuration files on all nodes at this
  site.
  
  4. Update the cluster stack and booth.
  
  5. Once the cluster stack and all resources are running, use
  booth list to verify that the ticket is still granted to this
  site. If it is not, then grant the ticket. Also verify that the
  arbitrators have booth list output consistent with the site.
  
  6. Add back the order and rsc_ticket constraints removed in
  step 1.
  
  ==========
  
  On the inactive site, we'll have just the booth configuration
  file modifications. The rest shouldn't matter. -->
</sect1>
 
 
</article>
