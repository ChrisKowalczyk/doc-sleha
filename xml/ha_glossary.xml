<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE glossary PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]><!--taroth 2012-01-13: more terms to add for the future:
 * constraint (+ types of constraints)
 * types of resources: primitives, groups, etc.
 * meta attributes, instance attributes, operations, utilization attributes
 * transition graph
 * daemons: attrd, lrmd, mgmtd, stonithd
 * tools: hawk, crm shell, yast
 * resresource ource templates, configuration templates
 * resource set-->
<glossary id="gl.heartb">
 <title>Terminology</title>
 <glossentry>
  <glossterm>active/active, active/passive</glossterm>
  <glossdef>
   <para>
    <remark>taroth 2011-01-13: maybe also mention the term in the context
    of DRBD?</remark>
    A concept of how services are running on nodes. An active-passive
    scenario means that one or more services are running on the active node
    and the passive node waits for the active node to fail. Active-active
    means that each node is active and passive at the same time.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>arbitrator</glossterm>
  <glossdef>
   <para>Additional instance in a multi-site cluster that helps to reach
  consensus about decisions such as failover of resources across sites. 
  Arbitrators are single machines that run a booth instance
   in a special mode. </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>&ay;</glossterm>
  <glossdef>
   <para>
      &ay; is a system for installing one or more &sle; systems
      automatically and without user intervention.
     </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>booth</glossterm>
  <glossdef>
   <para>The instance that manages the failover process between the sites 
  of a multi-site cluster. It guarantees that the cluster resources 
   will be highly available across different cluster sites. This is 
   achieved by using so-called tickets that are treated as failover domain 
   between cluster sites, in case a site should be down.
 </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>booth daemon 
  (<systemitem class="daemon">boothd</systemitem>)</glossterm>
  <glossdef>
   <para>Each of the participating clusters and arbitrators in a multi-site 
    cluster runs a service, the boothd. It connects to the booth daemons 
    running at the other sites and exchanges connectivity details.</para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>cluster</glossterm>
  <glossdef>
   <para>
    A high-performance cluster is a group of computers (real or virtual)
    sharing the application load in order to achieve faster results. A
    high-availability cluster is designed primarily to secure the highest
    possible availability of services.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>cluster information base (CIB)</glossterm>
  <glossdef>
   <para>
    A representation of the whole cluster configuration and status (cluster 
    options, nodes, resources, constraints and the relationship to 
    each other). It is written in XML and resides in memory. A master CIB is 
    kept and maintained on the <xref linkend="glos.dc" xrefstyle="select:nopage"/> 
    and replicated to the other nodes. Normal read and write operations on the 
    CIB are serialized through the master CIB. 
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>cluster partition</glossterm>
  <glossdef>
   <para>
    <remark>fabian: TODO: check emap: I tried to make the definition a little 
    clearer according to the comments above. Please check for accuracy. 
    If okay, please remove all comments.</remark>
    Whenever communication fails between one or more nodes and the rest of the
    cluster, a cluster partition occurs. The nodes of a cluster are split in
    partitions but still active. They can only communicate with nodes in the
    same partition and are unaware of the separated nodes. As the loss of the
    nodes on the other partition cannot be confirmed, a split brain scenario
    develops (see also <xref linkend="glos.splitbrain"/>).
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>cluster resource manager (CRM)</glossterm>
  <glossdef>
   <para>
    The main management entity responsible for coordinating all non-local
    interactions. The &hasi; uses Pacemaker as CRM. 
    Each node of the cluster has its own CRM instance, but the one
    running on the DC is the one elected to relay decisions to the other
    non-local CRMs and process their input. A CRM interacts with a number of
    components: local resource managers, both on its own node and on the
    other nodes, non-local CRMs, administrative commands, the fencing
    functionality, and the membership layer.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>cluster resource manager daemon 
  (<systemitem class="daemon">crmd</systemitem>)</glossterm>
  <glossdef>
   <para>The CRM is implemented as daemon, crmd. It  has an instance on each 
   cluster node. All cluster decision-making is centralized by electing
   one of the crmd instances to act as a master. If the elected crmd process
   fails (or the node it is on), a new one is established. </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>concurrency violation</glossterm>
  <glossdef>
   <para>A resource that should be running on only one node in
   the cluster is running on several nodes.   
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>consensus cluster membership (CCM)</glossterm>
  <glossdef>
   <para>
    The CCM determines which nodes make up the cluster and shares this
    information across the cluster. Any new addition and any loss of nodes
    or quorum is delivered by the CCM. A CCM module runs on each node of the
    cluster.
   </para>
  </glossdef>
 </glossentry>
  <glossentry>
  <glossterm>&csync;</glossterm>
  <glossdef>
   <para>
      A synchronization tool that can be used to replicate configuration
      files across all nodes in the cluster. </para>
  </glossdef>
 </glossentry>
 <glossentry id="glos.dc">
  <glossterm>designated coordinator (DC)</glossterm>
  <glossdef>
   <para>
    One CRM in the cluster is elected as the Designated Coordinator (DC). 
    The DC is the only entity in the cluster that can decide that a cluster-wide 
    change needs to be performed, such as fencing a node or moving resources around. 
    The DC is also the node where the master copy of the CIB is kept. All other 
    nodes get their configuration and resource allocation information from the 
    current DC. The DC is elected from all nodes in the cluster after a 
    membership change.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>distributed lock manager (DLM)</glossterm>
  <glossdef>
   <para>
    DLM coordinates disk access for clustered file systems and administers
    file locking to increase performance and availability.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>distributed replicated block device (DRBD)</glossterm>
  <glossdef>
   <para>
    DRBD is a block device designed for building high availability clusters.
    The whole block device is mirrored via a dedicated network and is seen
    as a network RAID-1.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>failover</glossterm>
  <glossdef>
   <para>
    Occurs when a resource or node fails on one machine and the affected
    resources are started on another node.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>failover domain</glossterm>
  <glossdef>
   <para>A named subset of cluster nodes that are eligible to run a
   cluster service in the event of a node failure.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>fencing</glossterm>
  <glossdef>
   <para>
    Describes the concept of preventing access to a shared resource by
    isolated or failing cluster members. Should a cluster node fail, it will be 
    shut down or reset to prevent it from causing trouble. This way, resources 
    are locked out of a node whose status is uncertain.
    <!--Fencing is distinguished between node and resource fencing.-->
    <remark>fabian: TODO emap: Since the difference is not explained, better 
    drop it.</remark>
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>geographically dispersed cluster (geo cluster)</glossterm>
  <glossdef>
   <para>See <xref linkend="glos.geo"/>.</para>
  </glossdef>
 </glossentry>
 <!--taroth 2012-01-09: new term (merged from Fabian's version of ha_glossary.xml)--> 
<!--<glossentry><glossterm>high availability</glossterm>
<glossdef>
<para>High availability is a system design approach and associated service
implementation that ensures a prearranged level of operational
performance will be met during a contractual measurement period.</para>
<para>Availability is a key aspect of service quality. Availability is usually calculated based on a model involving the Availability Ratio and techniques such as Fault Tree Analysis.</para>
<para>See also: <ulink url="http://en.wikipedia.org/wiki/High_availability/"/> <ulink url="http://www.itlibrary.org/index.php?page=Availability_Management"/></para>
</glossdef>
</glossentry>-->
 <glossentry>
  <glossterm>local cluster</glossterm>
  <glossdef>
   <para>A single cluster in one location (for example, all nodes are located
   in one data center). Network latency can be neglected. Storage is typically 
   accessed synchronously by all nodes. </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>local resource manager (LRM)</glossterm>
  <glossdef><para>
    The local resource manager (LRM) is responsible for performing
    operations on resources. It uses the resource agent scripts to carry out
    these operations. The LRM is <quote>dumb</quote> in that it does not know of any
    policy. It needs the DC to tell it what to do.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>metro cluster</glossterm>
  <glossdef>
   <para>A single cluster that can stretch over multiple buildings or data 
  centers, with all sites connected by fibre channel. Network latency is usually
  low (&lt;5 ms for distances of approximately 20&nbsp;miles). Storage is 
  frequently replicated (mirroring or synchronous replication).</para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>multicast</glossterm>
  <glossdef>
   <para>A technology used for a one-to-many communication within a network
      that can be used for cluster communication. &corosync; supports both
      multicast and unicast.
     </para>
  </glossdef>
 </glossentry>
 <glossentry id="glos.geo">
  <glossterm>multi-site cluster</glossterm>
  <glossdef>
   <para>Consists of multiple, geographically dispersed sites with a 
 local cluster each. The sites communicate via IP. Failover across the sites is 
 coordinated by a higher-level entity, the booth. Multi-site clusters have to 
 cope with limited network bandwidth and high latency. Storage is replicated 
 asynchronously.
  </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>node</glossterm>
  <glossdef>
   <para>
    Any computer (real or virtual) that is a member of a cluster and
    invisible to the user.
   </para>
  </glossdef>
 </glossentry>
 <!-- pingd is deprecated!! the resource is named ping now
 <glossentry><glossterm>pingd</glossterm>
  <glossdef>
   <para>
    The ping daemon. It continuously contacts one or more servers outside
    the cluster with ICMP pings.
   </para>
  </glossdef>
 </glossentry>
 -->
 <glossentry>
  <glossterm>policy engine (PE)</glossterm>
  <glossdef>
   <para>
    The policy engine computes the actions that need to be taken to
    implement policy changes in the CIB. The PE also produces a transition graph 
    containing a list of (resource) actions and dependencies to achieve the next
    cluster state. The PE always runs on the DC.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>quorum</glossterm>
  <glossdef>
   <para>
    In a cluster, a cluster partition is defined to have quorum (is
    <quote>quorate</quote>) if it has the majority of nodes (or votes).
    Quorum distinguishes exactly one partition. It is part of the algorithm
    to prevent several disconnected partitions or nodes from proceeding and
    causing data and service corruption (split brain). Quorum is a
    prerequisite for fencing, which then ensures that quorum is indeed
    unique.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>resource</glossterm>
  <glossdef>
   <para>
    Any type of service or application that is known to Pacemaker. Examples
    include an IP address, a file system, or a database.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>resource agent (RA)</glossterm>
  <glossdef>
   <para>
    A resource agent (RA) is a script acting as a proxy to manage a
    resource (for example, to start, stop or monitor a resource). The &hasi; 
    supports three different kinds of resource agents: OCF (Open Cluster 
    Framework) resource agents, LSB (Linux Standards Base) resource agents 
    (Standard LSB init scripts), and &hb; resource agents (&hb; v1 resources). 
    For more information, refer to 
    <xref linkend="sec.ha.configuration.basics.raclasses"/>.
   </para>
  </glossdef>
 </glossentry>
 <!--taroth 2012-01-09: new term (merged from Fabian's version of ha_glossary.xml)-->
 <glossentry>
  <glossterm>shared disk file exclusiveness (SFEX)</glossterm>
  <glossdef>
   <para>SFEX provides storage protection over SAN.</para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>single point of failure (SPOF)</glossterm>
  <glossdef>
   <para>
    A single point of failure (SPOF) is any component of a cluster that,
    should it fail, triggers the failure of the entire cluster.
   </para>
  </glossdef>
 </glossentry>
 <!--taroth 2012-01-09: new term (merged from Fabian's version of ha_glossary.xml)-->
 <glossentry id="glos.splitbrain">
  <glossterm>split brain</glossterm>
  <glossdef>
   <para>
    A scenario in which the cluster nodes are divided into two or more
    groups that do not know of each other (either through a software or
    hardware failure). STONITH prevents a split brain situation from badly
    affecting the entire cluster. Also known as a <quote>partitioned
    cluster</quote> scenario.
   </para>
   <para>
    The term split brain is also used in DRBD but means that the two nodes
    contain different data.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>&stonith;</glossterm>
  <glossdef>
   <para>
    The acronym for <quote>Shoot the other node in the head</quote>, which
    refers to the fencing mechanism that shuts down a misbehaving node to
    prevent it from causing trouble in a cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>&stonith; block device (SBD)</glossterm>
  <glossdef>
   <para>In an environment where all nodes have access to shared storage, a
small partition is used for disk-based fencing.</para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>ticket</glossterm>
  <glossdef>
   <para>A component used in multi-site clusters. A ticket grants the right to 
    run certain resources on a specific cluster site. A ticket can only be owned
    by one site at a time. Resources can be bound to a certain ticket by 
    dependencies. Only if the defined ticket is available at a site, the 
    respective resources are started. Vice versa, if the ticket is removed, the 
    resources depending on that ticket are automatically stopped.
    </para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>Totem redundant ring protocol (RRP)</glossterm>
  <glossdef>
   <para>
      Allows the use of multiple redundant local-area networks for resilience against
      partial or total network faults. This way, cluster communication can
      still be kept up as long as a single network is operational.A logical
      token-passing ring is imposed on all participating nodes to deliver
      messages in a reliable and sorted manner. A node is allowed to broadcast a
      message only if it holds the token.</para>
  </glossdef>
 </glossentry>
 <glossentry>
  <glossterm>unicast</glossterm>
  <glossdef>
   <para>
      A technology for sending messages to a single network destination. 
      &corosync; supports both multicast and unicast. In
      &corosync;, unicast is implemented as UDP-unicast (UDPU).
     </para>
  </glossdef>
 </glossentry>
</glossary>
