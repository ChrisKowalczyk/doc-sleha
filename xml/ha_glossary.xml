<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE glossary PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<glossary id="gl.heartb">
 <title>Terminology</title>
<!-- active/active and active/passive -->
 <glossentry><glossterm>active/active, active/passive</glossterm>
  <glossdef>
   <para>
    A concept about how services are running on nodes. An active-passive
    scenario means that one or more services are running on the active node
    and the passive nodes waits for the active node to fail. Active-active
    means that each node is active and passive at the same time.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>cluster</glossterm>
  <glossdef>
   <para>
    A high-performance cluster is a group of computers (real or virtual)
    sharing the application load in order to achieve faster results. A
    high-availability cluster is designed primarily to secure the highest
    possible availability of services.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>cluster partition</glossterm>
  <glossdef>
   <para>
    <remark>rwalter: i think this could be defined better.  what part is the
     cluster partition or is it the state of a cluster that is divided by a
     communication failure? how do you know which part is the
     partition?</remark>
    <remark role="clarity">
     2007-03-19 - jjaeger: cluster partition is the state of the cluster and it
     can't be said which part is the partition ... the *state* is called
     cluster partition.
     </remark>
    Whenever communication fails between one or more nodes and the rest of
    the cluster, a cluster partition occurs. The nodes of a cluster
    partition are still active and able to communicate with each other, but
    they are unaware of the nodes with which they cannot communicate. As the
    loss of the other partition cannot be confirmed, a split brain scenario
    develops (see also <xref linkend="glos.splitbrain"/>).
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>consensus cluster membership (CCM)</glossterm>
  <glossdef>
   <para>
    <remark>rwalter: this says what it does but not what it is. does that
     matter?</remark>
    <remark role="clarity">
     2007-03-19 - jjaeger: no, it doesn't matter - it is the closest to a
     definition you can get from the HB guys. I'd leave it this way.
     </remark>
    The CCM determines which nodes make up the cluster and shares this
    information across the cluster. Any new addition and any loss of nodes
    or quorum is delivered by the CCM. A CCM module runs on each node of the
    cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>cluster information base (CIB)</glossterm>
  <glossdef>
   <para>
    A representation of the whole cluster configuration and status (node
    membership, resources, constraints, etc.) written in XML and residing in
    memory. A master CIB is kept and maintained on the DC and replicated to
    the other nodes.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>cluster resource manager (CRM)</glossterm>
  <glossdef>
   <para>
    The main management entity responsible for coordinating all nonlocal
    interactions. Each node of the cluster has its own CRM, but the one
    running on the DC is the one elected to relay decisions to the other
    nonlocal CRMs and process their input. A CRM interacts with a number of
    components: local resource managers both on its own node and on the
    other nodes, nonlocal CRMs, administrative commands, the fencing
    functionality, and the membership layer.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>designated coordinator (DC)</glossterm>
  <glossdef>
   <para>
    The <quote>master</quote> node. This node is where the master copy of
    the CIB is kept. All other nodes get their configuration and resource
    allocation information from the current DC. The DC is elected from all
    nodes in the cluster after a membership change.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Distributed replicated block device (drbd)</glossterm>
  <glossdef>
   <para>
    DRBD is a block device designed for building high availability clusters.
    The whole block device is mirrored via a dedicated network and is seen
    as a network RAID-1.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>failover</glossterm>
  <glossdef>
   <para>
    Occurs when a resource or node fails on one machine and the affected
    resources are started on another node.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>fencing</glossterm>
  <glossdef>
   <para>
    Describes the concept of preventing access to a shared resource by
    non-cluster members. It can be archived by killing (shutting down) a
    <quote>misbehaving</quote> node to prevent it from causing trouble,
    locking resources away from a node whose status is uncertain, or in
    several other ways. Furthermore, fencing is distinguished between node
    and resource fencing.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>&hb; resource agent</glossterm>
  <glossdef>
   <para>
    &hb; resource agents were widely used with &hb; version 1. Their use is
    deprecated, but still supported in version 2. A &hb; resource agent can
    perform <option>start</option>, <option>stop</option>, and
    <option>status</option> operations and resides under
    <filename>/etc/ha.d/resource.d </filename> or
    <filename>/etc/init.d</filename>. For more information about &hb;
    resource agents, refer to
    <ulink url="http://www.linux-ha.org/HeartbeatResourceAgent"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>local resource manager (LRM)</glossterm>
  <glossdef>
   <para>
    The local resource manager (LRM) is responsible for performing
    operations on resources. It uses the resource agent scripts to carry out
    the work. The LRM is <quote>dumb</quote> in that it does not know of any
    policy by itself. It needs the DC to tell it what to do.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>LSB resource agent</glossterm>
  <glossdef>
   <para>
    LSB resource agents are standard LSB init scripts. LSB init scripts are
    not limited to use in a high availability context. Any LSB-compliant
    Linux system uses LSB init scripts to control services. Any LSB resource
    agent supports a <option>start</option>, <option>stop</option>,
    <option>restart</option>, <option>status</option> and
    <option>force-reload</option> option and may optionally provide
    <option>try-restart</option> and <option>reload</option> as well. LSB
    resource agents are located in <filename>/etc/init.d</filename>. Find
    more information about LSB resource agents and the actual specification
    at <ulink url="http://www.linux-ha.org/LSBResourceAgent"/> and
    <ulink url="http://www.linux-foundation.org/spec/refspecs/LSB_3.0.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html"/>
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>node</glossterm>
  <glossdef>
   <para>
    Any computer (real or virtual) that is a member of a cluster and
    invisible to the user.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>pingd</glossterm>
  <glossdef>
   <para>
    The ping daemon. It continuously contacts one or more servers outside
    the cluster with ICMP pings.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>policy engine (PE)</glossterm>
  <glossdef>
   <para>
    The policy engine computes the actions that need to be taken to
    implement policy changes in the CIB. This information is then passed on
    to the transaction engine, which in turn implements the policy changes
    in the cluster setup. The PE always runs on the DC.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>OCF resource agent</glossterm>
  <glossdef>
   <para>
    OCF resource agents are similar to LSB resource agents (init scripts).
    Any OCF resource agent must support <option>start</option>,
    <option>stop</option>, and <option>status</option> (sometimes called
    <option>monitor</option>) options. Additionally, they support a
    <option>metadata</option> option that returns the description of the
    resource agent type in XML. Additional options may be supported, but are
    not mandatory. OCF resource agents reside in
    <filename>/usr/lib/ocf/resource.d/<replaceable>provider</replaceable>.</filename>
    Find more information about OCF resource agents and a draft of the
    specification at <ulink url="http://www.linux-ha.org/OCFResourceAgent"/>
    and
    <ulink url="http://www.opencf.org/cgi-bin/viewcvs.cgi/specs/ra/resource-agent-api.txt?rev=HEAD"/>.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>quorum</glossterm>
  <glossdef>
   <para>
    In a cluster, a cluster partition is defined to have quorum (is
    <quote>quorate</quote>) if it has the majority of nodes (or votes).
    Quorum distinguishes exactly one partition. It is part of the algorithm
    to prevent several disconnected partitions or nodes from proceeding and
    causing data and service corruption (split brain). Quorum is a
    prerequisite for fencing, which then ensures that quorum is indeed
    unique.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>resource</glossterm>
  <glossdef>
   <para>
    Any type of service or application that is known to &hb;. Examples
    include an IP address, a file system, or a database.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>resource agent (RA)</glossterm>
  <glossdef>
   <para>
    A resource agent (RA) is a script acting as a proxy to manage a
    resource. There are three different kinds of resource agents: OCF (Open
    Cluster Framework) resource agents, LSB resource agents (Standard LSB
    init scripts), and &hb; resource agents (&hb; v1 resources).
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>Single Point of Failure (SPOF)</glossterm>
  <glossdef>
   <para>
    A single point of failure (SPOF) is any component of a cluster that,
    should it fail, triggers the failure of the entire cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry id="glos.splitbrain"><glossterm>split brain</glossterm>
  <glossdef>
   <para>
    A scenario in which the cluster nodes are divided into two or more
    groups that do not know of each other (either through a software or
    hardware failure). STONITH prevents a split brain situation from badly
    affecting the entire cluster. Also known as a <quote>partitioned
    cluster</quote> scenario.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>STONITH</glossterm>
  <glossdef>
   <para>
    The acronym for <quote>Shoot the other node in the head</quote> which is
    basically bringing down a misbehaving node to prevent it from causing
    trouble in the cluster.
   </para>
  </glossdef>
 </glossentry>
 <glossentry><glossterm>transition engine (TE)</glossterm>
  <glossdef>
   <para>
    The transition engine (TE) takes the policy directives from the PE and
    carries them out. The TE always runs on the DC. From there, it instructs
    the local resource managers on the others nodes of which actions to
    take.
   </para>
  </glossdef>
 </glossentry>
</glossary>
