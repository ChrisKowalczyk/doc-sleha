<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ha.start">
 <title>Getting Started</title>
 <para>
  In the following, learn about the system requirements and which
  preparations
 </para>
 <para/>
 <para/>
 <para/>
 <para>
  to take before installing the &hasi;. Find a short overview of the basic
  steps to install and set up a cluster.
 </para>
 <sect1 id="sec.ha.start.hwreq">
  <title>Hardware Requirements</title>

  <para>
   The following list specifies hardware requirements for a cluster based on
   &productnamereg;. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     1 to 16 Linux servers with software as specified in
     <xref linkend="sec.ha.start.swreq"/>. The servers do not require
     identical hardware (memory, disk space, etc.).
    </para>
   </listitem>
   <listitem>
    <para>
     At least two TCP/IP communication media. Cluster nodes use multicast
     for communication so the network equipment must support multicasting.
     The communication media should support a data rate of 100 Mbit/s or
     higher. Preferably, the Ethernet channels should be bonded.
    </para>
   </listitem>
   <listitem>
    <para>
     Optional: A shared disk subsystem connected to all servers in the
     cluster from where it needs to be accessed.
    </para>
   </listitem>
   <listitem>
    <para> A &stonith; mechanism. &stonith; is an acronym for
      <quote>Shoot the other node in the head</quote>. A &stonith; device is
     a power switch which the cluster uses to reset nodes that are thought to be
     dead or behaving in a strange manner. Resetting non-heartbeating nodes is
     the only reliable way to ensure that no data corruption is performed by
     nodes that hang and only appear to be dead. </para>
    <para>
     For more information, refer to <xref linkend="cha.ha.fencing"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
<?dbfo-need height="10em"?>
 <sect1 id="sec.ha.start.swreq">
  <title>Software Requirements</title>

  <para>
   Ensure that the following software requirements are met:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &slsreg; &productnumber; with all available online updates installed on
     all nodes that will be part of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; &productnumber; including all available online updates
     installed on all nodes that will be part of the cluster.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.start.diskreq">
  <title>Shared Disk System Requirements</title>

  <para>
   A shared disk system (Storage Area Network, or SAN) is recommended for
   your cluster if you want data to be highly available. If a shared disk
   subsystem is used, ensure the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The shared disk system is properly set up and functional according to
     the manufacturer&rsquo;s instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
     Hardware-based RAID is recommended. Host-based software RAID is not
     supported for all configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </para>
   </listitem>
   <listitem>
    <para>
     When using DRBD to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the replicated
     device. Use the same (bonded) NICs that the rest of the cluster uses to
     leverage the redundancy provided there.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.start.prep">
  <title>Preparations</title>

  <para>
   Prior to installation, execute the following preparatory steps:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Configure hostname resolution and use static host information by
     <remark>taroth 090331:
      removing snippet about DNS server - Dejan 090327: I'd recommend using strictly static host
      information (/etc/hosts). If DNS fails for whatever reason, there'd be absolute chaos.
     </remark>
<!--either setting up a DNS server or-->
     by editing the <filename>/etc/hosts</filename> file on each server in
     the cluster. For more information, refer to
     <citetitle>&sls;</citetitle>
     &admin;, chapter
     <citetitle>Basic Networking</citetitle>
     , section
     <citetitle>Configuring
      Hostname and DNS</citetitle>
<!-- as described in  <xref
      linkend="proc.ha.start.prep.hostnames"/>-->
     , available at <ulink url="http://www.novell.com/documentation"/>.
    </para>
    <para>
     It is essential that members of the cluster are able to find each other
     by name. If the names are not available, internal cluster communication
     will fail.
    </para>
   </listitem>
   <listitem>
    <para>
     Configure time synchronization by making cluster nodes synchronize to a
     time server outside the cluster
<!-- as described in <xref linkend="proc.ha.start.prep.synchron"/>-->
     . For more information, refer to
     <citetitle>&sls;</citetitle>
     &admin;, chapter
     <citetitle>Time
      Synchronization with NTP</citetitle>
     , available at
     <ulink
      url="http://www.novell.com/documentation"/>.
    </para>
    <para>
     The cluster nodes will use the time server as their time
     synchronization source.
    </para>
   </listitem>
  </itemizedlist>

<!--<procedure id="proc.ha.start.prep.hostnames">
   <title>Modifying <filename>/etc/hosts</filename> with &yast;</title>
   <step id="st.ha.start.prep.hostnames" performance="required">
    <para>
     On a cluster server, start &yast; and select <menuchoice>
     <guimenu>Network Services</guimenu> <guimenu>Hostnames</guimenu>
     </menuchoice>.
    </para>
   </step>
   <step id="st.ha.start.prep.hostnames.host_config"
    performance="required">
    <para>
     In the <guimenu>Host Configuration</guimenu> window, click
     <guimenu>Add</guimenu> and fill in the necessary information in the
     pop-up window for one other server in the cluster.
    </para>
    <para>
     This information includes the IP address, hostname (for example
     <literal> node2.novell.com</literal>), and host alias (for example
     <literal>node2</literal>) of the other server.
    </para>
    <para>
     Use node names for host aliases. You can find node names by executing
     <command>uname&nbsp;-n</command> at a command prompt on each server.
    </para>
   </step>
   <step id="st.ha.start.prep.hostnames.ok" performance="required">
    <para>
     Click <guimenu>OK</guimenu>, and then repeat
     <xref
      linkend="st.ha.start.prep.hostnames.host_config"
      xrefstyle="StepXRef"/>
     to add the other nodes in the cluster to the
     <filename>/etc/hosts</filename> file on this server.
    </para>
   </step>
   <step>
    <para>
     Repeat
     <xref linkend="st.ha.start.prep.hostnames"
      xrefstyle="StepXRef"/>
     through
     <xref
      linkend="st.ha.start.prep.hostnames.ok" xrefstyle="StepXRef"/>
     on each server in your cluster.
    </para>
   </step>
   <step>
    <para>
     To check if hostname resolution is functioning properly, ping the node
     name (host alias) you specified in
     <xref
      linkend="st.ha.start.prep.hostnames.host_config"
      xrefstyle="StepOnPage"/>.
    </para>
   </step>
  </procedure>
-->

<!--<procedure id="proc.ha.start.prep.synchron">
   <title>Configuring the Cluster Servers for Time Synchronization</title>
   <para>
    Time synchronization will not start unless the date and time on the
    cluster servers is already close. To manually set the date and time on
    cluster servers, use the <command>date</command> command at the command
    line of each cluster server to view, and if necessary change, the date
    and time for that server.
   </para>
   <para>
    To configure the nodes in the cluster to use the time server as their
    time source proceed as follows:
   </para>
   <step id="st.ha.start.prep.synchron.ntp" performance="required">
    <para>
     On a cluster server, start &yast; and select <menuchoice>
     <guimenu>Network Services</guimenu> <guimenu>NTP
     Configuration</guimenu> </menuchoice>.
    </para>
   </step>
   <step>
    <para>
     Choose to have the NTP daemon start during boot and then enter the IP
     address of the time server you configured.
    </para>
   </step>
   <step id="st.ha.start.prep.synchron.finish" performance="required">
    <para>
     Click <guimenu>OK</guimenu> and reboot this server to ensure the
     service starts correctly.
    </para>
   </step>
   <step>
    <para>
     Repeat
     <xref linkend="st.ha.start.prep.synchron.ntp"
      xrefstyle="StepXRef"/>
     through
     <xref
      linkend="st.ha.start.prep.synchron.finish" xrefstyle="StepXRef"/>
     on the other non-time server nodes in the cluster.
    </para>
   </step>
   <step>
    <para>
     Reboot all cluster nodes. After rebooting the nodes, time should be
     synchronized properly.
    </para>
   </step>
   <step>
    <para>
     To check if time synchronization is functioning properly, run the
     <command>ntpq -p command</command> on each cluster node. Running the
     <command>ntpq -p</command> command on a non-time server node will
     display the server that is being used as the time source.
    </para>
   </step>
  </procedure>-->
 </sect1>
 <sect1 id="sec.ha.start.oview">
  <title>Overview: Installing and Setting Up a Cluster</title>

  <para>
   After the preparations are done, the following steps are necessary to
   install and set up a cluster with &productnamereg;:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Installing &slsreg; &productnumber; and &productnamereg;
     &productnumber; as add-on on top of &sls;. For detailed information,
     see <xref linkend="sec.ha.installation.inst"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Configuring &ais;. For detailed information, see
     <xref
      linkend="sec.ha.installation.setup"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Starting &ais; and monitoring the cluster status. For detailed
     information, see <xref
      linkend="sec.ha.installation.start"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Adding and configuring cluster resources, either with a graphical user
     interface (GUI) or from command line. For detailed information, see
     <xref linkend="cha.ha.configuration.gui"/> or
     <xref linkend="cha.ha.manual_config"/>.
    </para>
    <para>
     To protect your data from possible corruption by means of fencing and
     &stonith;, make sure to configure &stonith; devices as resources. For
     detailed information, see <xref
      linkend="cha.ha.fencing"/>.
    </para>
    <para>
     You might also need to create file systems on a shared disk (Storage
     Area Network, SAN) if they do not already exist and, if necessary,
     configure those file systems as cluster resources.
    </para>
    <para>
     Both cluster-aware (OCFS 2) and non-cluster-aware file systems can be
     configured with the &hasi;. If needed, you can also make use of data
     replication with DRBD. For detailed information, see
     <xref linkend="part.storage"/>.
    </para>
   </listitem>
<!--taroth: for next revision, perhaps add monitoring and managing the
   cluster as separate listitem, if reflected in the book structure-->
  </orderedlist>
 </sect1>
</chapter>
