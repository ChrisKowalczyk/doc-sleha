<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!--
  TODOs:
* taroth 090713: for next revision, elaborate sec.ha.manual_config.remove with input
 from http://bugzilla.novell.com/show_bug.cgi?id=520718
 => Implemented

* FATE 305470 Enhance Data Replication capabilities
 => Done

 Future TODOs:
 * Correct IDs
 * FATE 304867 Multilevel administration rights for CIB
   => 2010-02-23: According to Lars, not fully functionaly yet

 http://www.clusterlabs.org/wiki/Example_configurations
-->

<chapter id="cha.ha.manual_config">
 <title>Configuring and Managing Cluster Resources (Command Line)</title>
 <abstract>
  <para>
   Like in
   <xref linkend="cha.ha.configuration.gui" xrefstyle="select:label"/>, a
   cluster resource must be created for every resource or application you
   run on the servers in your cluster. Cluster resources can include Web
   sites, e-mail servers, databases, file systems, virtual machines, and any
   other server-based applications or services that you want to make
   available to users at all times.
  </para>

  <para>
   You can either use the graphical HA Management Client utility, or the
   <command>crm</command> command line utility to create resources. This
   chapter introduces the several <command>crm</command> utilities.
  </para>
 </abstract>
 <sect1 id="sec.ha.manual_config.crm">
  <title><command>crm</command> Command Line Tool&mdash;Overview</title>
  <para> After installation usually you need the <command>crm</command>
   command only. This command has several subcommands which manage
   resources, CIBs, nodes, or resource agents, and others. Run
    <command>crm <option>help</option></command> to get an overview of
   all available commands. It has a thorough help system with embedded
   examples. </para>
  
  <para>The <command>crm</command> command can be used in the following
   ways:</para>
  
  <itemizedlist>
   <listitem>
    <formalpara>
     <title>Directly</title>
     <para>Add all subcommands to <command>crm</command>, press
      <keycap function="enter"/> and you see the output immediately. For
     example, enter <command>crm</command> <option>help ra</option> to
      get information about the <command>ra</command> subcommand.</para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>As Shell Script</title>
     <para>Use <command>crm</command> and a script with
       <command>crm</command> commands. This can be done in two ways:</para>
    </formalpara>
    <screen><command>crm</command> -f script.cli
<command>crm</command> &lt; script.cli</screen>
    <para>The script can have any command from <command>crm</command>.
    For example:</para>
    <screen># A small example
<command>status</command>
<command>node</command> list</screen>
    <para>Any line starting with the hash symbol (#) is a comment and is
    ignored. If a line is too long, insert a backslash (\) at the end
    and continue it in the next line.</para>
   </listitem>
   <listitem>
    <formalpara>
     <title>Interactive As Internal Shell</title>
     <para>Type <command>crm</command> to enter the internal shell. The
      prompt changes to <literal>crm(live)#</literal>. With <command>help</command> you
      can get an overview of the available subcommands. As the
      internal shell has different levels of subcommands,
      you can <quote>enter</quote> one by just typing this subcommand
      and press <keycap function="enter"/>.
     </para>
    </formalpara>
    <para>For example, if you type <command>resource</command> you enter
    the resource management level. It can easily be verified as your
    prompt changes to <literal>crm(live)resource#</literal>. You can
     always leave the internal shell with <command>quit</command>,
     <command>bye</command>, or <command>exit</command>. If you need to
     go back one level, use <command>up</command>,
     <command>end</command>, or <command>cd</command>.</para>
    <para>It is possible to enter the level directly, if you type
     <command>crm</command>, the respective subcommand and no options.</para>
    <para>The internal shell supports also tab completion for
     subcommands and resources. Type the
     beginning of a command, press <keycap function="tab"/> and
     <command>crm</command> completes the respective object.</para>
   </listitem>
  </itemizedlist>
  
  <note>
   <title>Differentiate Between Management and Configuration Subcommands</title>
   <para>The <command>crm</command> tool has management capability (the
    subcommands <command>resource</command> and <command>node</command>)
    and can be used for configuration (<command>cib</command>,
     <command>configure</command>). </para>
  </note>
  
  <para>The following subsections give you an overview about some
   important aspects of the <command>crm</command> tool.</para>
  
  <sect2 id="sec.ha.manual_config.ocf">
   <title>Displaying Information about OCF Resource Agents</title>
   <para>As you have to deal with resource agents in your cluster
    configuration all the time, the <command>crm</command> tool
    contains the <command>ra</command> command to get information
    about resource agents  and to manage them (for additional information, see also <xref
     linkend="sec.ha.configuration.basics.raclasses"/>):
   </para>
   <screen># <command>crm</command> ra
crm(live)ra#</screen>
   <para>
    The command <command>classes</command> gives you a list of all classes
    and providers:
   </para>
   <screen>crm(live)ra# <command>classes</command>
heartbeat
lsb
ocf / heartbeat linbit lvm2 ocfs2 pacemaker
stonith</screen>
   <para>
    To get an overview about all available resource agents for a class (and
    provider) use the <command>list</command> command:
   </para>
   <screen>crm(live)ra# <command>list</command> ocf
AoEtarget           AudibleAlarm        CTDB                ClusterMon
Delay               Dummy               EvmsSCC             Evmsd
Filesystem          HealthCPU           HealthSMART         ICP
IPaddr              IPaddr2             IPsrcaddr           IPv6addr
LVM                 LinuxSCSI           MailTo              ManageRAID
ManageVE            Pure-FTPd           Raid1               Route
SAPDatabase         SAPInstance         SendArp             ServeRAID
...</screen>
   
   <para>
    An overview about a resource agent can be viewed with
    <command>info</command>:
   </para>
   <screen>crm(live)ra# <command>info</command> ocf:drbd:linbit
This resource agent manages a DRBD resource
as a master/slave resource. DRBD is a shared-nothing replicated storage
device. (ocf:linbit:drbd)

Master/Slave OCF Resource Agent for DRBD

Parameters (* denotes required, [] the default):

drbd_resource* (string): drbd resource name
    The name of the drbd resource from the drbd.conf file.

drbdconf (string, [/etc/drbd.conf]): Path to drbd.conf
    Full path to the drbd.conf file.

Operations' defaults (advisory minimum):

    start         timeout=240
    promote       timeout=90 
    demote        timeout=90 
    notify        timeout=90 
    stop          timeout=100
    monitor_Slave_0 interval=20 timeout=20 start-delay=1m
    monitor_Master_0 interval=10 timeout=20 start-delay=1m</screen>

   <para>Leave the viewer by pressing <keycap>Q</keycap>. Find a
    configuration example at <xref linkend="cha.ha.quickstart"/>.
   </para>
   
   <tip>
    <title>Use <command>crm</command> Directly</title>
    <para>The example before used the internal shell of the
     <command>crm</command> command. However, it is not necessary to use
     it. You get the same results, if you add the respective subcommands
     to <command>crm</command>. For example, you can list all the OCF
     resource agents by entering <command>crm</command> <option>ra list
      ocf</option> in your shell.
     </para>
   </tip>
   
  </sect2>
  
  <sect2 id="sec.ha.manual_config.template">
   <title>Using Templates</title>
   <!-- Info from https://bugzilla.novell.com/show_bug.cgi?id=541490 -->

   <para>Templates are ready made cluster configurations. They require
    minimum effort to be tailored to particular user's needs. The
    configuration may be edited to suit user needs. </para>

   <para> The following procedure shows how to create a simple yet
    functional Apache configuration (<literal>#</literal> indicates the
    &rootuser; prompt): </para>

   <procedure id="pro.ha.manual_config.template">
    <step>
     <para> Log in as &rootuser;. </para>
    </step>
    <step>
     <para> Start the <command>crm</command> tool: </para>
     <screen># crm configure</screen>
    </step>
    <!--  -->
    <step>
     <para> Create a new configuration from a template: </para>
     <substeps>
      <step>
       <para> Switch to the <command>template</command> subcommand: </para>
       <screen>crm(live)configure# <emphasis role="strong">template</emphasis></screen>
      </step>
      <step>
       <para> List the available templates: </para>
       <screen>crm(live)configure template#  <emphasis role="strong">list templates</emphasis>
gfs2-base   filesystem  virtual-ip  apache   clvm     ocfs2    gfs2</screen>
      </step>
      <step>
       <para> Decide which template you need. As we need an Apache
        configuration, we choose the <literal>apache</literal> template: </para>
       <screen>crm(live)configure template#  <emphasis role="strong">new intranet apache</emphasis>
INFO: pulling in template apache
INFO: pulling in template virtual-ip</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para> Define your parameters: </para>
     <substeps>
      <step>
       <para> List the just created configuration: </para>
       <screen>crm(live)configure template#  <command>list</command>
intranet</screen>
      </step>
      <step id="st.config_cli.show">
       <para> Display the minimum required changes which have to be
        filled out by you: </para>
       <screen>crm(live)configure template#  <command>show</command>
ERROR: 23: required parameter ip not set
ERROR: 61: required parameter id not set
ERROR: 65: required parameter configfile not set</screen>
      </step>
      <step id="st.config_cli.edit">
       <para> Invoke your preferred text editor and fill out all lines
        which were displayed as error in <xref
         linkend="st.config_cli.show"/>: </para>
       <screen>crm(live)configure template#  <command>edit</command></screen>
      </step>
      <!--<step>
      <para>Edit all lines starting with <literal>%%</literal>. If you
       need an overview, use the following command in another &rootuser;
       shell:</para>
      <screen>grep -n "^%%" /root/.crmconf/intranet
23:%% ip
31:%% netmask
35:%% lvs_support
61:%% id  intranet
65:%% configfile
71:%% options
76:%% envfiles</screen>
     </step>-->
     </substeps>
    </step>
    <step>
     <para> Show the configuration and see if it is valid (bold text
      depends on your configuration which you have entered in <xref
       linkend="st.config_cli.edit" xrefstyle="select:label"/>): </para>
     <screen>crm(live)configure template#  <command>show</command>
primitive virtual-ip ocf:heartbeat:IPaddr \
    params ip=<emphasis role="strong">"192.168.1.101"</emphasis>
primitive apache ocf:heartbeat:apache \
    params configfile=<emphasis role="strong">"/etc/apache2/httpd.conf"</emphasis>
monitor apache 120s:60s
group <emphasis role="strong">intranet</emphasis> \
    apache virtual-ip</screen>
    </step>
    <step>
     <para> Apply the configuration: </para>
     <screen>crm(live)configure template#  <command>apply</command>
crm(live)configure#  <command>cd ..</command>
crm(live)configure#  <command>show</command></screen>
    </step>
    <step>
     <para> Submit your changes to the CIB: </para>
     <screen>crm(live)configure#  <emphasis role="strong">commit</emphasis></screen>
    </step>
   </procedure>

   <para>It is possible to simplify the commands even more, if you know
    the details. The above procedure can be summarized with the
    following command:</para>
   <screen>crm configure template \
   new intranet apache params \
   configfile="/etc/apache2/httpd.conf" ip="192.168.1.101"</screen>
   <para>However, the previous command only creates its configuration
    from the template. It does not apply nor commit it to the CIB.</para>
  </sect2>

  <sect2 id="sec.ha.manual_config.shadowconfig">
   <title>Testing with Shadow Configuration</title>
   <note>
    <title>For Experienced Administrators Only</title>
    <para> Although the concept is easy, it is nevertheless recommended
     to use shadow configuration only when you really need them, and if
     you are experienced with &ha;. </para>
   </note>

   <para> A shadow configuration is used to test different configuration
    scenarios. If you have created several shadow configurations, you
    can test them one by one to see the effects of your changes. </para>

   <para>The usual process looks like this: </para>

   <procedure>
    <step>
     <para>Open a shell and become &rootuser;.</para>
    </step>
    <step>
     <para>Start the crm shell with the following command:</para>
     <screen><command>crm</command> configure</screen>
    </step>
    <step>
     <para>If you figure out that your changes are risky or you 
      want to apply them later, save them into a new shadow configuration: </para>
     <screen>crm(live)configure# <command>cib</command> new myNewConfig
INFO: myNewConfig shadow CIB created
crm(myNewConfig)configure# <command>commit</command></screen>
    </step>
    <step>
     <para>Make your changes as usual. After you have created the
      shadow configuration, all changes go there.</para>
    </step>
    <step>
     <para>If you need the live cluster configuration again, switch back 
      with the following command: </para>
     <screen>crm(myNewConfig)configure# <command>cib</command> use
crm(live)configure#</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.manual_config.debugging">
  <title>Debugging Your Configuration Changes</title>

  <para>
   Before loading the changes back into the cluster, it is recommended to
   view your changes with <command>ptest</command>. The
   <command>ptest</command> can show a diagram of actions which would be
   induced by the changes to be committed. You need the
   <systemitem>graphviz</systemitem> package to display the diagrams. The
   following example is a transcript, adding a monitor operation:
  </para>

<screen># <command>crm</command> configure
crm(live)configure# <command>show</command> fence-node2 
primitive fence-node2 stonith:apcsmart \
        params hostlist="node2"
crm(live)configure# <command>monitor</command> fence-node2 120m:60s
crm(live)configure# <command>show</command> changed
primitive fence-node2 stonith:apcsmart \
        params hostlist="node2" \
        op monitor interval="120m" timeout="60s"
crm(live)configure# <emphasis role="strong">ptest</emphasis>
crm(live)configure# commit</screen>
 </sect2>
 
 </sect1>
 
 <!--<sect1 id="sec.ha.manual_config.adminrights">
  <title>Setting Multilevel Administration Rights</title>
  <remark>FATE#304867 Multilevel administration rights for CIB</remark>
  <para></para>
 </sect1>-->
 
 <sect1 id="sec.ha.config.crm.global">
  <title>Configuring Global Cluster Options</title>
  <para>Global cluster options control how the cluster behaves when
   confronted with certain situations. They can be viewed and modified
   with the <command>crm</command> tool. The predefined values can be
   kept in most cases. However, to make key functions of your cluster
   work correctly, you need to adjust the following parameters after
   basic cluster setup:</para>
  <itemizedlist>
   <listitem>
    <para>
     <xref linkend="sec.ha.configuration.basics.global.quorum"
      xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec.ha.configuration.basics.global.stonith"
      xrefstyle="select:title"/>
    </para>
   </listitem>
  </itemizedlist>
  
  <procedure>
   <title>Modifying Global Cluster Options With <command>crm</command></title>
   <step>
    <para>Open a shell and become &rootuser;.</para>
   </step>
   <step>
    <para>Enter <command>crm</command> <option>configure</option> to
     open the internal shell.</para>
   </step>
   <step>
    <para>Use the following commands to set the options for a two-node
     clusters only:</para>
    <screen>crm(live)configure# <command>property</command> no-quorum-policy=ignore
crm(live)configure# <command>property</command> stonith-enabled=false</screen>
   </step>
   <step>
    <para>Show your changes:</para>
    <screen>crm(live)configure# <command>show</command>
property $id="cib-bootstrap-options" \
   dc-version="1.1.1-530add2a3721a0ecccb24660a97dbfdaa3e68f51" \
   cluster-infrastructure="openais" \
   expected-quorum-votes="2" \
   no-quorum-policy="ignore" \
   stonith-enabled="false"</screen>
   </step>
   <step>
    <para>Commit your changes and exit:</para>
    <screen>crm(live)configure# <command>commit</command>
crm(live)configure# <command>exit</command></screen>
   </step>
  </procedure>
  
 </sect1>
 
 <sect1 id="sec.ha.config.crm.resources">
  <title>Configuring Cluster Resources</title>
  <para>As a cluster administrator, you need to create cluster resources
   for every resource or application you run on servers in your cluster.
   Cluster resources can include Web sites, e-mail servers, databases,
   file systems, virtual machines, and any other server-based
   applications or services you want to make available to users at all
   times.</para>
  <para>For an overview of resource types you can create, refer to
   <xref linkend="sec.ha.configuration.basics.resources.types"/>.</para>
  
  <sect2 id="sec.ha.manual_config.create">
   <title>Creating Cluster Resources</title>
   <para> There are three types of RAs (Resource Agents) available with the
    cluster (for background information, see <xref
     linkend="sec.ha.configuration.basics.raclasses"/>). To create a cluster
    resource use the <command>crm</command> tool. To add a new resource to the
    cluster, the general procedure is as follows: </para>
   
   <procedure id="proc.ha.manual_config.create">
    <step>
     <para>
      Open a shell and become &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm</command> <option>configure</option> to
      open the internal shell
     </para>
    </step>
    <step>
     <para>
      Configure a primitive IP address:
     </para>
     <screen>crm(live)configure# <command>primitive</command> myIP ocf:heartbeat:IPaddr \
     params ip=127.0.0.99 op monitor interval=60s</screen>
     <para>
      The previous command configures a <quote>primitive</quote> with the
      name <literal>myIP</literal>. You need the class (here
      <literal>ocf</literal>), provider (<literal>heartbeat</literal>), and
      type (<literal>IPaddr</literal>). Furthermore this primitive expects
      some parameters like the IP address. You have to change the address to
      your setup.
     </para>
    </step>
    <step>
     <para>
      Display and review the changes you have made:
     </para>
     <screen>crm(live)configure# <command>show</command></screen>
    </step>
    <step>
     <para>
      Commit your changes to take effect:
     </para>
     <screen>crm(live)configure# <command>commit</command></screen>
    </step>
   </procedure>
   
   <!--<sect2 id="sec.ha.manual_config.lsb">
    <title>LSB Initialization Scripts</title>
    <para>
    All LSB scripts are commonly found in the directory
    <filename>/etc/init.d</filename>. They must have several actions
    implemented, which are at least <literal>start</literal>,
    <literal>stop</literal>, <literal>restart</literal>,
    <literal>reload</literal>, <literal>force-reload</literal>, and
    <literal>status</literal> as explained in
    <ulink
    url="http://www.linux-foundation.org/spec/refspecs/LSB_1.3.0/gLSB/gLSB/iniscrptact.html"/>.
    </para>
    
    </sect2>-->
  </sect2>
  <sect2 id="sec.ha.manual_config.example">
   <title>Example Configuration for an NFS Server</title>
   <para>
    To set up the NFS server, the following steps are needed:
   </para>
   
   <procedure>
    <step>
     <para>Configure DRBD.</para>
    </step>
    <step>
     <para>Setting up a file system Resource.</para>
    </step>
    <step>
     <para>Setting up the NFS server and configure the IP address.</para>
    </step>
   </procedure>
   
   <para>The following subsection shows you how to do it.</para>
   
   <sect3 id="sec.ha.manual_config.example.drbd">
    <title>Configuring DRBD</title>
    <!-- See also DocComment#8052 -->
    <para>
     Before starting with the DRBD &ha; configuration, set up a
     DRBD device manually. Basically this is configuring
     DRBD and letting it synchronize. The exact procedure is described 
     in <xref linkend="cha.ha.drbd"/>.
     For now, assume that you configured a resource <literal>r0</literal>
     that may be accessed at the device <filename>/dev/drbd_r0</filename> on
     both of your cluster nodes.
    </para>
    <para>
     The DRBD resource is an OCF master slave resource.
     This can be found in the description of the metadata of the DRBD
     resource agent. However, more important is that there are the actions
     <literal>promote</literal> and <literal>demote</literal> in the
     <literal>actions</literal> section of the metadata. These are mandatory
     for master slave resources and commonly not available to other
     resources.
    </para>
    <para>
     For &ha;, master slave resources may have multiple masters on different
     nodes. It is even possible to have a master and slave on the same node.
     Therefore, configure this resource in a way that there is exactly one
     master and one slave, each running on different nodes. Do this with the
     meta attributes of the <literal>master</literal> resource. Master slave
     resources are special kinds of clone resources in &ha;. Every master
     and every slave counts as a clone.
    </para>
    <para>
     Use the following commands in the <command>crm</command> shell to
     configure a DRBD resource:
    </para>

    <procedure>
     <step>
      <para>Open a shell and become &rootuser;.</para>
     </step>
     <step>
      <para>Enter <command>crm</command> <option>configure</option> to
       open the internal shell.</para>
     </step>
     <step>
      <para>If you have a two node cluster, set the following
       properties:</para>
      <screen>crm(live)configure# <command>property</command> no-quorum-policy=ignore
crm(live)configure# <command>property</command> stonith-enabled=false</screen>
      <remark>toms 2010-03-01: The property globally-unique is not
       available anymore. Should I ignore it or replace it with something
       else?</remark>
     </step>
     <step>
      <para>Create a primitive DRBD resource:</para>
      <screen>crm(live)configure# <command
 >primitive</command> drbd_r0 ocf:linbit:drbd params \
 drbd drbd_resource=r0 op monitor interval="30s"</screen>
     </step>
     <step>
      <para>Create a master/slave resource:</para>
      <screen>crm(live)configure# <command
>ms</command> ms_drbd_r0 res_drbd_r0 meta master-max=1 \
 master-node-max=1 clone-max=2 clone-node-max=1 notify=true</screen>
     </step>
     <step>
      <para>Specify an colocation and order constraint:</para>
      <screen>crm(live)configure# <command
>colocation</command> fs_on_drbd_r0 inf: res_fs_r0 ms_drbd_r0:Master
crm(live)configure# <command
>order</command> fs_after_drbd_r0 inf: ms_drbd_r0:promote<!--
      --> res_fs_r0:start</screen>
     </step>
     <step>
      <para>Display your changes with the <command>show</command>
       command.</para>
     </step>
     <step>
      <para>Commit your changes with the <command>commit</command>
       command.</para>
     </step>
    </procedure>
    
    <!--<screen>crm(live)# <command>configure</command>
     crm(live)configure# <command
     >primitive</command> drbd_r0 ocf:linbit:drbd params \
     drbd drbd_resource=r0 op monitor interval="30s"
     crm(live)configure# <command>ms</command> drbd_resource drbd_r0 \
     meta clone_max=2 clone_node_max=1 master_max=1 master_node_max=1 notify=true
     crm(live)configure# <command>commit</command></screen>-->
   </sect3>
   
   <sect3 id="sec.ha.manual_config.example.filesystem">
    <title>Setting Up a File System Resource</title>
    <para>
     The <literal>filesystem</literal> resource is configured as an OCF
     primitive resource with DRBD. It has the task of mounting and unmounting a device
     to a directory on start and stop requests. In this case, the device is
     <filename>/dev/drbd_r0</filename> and the directory to use as mount point
     is <filename>/srv/failover</filename>. The file system used is
     <literal>xfs</literal>.
    </para>
    <para>
     Use the following commands in the <command>crm</command> shell to
     configure a filesystem resource:
    </para>
    <!-- 
     According to Philip Reiser (Linbit/DRBD inventor)
     ocf:heartbeat:Filesystem is unmaintained, therefor we have to use
     ocf:linbit:drbd
    -->
    <screen>crm(live)# <command>configure</command>
crm(live)configure# <command>primitive</command> filesystem_resource \
    ocf:linbit:drbd \
    params device=/dev/drbd_r0 directory=/srv/failover fstype=xfs</screen>
   </sect3>
   <sect3 id="sec.ha.manual_config.example.nfs">
    <title>NFS Server and IP Address</title>
    <para>
     To make the NFS server always available at the same IP address, use an
     additional IP address as well as the ones the machines use for their
     normal operation. This IP address is then assigned to the active NFS
     server in addition to the system's IP address.
    </para>
    <para>
     The NFS server and the IP address of the NFS server should always be
     active on the same machine. In this case, the start sequence is not
     very important. They may even be started at the same time. These are
     the typical requirements for a group resource.
    </para>
    <para>
     Before starting the &ha; RA configuration, configure the NFS server
     with &yast;. Do not let the system start the NFS server. Just set up
     the configuration file. If you want to do that manually, see the manual
     page exports(5) (<command>man 5 exports</command>). The configuration
     file is <filename>/etc/exports</filename>. The NFS server is configured
     as an LSB resource.
    </para>
    <para>
     Configure the IP address completely with the &ha; RA configuration. No
     additional modification is necessary in the system. The IP address RA
     is an OCF RA.
    </para>
    <screen>crm(live)# <command>configure</command>
crm(live)configure# <command>primitive</command> nfs_resource ocf:nfsserver \
     params nfs_ip=10.10.0.1  nfs_shared_infodir=/shared
crm(live)configure# <command>primitive</command> ip_resource ocf:heartbeat:IPaddr \
     params ip=10.10.0.1
crm(live)configure# <command>group</command> nfs_group nfs_resource ip_resource
crm(live)configure# <command>show</command>
primitive ip_res ocf:heartbeat:IPaddr \
        params ip="192.168.1.10"
primitive nfs_res ocf:heartbeat:nfsserver \
        params nfs_ip="192.168.1.10" nfs_shared_infodir="/shared"
group nfs_group nfs_res ip_res
crm(live)configure# <command>commit</command>
crm(live)configure# <command>end</command>
crm(live)# <command>quit</command></screen>
   </sect3>
  </sect2>
  <sect2 id="sec.ha.manual_create.stonith">
   <title>Creating a &stonith; Resource</title>
   
   <para>
    From the <command>crm</command> perspective, a &stonith; device is just
    another resource. To create a &stonith; resource, proceed as follows:
   </para>
   
   <procedure>
    <step>
     <para>Open a shell and become &rootuser;.
     </para>
    </step>
    <step>
     <para>Enter <command>crm</command> to open the internal shell.</para>
    </step>
    <step>
     <para>
      Get a list of all &stonith; types with the following command:
     </para>
     <screen>crm(live)# <command>ra</command> list stonith
apcmaster               apcsmart                baytech
cyclades                drac3                   external/drac5
external/hmchttp        external/ibmrsa         external/ibmrsa-telnet
external/ipmi           external/kdumpcheck     external/rackpdu
external/riloe          external/sbd            external/ssh
external/vmware         external/xen0           external/xen0-ha
ibmhmc                  ipmilan                 meatware
null                    nw_rpc100s              rcd_serial
rps10                   ssh                     suicide</screen>
    </step>
    <step id="st.ha.manual_create.stonith.type">
     <para>
      Choose a &stonith; type from the above list and view the list of
      possible options. Use the following command:
     </para>
     <screen>crm(live)# <command>ra</command> info stonith:external/ipmi
IPMI STONITH external device (stonith:external/ipmi)

ipmitool based power management. Apparently, the power off
method of ipmitool is intercepted by ACPI which then makes
a regular shutdown. If case of a split brain on a two-node
it may happen that no node survives. For two-node clusters
use only the reset method.

Parameters (* denotes required, [] the default):

hostname (string): Hostname
    The name of the host to be managed by this STONITH device.
...<!--
ipaddr (string): IP Address
    The IP address of the STONITH device.

userid (string): Login
    The username used for logging in to the STONITH device.

passwd (string): Password
    The password used for logging in to the STONITH device.

interface (string, [lan]): IPMI interface
    IPMI interface to use, such as "lan" or "lanplus".

stonith-timeout (time, [60s]):
    How long to wait for the STONITH action to complete. Overrides the stonith-timeout cluster property

priority (integer, [0]):
    The priority of the stonith resource. The lower the number, the higher the priority.

Operations' defaults (advisory minimum):

    start         timeout=15
    stop          timeout=15
    status        timeout=15
    monitor_0     interval=15 timeout=15 start-delay=15--></screen>
    </step>
    <step>
     <para>
      Create the &stonith; resource with the class
      <literal>stonith</literal>, the type you have chosen in
      <xref linkend="st.ha.manual_create.stonith.type" xrefstyle="select:label nopage"
      />,
      and the respective parameters if needed, for example:
     </para>
     <screen>crm(live)# <command>configure</command>
crm(live)configure# <command>primitive</command> my-stonith stonith:external/ipmi \
    params hostname="node1"
    ipaddr="&subnetI;.221" \
    userid="admin" passwd="secret" \
    op monitor interval=60m timeout=120s  </screen>
    </step>
   </procedure>
  </sect2>
  <sect2 id="sec.ha.manual_config.constraints">
   <title>Configuring Resource Constraints</title>
   
   <para>
    Having all the resources configured is only one part of the job. Even if
    the cluster knows all needed resources, it might still not be able to
    handle them correctly. For example, it would not make sense to try to
    mount the file system on the slave node of drbd (in fact, this would fail
    with drbd). To inform the cluster about these things, define constraints.
   </para>
   
   <para>For more information about constraints, see <xref
    linkend="sec.ha.configuration.basics.constraints"/>. </para>
   
   
   <sect3 id="sec.ha.manual_config.constraints.locational">
    <title>Locational Constraints</title>
    <para>
     This type of constraint may be added multiple times for each resource.
     All <literal>location</literal> constraints are evaluated for a
     given resource. A simple example that expresses a preference to run a
     resource with the ID <literal>fs1-loc</literal> on the node with the
     name <literal>&exampleclient;</literal> to 100 would be the following:
    </para>
    <!-- 
     location ID RSC {node_pref|rules}
     
     Grammar:
     node_pref :: <score>: <node>
     
     rules ::
     rule [id_spec] [$role=<role>] <score>: <expression>
     [rule [id_spec] [$role=<role>] <score>: <expression> ...]
     
     id_spec :: $id=<id> | $id-ref=<id>
     score :: <number> | <attribute> | [-]inf
     expression :: <single_exp> [bool_op <simple_exp> ...]
     | <date_expr>
     bool_op :: or | and
     single_exp :: <attribute> [type:]<binary_op> <value>
     | <unary_op> <attribute>
     type :: string | version | number
     binary_op :: lt | gt | lte | gte | eq | ne
     unary_op :: defined | not_defined
     
     date_expr :: date_op <start> [<end>]  (TBD) 
    -->
    <screen>crm(live)configure# <command>location</command> fs1-loc fs1 100: earth</screen>
    <para>Another example is a location with pingd:</para>
    <screen>crm(live)configure# <command>primitive</command> pingd pingd \
    params name=pingd dampen=5s multiplier=100 host_list="r1 r2"
crm(live)configure#  <command>location</command> node_pref internal_www \
    rule 50: #uname eq node1 \
    rule pingd: defined pingd</screen>
     
   </sect3>
   
   <sect3 id="sec.ha.manual_config.constraints.collocational">
    <title>Collocational Constraints</title>
    <para>
     The <command>colocation</command> command is used to define what
     resources should run on the same or on different hosts. 
    </para>
    <para>
     It is only possible to set a score of either +inf or -inf,
     defining resources that must always or must never run on the same node.
     It is also possible to use non-infinite scores. In that case the
     collocation is called <emphasis>advisory</emphasis> and the cluster
     may decide not to follow them in favour of not stopping other
     resources if there is a conflict.</para>
    <para>    
     For example, to run the two resources with the IDs
     <literal>filesystem_resource</literal> and <literal>nfs_group</literal>
     always on the same host, use the following constraint:
    </para>
    <!-- 
     colocation ID SCORE: RSC[:ROLE] RSC[:ROLE]
     
     Example: colocation dummy_and_apache -inf: apache dummy
    -->
    <screen>crm(live)configure# <command>colocation</command> nfs_on_filesystem inf: nfs_group filesystem_resource</screen>
    <para>
     For a master slave configuration, it is necessary to know if the current
     node is a master in addition to running the resource locally. 
     
    </para>
   </sect3>
   
   <sect3 id="sec.ha.manual_config.constraints.ordering">
    <title>Ordering Constraints</title>
    <para>
     Sometimes it is necessary to provide an order of resource actions
     or operations. For example, you cannot mount a file system before the device is
     available to a system. Ordering constraints can be used to start or stop
     a service right before or after a different resource meets a special
     condition, such as being started, stopped, or promoted to master. Use
     the following commands in the <command>crm</command> shell to configure
     an an ordering constraint:
    </para>
    <!-- order ID score-type: FIRST-RSC[:ACTION] THEN-RSC[:ACTION] 
     
     score-type :: advisory | mandatory | <score>
     
     Example: order c_apache_1 mandatory: apache:start ip_1
    -->
    <screen>crm(live)configure# <command
 >order</command> nfs_after_filesystem mandatory: group_nfs filesystem_resource</screen>
   </sect3>
   
   <sect3 id="sec.ha.manual_config.constraints.example">
    <title>Constraints for the Example Configuration</title>
    <para>
     The example used for this chapter would not work as expected without
     additional constraints. It is essential that all resources run on the
     same machine as the master of the drbd resource. Another thing that is
     critical is that the drbd resource must be master before any other
     resource starts. Trying to mount the drbd device when drbd is not master
     simply fails. The constraints that must be fulfilled look like the
     following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The file system must always be on the same node as the master of the
       drbd resource.
      </para>
      <screen>crm(live)configure# <command>colocation</command> filesystem_on_master inf: \
    filesystem_resource drbd_resource:Master</screen>
     </listitem>
     <listitem>
      <para>
       The NFS server as well as the IP address must be on the same node as
       the file system.
      </para>
      <screen>crm(live)configure# <command>colocation</command> nfs_with_fs inf: \
   nfs_group filesystem_resource</screen>
     </listitem>
     <listitem>
      <para>
       The NFS server as well as the IP address start after the file system
       is mounted:
      </para>
      <screen>crm(live)configure# <command>order</command> nfs_second mandatory: \
   filesystem_resource:start nfs_group</screen>
     </listitem>
     <listitem>
      <para>
       The file system must be mounted on a node after the drbd resource is
       promoted to master on this node.
      </para>
      <screen>crm(live)configure# <command>order</command> drbd_first inf: \
    drbd_resource:promote filesystem_resource</screen>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>
  <sect2 id="sec.ha.manual_config.failover">
   <title>Specifying Resource Failover Nodes</title>
   
   <para>
    To determine a resource failover, use the meta attribute
    migration-threshold. For example:
   </para>
   
   <!-- Example (rsc is r1): -->
   
   <screen>crm(live)configure# <command>location</command> r1-node1 r1 100: node1</screen>
   
   <para>
    Normally r1 prefers to run on node1. If it fails there,
    migration-threshold is checked and compared to the failcount. If
    failcount >= migration-threshold then it is migrated to the node with the
    next best preference.
   </para>
   
   <para>
    Start failures set the failcount to inf depends on the
    <option>start-failure-is-fatal</option> option. Stop failures cause
    fencing. If there's no STONITH defined, then the resource will not
    migrate at all.
   </para>
   <para>For an overview, refer to <xref
    linkend="sec.ha.configuration.basics.failover"/>.</para>
  </sect2>
  <sect2 id="sec.ha.manual_config.failback">
   <title>Specifying Resource Failback Nodes (Resource Stickiness)</title>
   
   <para>
    A rsc may failback after it has been migrated due to the number of
    failures only when the administrator resets the failcount or the failures
    have been expired (see failure-timeout meta attribute).
   </para>
   
   <screen><command>crm</command> resource failcount <replaceable>RSC</replaceable> delete <replaceable>NODE</replaceable></screen>
   <para>For an overview, refer to <xref
    linkend="sec.ha.configuration.basics.failback"/>.</para>
  </sect2>
  <sect2 id="sec.ha.manual_config.utilization">
   <title>Configuring Placement of Resources Based on Load Impact</title>
   <para>Configuring Placement of Resources Based on Load Impact</para>
   <para>Not all resources are equal. Some, such as &xen; guests, require
    that the node hosting them meets their capacity requirements. If
    resources are placed such that their combined need exceed the
    provided capacity, the resources diminish in performance (or even
    fail). </para>
   <para>To take this into account, the &hasi; allows you to specify the
    following parameters: </para>
   <orderedlist>
    <listitem>
     <para>
      The capacity a certain node <emphasis>provides</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      The capacity a certain resource <emphasis>requires</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      An overall strategy for placement of resources.
     </para>
    </listitem>
   </orderedlist>
   <para>
    For detailed background information about the parameters and a
    configuration example, refer to
    <xref linkend="sec.ha.configuration.basics.utilization"/>.
   </para>
   <para>
    To configure the resource's requirements and the capacity a node
    provides, use utilization attributes as described in
    <xref linkend="pro.ha.config.gui.capacity"/>. You can name the
    utilization attributes according to your preferences and define as many
    name/value pairs as your configuration needs.
   </para>
   <para>In the following example, we assume that you already have a basic
     configuration of cluster nodes and resources and now additionally want
     to configure the capacities a certain node provides and the capacity a
     certain resource requires. <!--The procedure of adding utilization
     attributes is basically the same and only differs in
     <xref
      linkend="step.ha.config.gui.capacity.node"/> and
     <xref
      linkend="step.ha.config.gui.capacity.resource"/>.--></para>
   <procedure>
    <title>Adding Or Modifying Utilization Attributes With <command>crm</command></title>
    <step>
     <para>Start the <command>crm</command> shell with the following
     command:</para>
     <screen><command>crm</command> configure</screen>
    </step>
    <step>
     <para>To specify the capacity a node <emphasis>provides</emphasis>,
      use the following command and replace the placeholder
      <replaceable>NODE_1</replaceable> with the name of your node:</para>
     <screen>crm(live)configure# <command>node</command><!--
     --> <replaceable>NODE_1</replaceable> utilization memory=16384<!--
      --> cpu=8</screen>
     <para>With these values, <replaceable>NODE_1</replaceable> would be
      assumed to provide 16GB of memory and 8 CPU cores to resources.</para>
    </step>
    <step>
     <para>To specify the capacity a resource
      <emphasis>requires</emphasis>, use:</para>
     <screen>crm(live)configure# <command>primitive</command><!--
     --> xen1 ocf:heartbeat:Xen ... \
     utilization memory=4096 cpu=4</screen>
     <para>This would make the resource consume 4096 of those memory
      units from nodeA, and 4 of the cpu units.</para>
    </step>
    <step>
     <para>Configure the placement strategy with the
      <command>property</command> command:</para>
     <screen>crm(live)configure# <command>property</command> ...</screen>
      <para>Four values are available for the placement strategy:</para>
     <variablelist>
      <varlistentry>
       <term><command>property</command> <option>placement-strategy=default</option></term>
       <listitem>
        <para>Utilization values are not taken into account at all; the
         default. Resources are allocated according to location scoring;
         if scores are equal, resources are evenly distributed across
         nodes.</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><command>property</command> <option>placement-strategy=utilization</option></term>
       <listitem>
        <para>Utilization values are taken into account when deciding
         whether a node is considered eligible if it has sufficient free
         capacity to satisfy the resource's requirements. However,
         load-balancing is still done based on number of resources
         allocated to a node.</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><command>property</command> <option>placement-strategy=minimal</option></term>
       <listitem>
        <para>Utilization values are taken into account when deciding
         whether a node is eligible to serve a resource; an attempt is
         made to concentrate the resources on as few nodes as possible,
         thereby enabling possible power savings on the remaining
         nodes.</para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><command>property</command> <option>placement-strategy=balanced</option></term>
       <listitem>
        <para>Utilization values are taken into account when deciding
         whether a node is eligible to serve a resource; an attempt is
         made to spread the resources evenly, optimizing resource
         performance. </para>
        <para> The placing strategies are best-effort, and do not yet
         utilize complex heuristic solvers to always reach an optimum
         allocation result. Ensure that resource priorities are properly
         set so that your most important resources are scheduled
         first.</para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>Commit your changes before leaving the crm shell:</para>
     <screen>crm(live)configure# <command>commit</command></screen>
    </step>
   </procedure>
   
   <para>The following example demonstrates a three node cluster of
   equal nodes, with 4 virtual machines:</para>
   <screen>crm(live)configure# <command>node</command> node1 utilization memory="4000"
crm(live)configure# <command>node</command> node2 utilization memory="4000"
crm(live)configure# <command>node</command> node3 utilization memory="4000"
crm(live)configure# <command>primitive</command> xenA ocf:heartbeat:Xen \
    utilization memory="3500" meta priority="10"
crm(live)configure# <command>primitive</command> xenB ocf:heartbeat:Xen \
    utilization memory="2000" meta priority="1"
crm(live)configure# <command>primitive</command> xenC ocf:heartbeat:Xen \
    utilization memory="2000" meta priority="1"
crm(live)configure# <command>primitive</command> xenD ocf:heartbeat:Xen \
    utilization memory="1000" meta priority="5"
crm(live)configure# <command>property</command> placement-strategy="minimal"</screen>
   <para>With all three nodes up, xenA will be placed onto a node first,
    followed by xenD. xenB and xenC would either be allocated together
    or one of them with xenD.</para>
   <para>If one node failed, too little total memory would be available
    to host them all. xenA would be ensured to be allocated, as would
    xenD; however, only one of xenB or xenC could still be placed, and
    since their priority is equal, the result is not defined yet. To
    resolve this ambiguity as well, you would need to set a higher
    priority for either one.</para>
  </sect2>
  
  <sect2 id="sec.ha.manual_config.monitor">
   <title>Configuring Resource Monitoring</title>
   <!--<remark>Add the new monitor command: $ crm configure monitor usage: monitor &lt;rsc>[:&lt;role>]
    &lt;interval>[:&lt;timeout>] </remark>-->
   
   <para>
    To monitor a resource, there are two possibilities: either define monitor
    operation with the <command>op</command> keyword or use the
    <command>monitor</command> command. The following example configures an
    Apache resource and monitors it for every 30 minutes with the
    <literal>op</literal> keyword:
   </para>
   
   <screen>crm(live)configure# <command>primitive</command> apache apache \
  params ... \
  <emphasis>op monitor interval=60s timeout=30s</emphasis></screen>
   
   <para>
    The same can be done with:
   </para>
   
   <screen>crm(live)configure# <command>primitive</command> apache apache \
   params ...
crm(live)configure# <command>monitor</command> apache 60s:30s</screen>
   <para>For an overview, refer to <xref
    linkend="sec.ha.configuration.basics.monitoring"/>.</para>
  </sect2>
  <sect2 id="sec.ha.manual_config.group">
   <title>Configuring a Cluster Resource Group</title>
   <!--Text mostly copied from "Advanced Configuration"-->
   
   <para>
    One of the most common elements of a cluster is a set of resources that
    needs to be located together. Start sequentially and stop in the reverse
    order. To simplify this configuration we support the concept of groups.
    The following example creates two primitives (an IP address and an email
    resource):
   </para>
   
   <procedure>
    <step>
     <para>
      Run the <command>crm</command> command as system administrator. The
      prompt changes to <literal>crm(live)</literal>.
     </para>
    </step>
    <step>
     <para>
      Configure the primitives:
     </para>
     <screen>crm(live)# <command>configure</command>
crm(live)configure# <command>primitive</command> Public-IP ocf:IPaddr:heartbeat \
   params ip=1.2.3.4
crm(live)configure# <command>primitive</command> Email lsb:exim</screen>
    </step>
    <step>
     <para>
      Group the primitives with their relevant identifiers in the correct
      order:
     </para>
     <screen>crm(live)configure# <command>group</command> shortcut Public-IP Email</screen>
    </step>
   </procedure>
   <para>For an overview, refer to <xref
    linkend="sec.ha.configuration.basics.resources.advanced.groups"/>.</para>
  </sect2>
  <sect2 id="sec.ha.manual_config.clone">
   <title>Configuring a Clone Resource</title>
   <!-- Text mostly copied from "Advanced Configuration" -->
   
   <para>
    Clones were initially conceived as a convenient way to start N instances
    of an IP resource and have them distributed throughout the cluster for
    load balancing. They have turned out to quite useful for a number of
    other purposes, including integrating with DLM, the fencing subsystem and
    OCFS2. You can clone any resource, provided the resource agent supports
    it.
   </para>
   
   <para>Learn more about cloned resources in <xref 
    linkend="sec.ha.configuration.basics.resources.advanced.clones"/>.
   </para>
   
   
   <sect3 id="sec.ha.manual_config.clone.anonymous">
    <title>Creating Anonymous Clone Resources</title>
    <para>
     To create an anonymouse clone resource, first create a primitive
     resource and then refer to it with the <command>clone</command> command.
     Do the following:
    </para>
    <procedure>
     <step>
      <para>Open a shell and become &rootuser;.</para>
     </step>
     <step>
      <para>Enter <command>crm</command> <option>configure</option> 
       to open the internal shell.</para>
     </step>
     <step>
      <para>
       Configure the primitive, for example:
      </para>
      <screen>crm(live)configure# <command>primitive</command> Apache lsb:apache</screen>
     </step>
     <step>
      <para>
       Clone the primitive:
      </para>
      <screen>crm(live)configure# clone apache-clone Apache \
   meta globally-unique=false</screen>
     </step>
    </procedure>
   </sect3>
    
   <!-- 
    <sect2 id="sec.ha.manual_config.clone.globally" os="notdefinied">
    <title>Creating Globally Unique Clone Resources</title>
    <remark>Haven't found an example. Any idea?</remark>
    <para> FIXME </para>
    </sect2>
   -->
   
   <sect3 id="sec.ha.manual_config.clone.stateful">
    <title>Creating Stateful/Multi-State Clone Resources</title>
    <para>
     To create an stateful clone resource, first create a primitive resource
     and then the master-slave resource.
    </para>
    <procedure>
     <step>
      <para>Open a shell and become &rootuser;.</para>
     </step>
     <step>
      <para>Enter <command>crm</command> <option>configure</option> 
       to open the internal shell.</para>
     </step>
     <step>
      <para>
       Configure the primitive. Change the intervals if needed:
      </para>
      <screen>crm(live)configure# <command>primitive</command> myRSC ocf:myCorp:myAppl \
    op monitor interval=60 \
    op monitor interval=61 role=Master</screen>
     </step>
     <step>
      <para>
       Create the master slave resource:
      </para>
      <screen>crm(live)configure# <command>clone</command> apache-clone Apache \
   meta globally-unique=false</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>
 
 <sect1 id="sec.ha.config.crm">
  <title>Managing Cluster Resources</title>
  <para>Apart from the possibility to configure your cluster resources,
   the <command>crm</command> tool also allows you to manage existing
   resources. The following subsections describe gives you an overview.</para>
  
  <sect2 id="sec.ha.manual_config.start">
   <title>Starting a New Cluster Resource</title>
   
   <para>
    To start a new cluster resource you need the respective identifier.
    Proceed as follows:
   </para>
   
   <procedure>
    <step>
     <para>Open a shell to become &rootuser;.</para>
    </step>
    <step>
     <para>Enter <command>crm</command> to open the internal shell.</para>
    </step>
    <step>
     <para>
      Search for the respective resource with the command
      <command>status</command> or use the tab completion mechanism fo
      find out about your resource IDs.
     </para>
    </step>
    <step>
     <para>
      Start the resource with:
     </para>
     <screen>crm(live)# <command>resource</command> start <replaceable>ID</replaceable></screen>
    </step>
   </procedure>
  </sect2>
  <sect2 id="sec.ha.manual_config.cleanup">
   <title>Cleaning Up Resources</title>
   <para>A resource will be automatically restarted if it fails, but
    each failure raises the resource's failcount. If a
     <literal>migration-threshold</literal> has been set for that
    resource, the node will no longer be allowed to run the resource as
    soon as the number of failures has reached the migration threshold.
   </para>
   <para></para>
   <remark>toms 2010-03-01: I'm not sure if the following procedure is
   correct in regard to bnc#520718.</remark>
   <remark>toms 2010-03-01: Couldn't reproduce as there was an error in
    crm. TODO: Insert the commands.</remark>
   <procedure>
    <step>
     <para>Clean up your corresponding resources on all nodes.</para>
    </step>
    <step>
     <para>If the resource is running, it has to be stopped first.
     </para>
    </step>
    <!--<step>
     <para>Remove all constraints that relate to this resource.</para>
    </step>-->
    <step>
     <para>Delete the resource itself:</para>
     <screen>crm configure delete <replaceable>ID</replaceable></screen>
    </step>
   </procedure>
  </sect2>
  
  <sect2 id="sec.ha.manual_config.remove">
   <title>Removing a Cluster Resource</title>
   <para>
    To remove a cluster resource you need the relevant identifier. Proceed as
    follows:
   </para>
   <procedure>
    <step>
     <para>Open a shell and become &rootuser;.</para>
    </step>
    <step>
     <para>
      Run the following command to get a list of your resources:
     </para>
     <screen>crm(live)# <command>resource</command> status</screen>
     <para>
      For example, the output can look like this (whereas myIP is the
      relevant identifier of your resource):
     </para>
     <screen>myIP    (ocf::IPaddr:heartbeat) ...</screen>
    </step>
    <step>
     <para>
      Delete the resource with the relevant identifier (which implies a
      <command>commit</command> too):
     </para>
     <screen>crm(live)# <command>configure</command> delete <replaceable>YOUR_ID</replaceable></screen>
    </step>
    <step>
     <para>
      Commit the changes:
     </para>
     <screen>crm(live)# <command>configure</command> commit</screen>
    </step>
   </procedure>
  </sect2>
  
  <sect2 id="sec.ha.manual_config.migrate">
   <title>Migrating a Cluster Resource</title>
   
   <para>
    Although resources are configured to automatically fail over (or migrate)
    to other nodes of the cluster in the event of a hardware or software
    failure, you can also manually migrate a resource to another node in the
    cluster using either the &hbgui; or the command line.
   </para>
   
   <procedure>
    <step>
     <para>Open a shell to become &rootuser;.</para>
    </step>
    <step>
     <para>Enter <command>crm</command> to open the internal shell.</para>
    </step>
    <step>
     <para>
      To migrate a resource named <literal>ipaddress1</literal> to a cluster
      node named <literal>node2</literal>, enter:
     </para>
     <screen>crm(live)# <command>resource</command>
crm(live)resource# <command>migrate</command> ipaddress1 node2</screen>
    </step>
   </procedure>
  </sect2>
  
 </sect1>
 
</chapter>
