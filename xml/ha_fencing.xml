<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ha.fencing">
 <title>Fencing and &stonith;</title>
<!-- http://hg.clusterlabs.org/pacemaker/stable-1.0/file/tip/doc/crm_fencing.txt -->
 <abstract>
 
  <para>
   Fencing is a very important concept in computer clusters for HA (High
   Availability). A cluster sometimes detects that one of the nodes is
   behaving strangely and needs to remove it. This is called
   <emphasis>fencing</emphasis> and is commonly done with a &stonith;
   resource. Fencing may be defined as a method to bring an HA cluster to a
   known state.
  </para>

  <para>
   Every resource in a cluster has a state attached. For example:
   <quote>resource r1 is started on &node1;</quote>. In an HA cluster, such a
   state implies that <quote>resource r1 is stopped on all nodes except
   &node1;</quote>, because the cluster must make sure that every resource
   may be started on only one node. Every node must report every change that
   happens to a resource. The cluster state is thus a collection of resource
   states and node states.
  </para>

  <para>
   When the state of a node or resource cannot be established with
   certainty, fencing comes in. Even when the cluster is not aware of what
   is happening on a given node, fencing can ensure that the node does not
   run any important resources.
  </para>
 </abstract>
 <remark>taroth 2014-08-14: todo : https://fate.suse.com/316529: Improve
  modularity of fence-agent packages (prio: important) -> check if that is going
  to be the only change and have a look at the fencing chapter what packages are
  mentioned there.</remark>
 <remark>taroth 2014-08-14: todo - check doc impact of
  https://fate.suse.com/312345: Resource Agent needed to support SCSI
  reservations and https://fate.suse.com/316476: Support fencing via SCSI
  reservations</remark>
 <sect1 id="sec.ha.fencing.classes">
  <title>Classes of Fencing</title>

  <para>
   There are two classes of fencing: resource level and node level fencing.
   The latter is the primary subject of this chapter.
  </para>

  <variablelist>
   <varlistentry>
    <term>Resource Level Fencing</term>
    <listitem>
     <para>
      Using resource level fencing the cluster can ensure that a node cannot
      access one or more resources. One typical example is a SAN, where a
      fencing operation changes rules on a SAN switch to deny access from
      the node.
     </para>
     <para>
      Resource level fencing can be achieved by using normal resources on
      which the resource you want to protect depends. Such a resource would
      simply refuse to start on this node and therefore resources which
      depend on it will not run on the same node.
      <remark>emap: 2011-11-24: This
      doesn't really make sense or explain how to do resource level
      fencing, but I couldn't find a better explanation. - taroth 2011-11-26:
      will rephrase for next revision</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Node Level Fencing</term>
    <listitem>
     <para>
      Node level fencing ensures that a node does not run any resources at
      all. This is usually done in a simple if brutal way: reset or power
      off the node.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.fencing.nodes">
  <title>Node Level Fencing</title>

  <para>
   In &productnamereg;, the fencing implementation is &stonith; (Shoot The
   Other Node in the Head). It provides node level fencing. The &hasi;
   includes the <command>stonith</command> command line tool, an extensible
   interface for remotely powering down a node in the cluster. For an
   overview of the available options, run <command>stonith --help</command>
   or refer to the man page of <command>stonith</command> for more
   information.
  </para>

  <sect2 id="sec.ha.fencing.nodes.devices">
   <title>&stonith; Devices</title>
   <para>
    To use node level fencing, you first need to have a fencing device. To
    get a list of &stonith; devices which are supported by the &hasi;, run
    the following command as &rootuser; on any of the nodes:
   </para>
<screen>stonith -L</screen>
   <para>
    &stonith; devices may be classified into the following categories:
   </para>
   <variablelist>
    <varlistentry>
     <term>Power Distribution Units (PDU)</term>
     <listitem>
      <para>
       Power Distribution Units are an essential element in managing power
       capacity and functionality for critical network, server and data
       center equipment. They can provide remote load monitoring of
       connected equipment and individual outlet power control for remote
       power recycling.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Uninterruptible Power Supplies (UPS)</term>
     <listitem>
      <para>
       A stable power supply provides emergency power to connected equipment
       by supplying power from a separate source in the event of utility
       power failure.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Blade Power Control Devices</term>
     <listitem>
      <para>
       If you are running a cluster on a set of blades, then the power
       control device in the blade enclosure is the only candidate for
       fencing. Of course, this device must be capable of managing single
       blade computers.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Lights-out Devices </term>
     <listitem>
      <para>
       Lights-out devices (IBM RSA, HP iLO, Dell DRAC) are becoming
       increasingly popular and may even become standard in off-the-shelf
       computers. However, they are inferior to UPS devices, because they
       share a power supply with their host (a cluster node). If a node
       stays without power, the device supposed to control it would be 
       useless. In that case, the CRM would continue its attempts to
       fence the node indefinitely while all other resource operations would
       wait for the fencing/&stonith; operation to complete.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Testing Devices</term>
     <listitem>
      <para>
       Testing devices are used exclusively for testing purposes. They are
       usually more gentle on the hardware. Before the cluster goes into
       production, they must be replaced with real fencing devices.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The choice of the &stonith; device depends mainly on your budget and the
    kind of hardware you use.
   </para>
  </sect2>

  <sect2 id="sec.ha.fencing.nodes.implementation">
   <title>&stonith; Implementation</title>
   <para>
    The &stonith; implementation of &productnamereg; consists of two
    components:
   </para>
   <variablelist>
    <varlistentry>
     <term>stonithd</term>
     <listitem>
      <para>
       stonithd is a daemon which can be accessed by local processes or over
       the network. It accepts the commands which correspond to fencing
       operations: reset, power-off, and power-on. It can also check the
       status of the fencing device.
      </para>
      <para>
       The stonithd daemon runs on every node in the CRM HA cluster. The
       stonithd instance running on the DC node receives a fencing request
       from the CRM. It is up to this and other stonithd programs to carry
       out the desired fencing operation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&stonith; Plug-ins</term>
     <listitem>
      <para>
       For every supported fencing device there is a &stonith; plug-in which
       is capable of controlling said device. A &stonith; plug-in is the
       interface to the fencing device. On each node, all &stonith; plug-ins
       reside in <filename>/usr/lib/stonith/plugins</filename> (or in
       <filename>/usr/lib64/stonith/plugins</filename> for 64-bit
       architectures). All &stonith; plug-ins look the same to stonithd, but
       are quite different on the other side reflecting the nature of the
       fencing device.
      </para>
      <para>
       Some plug-ins support more than one device. A typical example is
       <literal>ipmilan</literal> (or <literal>external/ipmi</literal>)
       which implements the IPMI protocol and can control any device which
       supports this protocol.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 id="sec.ha.fencing.config">
  <title>&stonith; Configuration</title>

  <para>
   To set up fencing, you need to configure one or more &stonith;
   resources&mdash;the stonithd daemon requires no configuration. All
   configuration is stored in the CIB. A &stonith; resource is a resource of
   class <literal>stonith</literal> (see
   <xref
    linkend="sec.ha.config.basics.raclasses"/>). &stonith;
   resources are a representation of &stonith; plug-ins in the CIB. Apart
   from the fencing operations, the &stonith; resources can be started,
   stopped and monitored, like any other resource. Starting or stopping
   &stonith; resources means loading and unloading the &stonith; device
   driver on a node. Starting and stopping are thus only administrative
   operations and do not translate to any operation on the fencing device
   itself. However, monitoring does translate to logging it to the device
   (to verify that the device will work in case it is needed). When a
   &stonith; resource fails over to another node it enables the current node
   to talk to the &stonith; device by loading the respective driver.
  </para>

  <para>
   &stonith; resources can be configured like any other resource. For
   more information about configuring resources, see
<!-- <xref
    linkend="sec.ha.configuration.stonith"/>,-->
   <xref
    linkend="sec.ha.config.hawk.stonith"/>, or
   <xref
    linkend="sec.ha.manual_create.stonith"/>.
  </para>

  <para>
   The list of parameters (attributes) depends on the respective &stonith;
   type. To view a list of parameters for a specific device, use the
   <command>stonith</command> command:
  </para>

<screen>stonith -t <replaceable>stonith-device-type</replaceable> -n</screen>

  <para>
   For example, to view the parameters for the <literal>ibmhmc</literal>
   device type, enter the following:
  </para>

<screen>stonith -t ibmhmc -n</screen>

<!--    <note>
    <para> It is easy to guess the class of a fencing device from the
     set of attribute names. </para>
   </note>-->

  <para>
   To get a short help text for the device, use the <option>-h</option>
   option:
  </para>

<screen>stonith -t <replaceable>stonith-device-type</replaceable> -h</screen>

  <sect2 id="sec.ha.fencing.config.examples">
   <title>Example &stonith; Resource Configurations</title>
   <para>
    In the following, find some example configurations written in the syntax
    of the <command>crm</command> command line tool. To apply them, put the
    sample in a text file (for example, <filename>sample.txt</filename>) and
    run:
   </para>
<screen>&prompt.root;<command>crm</command> &lt; sample.txt</screen>
   <para>
    For more information about configuring resources with the
    <command>crm</command> command line tool, refer to
    <xref
     linkend="cha.ha.manual_config"/>.
   </para>
   <warning>
    <title>Testing Configurations</title>
    <para>
     Some examples below are for demonstration and testing purposes
     only. Do not use any of the <literal>Testing Configuration</literal>
     examples in real-life cluster scenarios.
    </para>
   </warning>
   <example>
    <title>Testing Configuration</title>
    <para><remark>taroth 2014-08-14: todo -
     https://bugzilla.novell.com/show_bug.cgi?id=870963 (NEEDINFO)</remark></para>
<screen>configure
primitive st-null stonith:null \
params hostlist="&node1; &node2;"
clone fencing st-null
commit
   </screen>
   </example>
   <example>
    <title>Testing Configuration</title>
    <para>
     An alternative configuration:
    </para>
<screen>configure
 primitive st-&node1; stonith:null \
 params hostlist="&node1;"
 primitive st-&node2; stonith:null \
 params hostlist="&node2;"
 location l-st-&node1; st-&node1; -inf: &node1;
 location l-st-&node2; st-&node2; -inf: &node2;
 commit</screen>
    <para>
     This configuration example is perfectly alright as far as the cluster
     software is concerned. The only difference to a real world
     configuration is that no fencing operation takes place.
    </para>
   </example>
   <example>
    <title>Testing Configuration</title>
    <para>
     A more realistic example (but still only for testing) is the following
     external/ssh configuration: <remark>taroth 2014-08-14: todo -
      https://bugzilla.novell.com/show_bug.cgi?id=870963 (NEEDINFO)</remark>
    </para>
<screen>configure
 primitive st-ssh stonith:external/ssh \
 params hostlist="&node1; &node2;"
 clone fencing st-ssh
 commit</screen>
    <para><remark>taroth 2014-08-14: todo -
     https://bugzilla.novell.com/show_bug.cgi?id=870963 (NEEDINFO)</remark>
     This one can also reset nodes. The configuration is similar to the
     first one which features the null &stonith; device. In this example,
     clones are used. They are a CRM/Pacemaker feature. A clone is 
     a shortcut: instead of defining <literal>n</literal> identical, yet
     differently named resources, a single cloned resource suffices. By far
     the most common use of clones is with &stonith; resources, as long as
     the &stonith; device is accessible from all nodes.
    </para>
   </example>
   <example>
    <title>Configuration of an IBM RSA Lights-out Device</title>
    <para>
     The real device configuration is not much different, though some
     devices may require more attributes. An IBM RSA lights-out device might
     be configured like this:
    </para>
<screen>configure
primitive st-ibmrsa-1 stonith:external/ibmrsa-telnet \
params nodename=&node1; ipaddr=192.168.0.101 \
userid=USERID passwd=PASSW0RD
primitive st-ibmrsa-2 stonith:external/ibmrsa-telnet \
params nodename=&node2; ipaddr=192.168.0.102 \
userid=USERID passwd=PASSW0RD
location l-st-&node1; st-ibmrsa-1 -inf: &node1;
location l-st-&node2; st-ibmrsa-2 -inf: &node2;
commit</screen>
    <para>
     In this example, location constraints are used for the following
     reason: There is always a certain probability that the &stonith;
     operation is going to fail. Therefore, a &stonith; operation on the
     node which is the executioner as well is not reliable. If the node is
     reset, it cannot send the notification about the fencing operation
     outcome. The only way to do that is to assume that the operation is
     going to succeed and send the notification beforehand. But if the
     operation fails, problems could arise. Therefore, by convention,
     stonithd refuses to terminate its host.
    </para>
   </example>
   <example>
    <title>Configuration of an UPS Fencing Device</title>
    <para>
     The configuration of a UPS type fencing device is similar to the
     examples above. The details are not covered here. All UPS devices
     employ the same mechanics for fencing. How the device is accessed
     varies. Old UPS devices only had a serial port, usually connected
     at 1200baud using a special serial cable. Many new ones still have a
     serial port, but often they also use a USB or Ethernet interface. The
     kind of connection you can use depends on what the plug-in supports.
    </para>
    <para>
     For example, compare the <literal>apcmaster</literal> with the
     <literal>apcsmart</literal> device by using the <command>stonith -t
     <replaceable>stonith-device-type</replaceable> -n</command> command:
    </para>
<screen>stonith -t apcmaster -h</screen>
    <para>
     returns the following information:
    </para>
<screen>STONITH Device: apcmaster - APC MasterSwitch (via telnet)
NOTE: The APC MasterSwitch accepts only one (telnet)
connection/session a time. When one session is active,
subsequent attempts to connect to the MasterSwitch will fail.
For more information see http://www.apc.com/
List of valid parameter names for apcmaster STONITH device:
ipaddr
login
 password</screen>
    <para>
     With
    </para>
<screen>stonith -t apcsmart -h</screen>
    <para>
     you get the following output:
    </para>
<screen>STONITH Device: apcsmart - APC Smart UPS
(via serial port - NOT USB!). 
Works with higher-end APC UPSes, like
Back-UPS Pro, Smart-UPS, Matrix-UPS, etc.
(Smart-UPS may have to be >= Smart-UPS 700?).
See http://www.networkupstools.org/protocols/apcsmart.html
for protocol compatibility details.
For more information see http://www.apc.com/
List of valid parameter names for apcsmart STONITH device:
ttydev
hostlist</screen>
    <para>
     The first plug-in supports APC UPS with a network port and telnet
     protocol. The second plug-in uses the APC SMART protocol over the
     serial line, which is supported by many different APC UPS product
     lines.
    </para>
   </example>
  </sect2>

  <sect2 id="sec.ha.fencing.config.parameters">
   <title>Constraints Versus Clones</title>
   <para><remark>taroth 2014-08-14: todo -
    https://bugzilla.novell.com/show_bug.cgi?id=870963 (NEEDINFO)</remark>
    As explained in <xref linkend="sec.ha.fencing.config.examples"/>, there
    are several ways to configure a &stonith; resource: using constraints,
    clones, or both. The choice of which construct to use for configuration
    depends on several factors: nature of the fencing device, number of
    hosts managed by the device, number of cluster nodes, or personal
    preference.
   </para>
   <para><remark>taroth 2014-08-14: todo -
    https://bugzilla.novell.com/show_bug.cgi?id=870963 (NEEDINFO)</remark>
    If clones are safe to use with your configuration and they reduce the
    configuration, then use cloned &stonith; resources.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.ha.fencing.monitor">
  <title>Monitoring Fencing Devices</title>

  <para>
   Like any other resource, the &stonith; class agents also support the
   monitoring operation for checking status.
  </para>

<!--fate#310010-->

  <note>
   <title>Monitoring &stonith; Resources</title>
   <para>
    Monitor &stonith; resources regularly, yet sparingly. For most devices a
    monitoring interval of at least 1800 seconds (30 minutes) should
    suffice.
   </para>
  </note>

  <para>
   Fencing devices are an indispensable part of an HA cluster, but the less
   you need to use them, the better. Power management equipment is often
   affected by too much broadcast traffic. Some devices cannot handle more
   than ten or so connections per minute. Some get confused if two clients
   try to connect at the same time. Most cannot handle more than one session
   at a time.
  </para>

  <para>
   Checking the status of fencing devices once every few hours should usually be
   enough. The probability that a fencing operation needs to
   be performed and the power switch fails is low.
  </para>

  <para>
   For detailed information on how to configure monitor operations, refer to
<!--
   <xref linkend="pro.ha.config.gui.parameters"/> for the GUI approach or to-->
   <xref linkend="sec.ha.manual_config.monitor"/> for the command line
   approach.
  </para>
 </sect1>
 <sect1 id="sec.ha.fencing.special">
  <title>Special Fencing Devices</title>

  <para>
   In addition to plug-ins which handle real &stonith; devices, there are
   special purpose &stonith; plug-ins.
  </para>

  <warning>
   <title>For Testing Only</title>
   <para>
    Some of the &stonith; plug-ins mentioned below are for demonstration and
    testing purposes only. Do not use any of the following devices in
    real-life scenarios because this may lead to data corruption and
    unpredictable results:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>external/ssh </literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>ssh </literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>null</literal>
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <variablelist>
   <varlistentry>
    <term><literal>external/kdumpcheck</literal>
    </term>
    <listitem>
     <para>
      This plug-in checks if a Kernel dump is in progress on a node. If so,
      it returns <literal>true</literal>, and acts as if the node has been
      fenced. The node cannot run any resources during the dump anyway. This
      avoids fencing a node that is already down but doing a dump, which
      takes some time. The plug-in must be used in concert with another,
      real &stonith; device. For more details, see
      <filename>/usr/share/doc/packages/cluster-glue/README_kdumpcheck.txt</filename>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>external/sbd</literal>
    </term>
    <listitem>
     <para>
      This is a self-fencing device. It reacts to a so-called <quote>poison
      pill</quote> which can be inserted into a shared disk. On
      shared-storage connection loss, it stops the node from operating.
      Learn how to use this &stonith; agent to implement storage-based
      fencing in <xref linkend="cha.ha.storage.protect"/>. See also
      <ulink
       url="http://www.linux-ha.org/wiki/SBD_Fencing"/> for
      more details.
     </para>
<!--latest feedback by Fabian Herrschel/lmb, also ack'ed by emap -->
     <important>
      <title><literal>external/sbd</literal> and DRBD</title>
      <para>
       The <literal>external/sbd</literal> fencing mechanism requires that
       the SBD partition is readable directly from each node. Thus, a DRBD*
       device must not be used for an SBD partition.
      </para>
      <para>
       However, you can use the fencing mechanism for a DRBD cluster,
       provided the SBD partition is located on a shared disk that is not
       mirrored or replicated.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>external/ssh</literal>
    </term>
    <listitem>
     <para>
      Another software-based <quote>fencing</quote> mechanism. The nodes
      must be able to log in to each other as &rootuser; without passwords.
      It takes a single parameter, <literal>hostlist</literal>, specifying
      the nodes that it will target. As it is not able to reset a truly
      failed node, it must not be used for real-life clusters&mdash;for
      testing and demonstration purposes only. Using it for shared storage
      would result in data corruption.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>meatware</literal>
    </term>
    <listitem>
     <para>
      <literal>meatware</literal> requires help from the user to operate.
      Whenever invoked, <literal>meatware</literal> logs a CRIT severity
      message which shows up on the node's console. The operator then
      confirms that the node is down and issues a
      <command>meatclient(8)</command> command. This tells
      <literal>meatware</literal> to inform the cluster that the node should
      be considered dead. See
      <filename>/usr/share/doc/packages/cluster-glue/README.meatware</filename>
      for more information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>null</literal>
    </term>
    <listitem>
     <para>
      This is a fake device used in various testing scenarios. It always
      claims that it has shot a node, but never does anything. Do not use it
      unless you know what you are doing.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><literal>suicide</literal>
    </term>
    <listitem>
     <para>
      This is a software-only device, which can reboot a node it is running
      on, using the <command>reboot</command> command. This requires action
      by the node's operating system and can fail under certain
      circumstances. Therefore avoid using this device whenever possible.
      However, it is safe to use on one-node clusters.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   <literal>suicide</literal> and <literal>null</literal> are the only
   exceptions to the <quote>I do not shoot my host</quote> rule.
  </para>
 </sect1>
 <sect1 id="sec.ha.fencing.recommend">
  <title>Basic Recommendations</title>

  <para>
   Check the following list of recommendations to avoid common mistakes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Do not configure several power switches in parallel.
    </para>
   </listitem>
   <listitem>
    <para>
     To test your &stonith; devices and their configuration, pull the plug
     once from each node and verify that fencing the node does takes place.
    </para>
   </listitem>
   <listitem>
    <para>
     Test your resources under load and verify the timeout values are
     appropriate. Setting timeout values too low can trigger (unnecessary)
     fencing operations. For details, refer to
     <xref
      linkend="sec.ha.config.basics.timeouts"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Use appropriate fencing devices for your setup. For details, also refer
     to <xref linkend="sec.ha.fencing.special"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Configure one ore more &stonith; resources. By default, the global
     cluster option <literal>stonith-enabled</literal> is set to
     <literal>true</literal>. If no &stonith; resources have been defined,
     the cluster will refuse to start any resources.
    </para>
   </listitem>
   <listitem>
    <para>
     Do not set the global cluster option
     <systemitem>stonith-enabled</systemitem> to <literal>false</literal>
     for the following reasons:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Clusters without &stonith; enabled are not supported.
      </para>
     </listitem>
     <listitem>
      <para>
       DLM/OCFS2 will block forever waiting for a fencing operation that
       will never happen.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Do not set the global cluster option
     <systemitem>startup-fencing</systemitem> to <literal>false</literal>.
     By default, it is set to <literal>true</literal> for the following
     reason: If a node is in an unknown state during cluster start-up, the
     node will be fenced once to clarify its status.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.fencing.more">
  <title>For More Information</title>

  <variablelist>
   <varlistentry>
    <term><filename>/usr/share/doc/packages/cluster-glue</filename>
    </term>
    <listitem>
     <para>
      In your installed system, this directory contains README files for
      many &stonith; plug-ins and devices.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><ulink url="http://www.linux-ha.org/wiki/STONITH"/>
    </term>
    <listitem>
     <para>
      Information about &stonith; on the home page of the The High
      Availability Linux Project.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&clusterlabs-doc;
    </term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        <citetitle>Fencing and Stonith</citetitle>: Information about
        fencing on the home page of the Pacemaker Project.
       </para>
      </listitem>
      <listitem>
       <para>&paceex;: Explains the concepts
        used to configure Pacemaker. Contains comprehensive and very
        detailed information for reference.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><ulink
      url="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html"/>
    </term>
    <listitem>
     <para>
      Article explaining the concepts of split brain, quorum and fencing in
      HA clusters.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
