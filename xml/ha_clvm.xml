<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>

<!--taroth 2010-02-10: todo: https://fate.novell.com/307380
  Testcase
setup 
1. start cmirrord 
2. vgconvert -cy $VGNAME 
3. lvcreate -m 1 ++name $LVNAME ++size $LVSIZE $VGNAME 
   A mirrored $LVNAME should be created successfully with userspace log. 
   recover: Following scenarios should be tested under stressed IO load . 
   device failure during writing
   device failure during syncing
   system crash during writing
   system crash during syncing

-->
<chapter id="cha.ha.clvm">
 <title>Cluster LVM</title>
 <abstract>
  <para>
  When managing shared storage on a cluster, every node must be informed
  about changes that are done to the storage subsystem. The Linux Volume
  Manager&nbsp;2 (LVM2), which is widely used to manage local storage, has been
  extended to support transparent management of volume groups across the
  whole cluster. Clustered volume groups can be managed using the same
  commands as local storage.
 </para>
 </abstract>
 
 <sect1 id="sec.ha.clvm.overview">
  <title>Conceptual Overview</title>
  <!--<para>
   Internally, cLVM uses the Distributed Lock Manager (DLM) component of the
   cluster stack to coordinate access to the LVM2 metadata. As DLM in turn
   integrates with the other components of the stack such as fencing, the
   integrity of the shared storage is protected at all times.
  </para>-->
  <para>Clustered LVM is coordinated with different tools:</para>
  
  <variablelist>
   <varlistentry>
    <term>Distributed Lock Manager (DLM)</term>
    <listitem>
     <para>Coordinates disk access for cLVM.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Logical Volume Manager2 (LVM2)</term>
    <listitem>
     <para>Enables flexible distribution of one filesystem over several
      disk. LVM provides a virtual pool of disk space.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Clustered Logical Volume Manager (cLVM)</term>
    <listitem>
     <para>Coordinates access to the LVM2 metadata so every node knows
      about changes. cLVM does not coordinate access to the shared data
      itself; to do so, you must configure OCFS2 or other cluster-aware
      applications on top of the cLVM-managed storage.</para>
    </listitem>
   </varlistentry>
  </variablelist>
  
  <para>Depending on your scenario it is possible to create a
   RAID&nbsp;1 device with cLVM with the following layers:</para>
  
  <remark>toms 2010-03-12 (DEV): What are the advantages and disadvantages?</remark>
  <itemizedlist>
   <listitem>
    <formalpara>
     <title>LVM</title>
     <para>This is a very flexible solution if you want to increase or
      decrease your filesystem size, add more physical storage, or
      create snapshots of your filesystems.</para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>DRBD</title>
     <para>This solution only provides RAID&nbsp;0 (striping) and
      RAID&nbsp;1 (mirroring). </para>
    </formalpara>
   </listitem>
   <listitem>
    <formalpara>
     <title>MD Devices (Linux Software RAID or
      <command>mdadm</command>)</title>
     <para>This solution provides all RAID levels. However, this
      method should not be used for an active-active cluster.</para>
    </formalpara>
   </listitem>
  </itemizedlist>
 </sect1>
 
 <sect1 id="sec.ha.clvm.config">
  <title>Configuration of cLVM</title>
  <para>Make sure you have fullfilled the following prerequisites:</para>
  <itemizedlist>
   <listitem>
    <para>A shared storage device is available, such as
     provided by a Fibre Channel, FCoE, SCSI, iSCSI SAN, or DRBD.</para>
   </listitem>
   <listitem>
    <para>In case of DRBD, both nodes must be primary (it is described
     in the following procedure).</para>
   </listitem>
   <listitem>
    <para>Check if the locking type of LVM2 is cluster-aware.
     The keyword <literal>locking_type</literal> in
      <filename>/etc/lvm/lvm.conf</filename> must contains the value 3
     (should be the default.) Copy the configuration to all nodes, if
     necessary.</para>
   </listitem>
  </itemizedlist>
  
  <note>
   <title>Create Cluster Resources First</title>
   <para>Create your cluster resources first and then your LVM volumes.
    Otherwise it is impossible to remove the volumes later.</para>
  </note>
  
  <sect2 id="sec.ha.clvm.scenario.iscsi">
   <title>Scenario: cLVM With iSCSI</title>
   <para></para>
   
   <warning>
    <title>Data Loss</title>
    <para>The following procedure will destroy any data on your disks!</para>
   </warning>
   
   <remark>toms 2010-03-12: Add some more prerequisites?</remark>
   
   <procedure id="pro.ha.clvm.dlmresource">
    <title>Creating DLM Resource</title>
    <step>
     <para>Start a shell and log in as &rootuser;.</para>
    </step>
    <step>
     <para>Run <command>crm <option>configure</option></command>.</para>
    </step>
    <step>
     <para>Enter the following commands:</para>
     <screen><command>primitive</command> dlm ocf:pacemaker:controld
<command>primitive</command> clvm ocf:lvm2:clvmd \
        params daemon_timeout="30"
<command>group</command> dlm-clvm dlm clvm
<command>clone</command> dlm-clvm-clone dlm-clvm \
        meta interleave="true" ordered="true"</screen>
    </step>
    <step>
     <para>View your changes with <command>show</command>.</para>
    </step>
    <step>
     <para>If everything is correct, enter <command>commit</command> and
     leave <command>crm</command> with <command>exit</command>.</para>
    </step>
   </procedure>
   
   
   <procedure id="pro.ha.clvm.scenario.iscsi">
    <title>Configure iSCSI Targets (Server)</title>
    <!-- 
     Another setup:
     1. Configure iSCSI daemon on file /etc/ietd.conf
     
     Target iqn.2010-03.de.&wsI;disks
     Lun 0 Sectors=x,Type=nullio
     Lun 1 Path=/dev/sda1,Type=blockio,ScsiId=0
     Lun 2 Path=/dev/sda2,Type=blockio,ScsiId=1
     
     Lun 0 are used only for testing and tuning purpose.
     
     2. Start the Daemon:
     # rciscsitarget start
     
     3. Check:
     # netstat -nltp | grep iet
     tcp 0 0 0.0.0.0:3260 0.0.0.0:* LISTEN 12586/ietd
     tcp 0 0 :::3260 :::* LISTEN 12586/ietd
     
     # tail /var/log/messages
     <ADD LOG MESSAGE>
     
     => Disks are shared over Ethernet
     
     
     Configuring Node Machines:
     1. /etc/iscsi/iscsi.conf:
      node.start = automatic
      
     2. rcopen-iscsi start
     
     3. Detect remote disks:
     # iscsi_discovery <SERVER_IP>
     192.168.1.2:3260,1 iqn.2010-03.&wsI;disks
     192.168.1.2:3260,1 iqn.2010-03.&wsI;disks
     
    
    -->
    <step>
     <para>Run &yast; and click <menuchoice>
      <guimenu>Network Services</guimenu>
      <guimenu>iSCSI Target</guimenu>
     </menuchoice> to start the iSCSI Server module.</para>
    </step>
    <step>
     <para>If you want to start the iSCSI target whenever your computer
      is booted, choose <guimenu>When Booting</guimenu> otherwise set
       <guimenu>Manually</guimenu>.</para>
    </step>
    <step>
     <para>If you have a Firewall, enable <guimenu>Open Port in
      Firewall</guimenu>.</para>
    </step>
    <step>
     <para>Switch to the <guimenu>Global</guimenu> tab. If you need
      authentication enable incoming or outgoing authentication or both.
      In this example, we select <guimenu>No
      Authentication</guimenu>.</para>
    </step>
    <step>
     <para>Add a new iSCSI target:</para>
     <substeps>
      <step>
       <para>Switch to the <guimenu>Targets</guimenu> tab.</para>
      </step>
      <step>
       <para>Click <guimenu>Add</guimenu>.</para>
      </step>
      <step id="st.ha.clvm.iscsi.iqn">
       <para>Enter a target name. This must be in the format:</para>
       <screen>iqn.<replaceable>DATE</replaceable>.<replaceable>DOMAIN</replaceable></screen>
      </step>
      <step>
       <para>If you want a more descriptive name, you can change it as
        long as your identifier is unique between your different
        targets.</para>
      </step>
      <step>
       <para>Click <guimenu>Add</guimenu>.</para>
      </step>
      <step>
       <para>Enter the device name in <guimenu>Path</guimenu> and use a
        <guimenu>Scsiid</guimenu>.</para>
      </step>
      <step>
       <para>Click <guimenu>Next</guimenu> two times.</para>
      </step>
     </substeps>
    </step>
    <step>
     <para>Confirm the warning box with <guimenu>Yes</guimenu>.</para>
    </step>
    <step>
     <para>Open the configuration file
      <filename>/etc/iscsi/iscsi.conf</filename> and change the
      parameter <literal>node.startup</literal> to
      <literal>automatic</literal>.</para>
    </step>
   </procedure>
   
   <procedure>
    <title>Configure iSCSI Initiator (Clients)</title>
    <step>
     <para>Start &yast; and the iSCSI Client module.</para>
    </step>
    <step>
     <para>If you want to start the iSCSI target whenever your computer
      is booted, choose <guimenu>When Booting</guimenu> otherwise set
       <guimenu>Manually</guimenu>.</para>
    </step>
    <step>
     <para>Enter your <guimenu>Initiator Name</guimenu> which you have
      created in <xref linkend="st.ha.clvm.iscsi.iqn"/>.</para>
    </step>
   </procedure>
   
   
  </sect2>
  
  
  <!--<para>
   To set up a cluster-aware volume group, several tasks must be completed
   successfully:
  </para>-->

 <sect2 id="sec.ha.clvm.scenario.drbd">
  <title>Scenario: cLVM With DRBD</title>
  <para>The following scenarios can be used if you have data centers
   which were located on different parts of the city, country, or
   continent. </para>
  <procedure id="proc.ha.clvm.withdrbd">
   <title>Creating a Cluster-aware Volume Group With DRBD</title>
   <step>
    <para>Create a primary/primary DRBD resource:</para>
    <substeps>
     <step>
      <para>Set up a DRBD device as primary/secondary first as described in
       <xref linkend="step.drbd.configure"/>. Make sure disk state is
       <literal>UpToDate</literal> on both nodes. Check it with
       <command>cat</command> <option>/proc/drbd</option> or with
       <command>rcdrbd</command> <option>status</option>.</para>
     </step>
     <step>
      <para>Add the following options to your configuration file
       (usually something like
       <filename>/etc/drbd.d/r0.res</filename>):</para>
      <screen>resource r0 {
  startup {
    become-primary-on both;
  }

  net {
     allow-two-primaries;
  }
  ...
}</screen>
     </step>
     <step>
      <para>Copy the changed configuration file to the other node, for
      example:</para>
      <screen><command>scp</command> /etc/drbd.d/r0.res &wsII;:/etc/drbd.d/</screen>
     </step>
     <step>
      <para>Run on <emphasis>both</emphasis> nodes:</para>
      <screen><command>drbdadm</command> disconnect r0
<command>drbdadm</command> connect r0
<command>drbdadm</command> primary r0</screen>
     </step>
     <step>
      <para>Check the status of your nodes:</para>
      <screen>cat /proc/drbd
...
 0: cs:Connected ro:Primary/Primary ds:UpToDate/UpToDate C r----</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Include the clvmd resource as a clone in the pacemaker configuration,
     and make it depend on the DLM clone resource, see
     <xref linkend="pro.ha.clvm.dlmresource"/> how to do it.
     Before proceeding, confirm that these resources have started
     successfully in your cluster. You may use <command>crm_mon</command> 
     or the GUI to check the running services.
    </para>
   </step>
   <step>
    <para>
     Prepare the physical volume for LVM with the command
     <command>pvcreate</command>, for example on the device 
     <filename>/dev/sdb</filename>:
    </para>
<screen>pvcreate /dev/sdb</screen>
   </step>
   <step>
    <para>
     Create a cluster aware volume group:
    </para>
<screen>vgcreate --clustered y myclusterfs /dev/sdb</screen>
   </step>
   <step>
    <para>
     Create logical volumes as needed. You may probably change the size
     (4G) of the logical volume. For example:
    </para>
<screen>lvcreate --name testlv -L 4G myclusterfs</screen>
   </step>
   <step>
    <para>
     To ensure that the volume group is activated cluster-wide, configure a
     LVM resource as follows:
    </para>
<screen><command>primitive</command> vg1 ocf:heartbeat:LVM \
        params volgrpname="myclusterfs"
<command>clone</command> vg1-clone vg1 \
        meta interleave="true" ordered="true"
<command>colocation</command> colo-vg1 inf: vg1-clone dlm-clvm-clone
<command>order</command> order-vg1 inf: dlm-clvm-clone vg1-clone</screen>
   </step>
   <step>
    <para>
     If you want the volume group to only be activated exclusively on one
     node, use the following example; in this case, cLVM will protect all
     logical volumes within the VG from being activated on multiple nodes,
     as an additional measure of protection for non-clustered applications:
    </para>
<screen><command>primitive</command> vg1 ocf:heartbeat:LVM \
        params volgrpname="myclusterfs" exclusive="yes"
<command>colocation</command> colo-vg1 inf: vg1 dlm-clvm-clone
<command>order</command> order-vg1 inf: dlm-clvm-clone vg1</screen>
   </step>
   <step>
    <para>
     The logical volumes within the VG are now available as file system
     mounts or raw usage. Ensure that services using them must have proper
     dependencies to collocate them with and order them after the VG has
     been activated.
    </para>
   </step>
  </procedure>

  <para>
   After finishing these configuration steps, the LVM2 configuration can be
   done just like on any standalone workstation.
  </para>
 </sect2>
 </sect1>
 <sect1 id="sec.ha.clvm.drbd">
  <title>Configuring Eligible LVM2 Devices Explicitly</title>

  <para>
   When several devices seemingly share the same physical volume signature
   (as can be the case for multipath devices or drbd), it is recommended to
   explicitly configure the devices which LVM2 scans for PVs.
  </para>

  <para>
   For example, if the command <command>vgcreate</command> uses the physical
   device instead of using the mirrored block device, DRBD will be confused
   which may result in a split brain condition for DRBD.
  </para>

  <para>
   To deactivate a single device for LVM2, do the following:
  </para>

  <procedure>
   <step>
    <para>
     Edit the file <filename>/etc/lvm/lvm.conf</filename> and search for the
     line starting with <literal>filter</literal>.
    </para>
   </step>
   <step>
    <para>
     The patterns there are handled as regular expressions. A leading
     <quote>a</quote> means to accept a device pattern to the scan, a
     leading <quote>r</quote> rejects the devices that follow the device
     pattern.
    </para>
   </step>
   <step>
    <para>
     To remove a device named <filename>/dev/sdb1</filename>, add the
     following expression to the filter rule:
    </para>
<screen>"r|^/dev/sdb1$|"</screen>
    <para>
     The complete filter line will look like the following:
    </para>
<screen>filter = [ "r|^/dev/sdb1$|", "r|/dev/.*/by-path/.*|",<!--
 --> "r|/dev/.*/by-id/.*|", "a/.*/" ]</screen>
    <para>
     A filter line, that accepts DRBD and MPIO devices but rejects all other
     devices would look like this:
    </para>
<screen>filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|",<!--
--> "r/.*/" ]</screen>
   </step>
<!--   
   <step>
    <para>
     Explicitly accept all the eligible devices, and reject all others.
     For example, a filter line which accepts all MPIO and drbd devices would
     look like this:
    </para>
    <screen>filter = [ "a|/dev/drbd.*|", "a|/dev/.*/by-id/dm-uuid-mpath-.*|", "r/.*/"]</screen>
   </step> -->
   <step>
    <para>
     Write the configuration file and copy it to all cluster nodes.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.clvm.more">
  <title>For More Information</title>

  <para>
   Much information is available from the pacemaker mailing list, available
   at <ulink url="http://www.clusterlabs.org/wiki/Help:Contents"/>.
  </para>

  <para>
   The official cLVM FAQ can be found at
   <ulink url="http://sources.redhat.com/cluster/wiki/FAQ/CLVM"/>.
  </para>
 </sect1>
</chapter>
