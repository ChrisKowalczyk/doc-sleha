<!DOCTYPE sect1 PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<sect1 id="sec.ha.geo.drbd">
 <title>Setting Up DRBD</title>
 <para>For a description of the overall scenario, see <xref linkend="sec.ha.geo.oview"/>. Assuming
  that you have two cluster sites that are connected with a routed IPv4 or IPv6 connection and a
  transmission speed ranging from a few Mbit/sec up to 10Gbit/sec, using a cluster file system
  across the sites will not be possible, because of the high latency. But you can use DRBD to
  replicate the data for a quick failover in case on of the sites goes down (active/passive setup).
  DRBD is a software for replicating storage data by mirroring the content of block devices (hard
  disks, partitions, logical volumes etc.) between hosts located on different sites. Failover is
  managed via the booth services, see <xref linkend="vle.ha.geo.components.booth"/>.</para>
 
 <para><xref linkend="fig.ha.geo.drbd.setup"/> shows a graphical representation of the setup and
  the resources that we will configure in the following.</para>
 <figure id="fig.ha.geo.drbd.setup">
  <title></title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ha_geo_drbd.png" width="50%"
     format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ha_geo_drbd.png" width="50%"
     format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <itemizedlist id="il.ha.geo.drbd.scenario">
  <title>Basic Setup</title>
  <listitem>
   <para>A file system is to be served across the &geo; cluster via NFS. </para>
  </listitem>
  <listitem>
   <para>On site &cluster1;, DRBD is running in protocol <literal>C</literal>. <remark>taroth
    2014-12-03: what does that mean? explain...</remark> It uses local IP addresses in a LAN.</para>
  </listitem>
  <listitem>
   <para>The upper layer DRDB runs on one node per site, and is responsible for replicating the
    data to the other site of the &geo; cluster.</para></listitem>
  <listitem>
   <para>The lower layer DRBD is responsible for the local replication of data (between the nodes
    of one cluster site). After activating one of the lower DRBD devices on one node per site, the
    service IP (to be configured as a cluster resource) will be started. </para>
  </listitem>
  <listitem>
   <para>The service IP is not only used for the service as such, but also as a fixed point that
    can be accessed by the upper DRBD device (which runs in secondary state) for
    replication.</para>
  </listitem>
  <listitem>
   <para>On the site that should run the file system service, the upper layer DRBD gets set
    <literal>primary</literal><remark>taroth 2014-12-03: DEVs, where does this happen? on cluster
     resource level? and if yes, will this be done automatically by one of the multi-state
     resources? or does this need manual interaction? </remark>. This means that the file system
    therein can be mounted and used by applications.</para>
  </listitem>
  <listitem>
   <para>Optionally, the DRBD connection to the other site of the &geo; cluster may use a DRBD
    Proxy in between.</para>
  </listitem>
 </itemizedlist>
 
 <sect2 id="sec.ha.geo.drbd.cfg">
  <title>DRBD Configuration</title>
  <para>Beginning with DRBD 8.3, the DRBD configuration file is split into
   separate files. They must be located in the <filename>/etc/drbd.d/</filename>
   directory. The following DRBD configuration snippets show a basic DRBD
   configuration for the scenario mentioned in <xref
    linkend="il.ha.geo.drbd.scenario"/>. All snippets can be added to a single
   DRBD resource configuration file, for example,
    <filename>/etc/drbd.d/nfs.res</filename> that can then by synchronized using
   &csync; as described in <xref
    linkend="sec.ha.geo.booth.sync.csync2.setup"/>. Note that the DRBD
   configuration snippets below are bare-bones&mdash;they does not include
   any performance tuning options or similar. </para>
 </sect2>
 
 
 
</sect1>
