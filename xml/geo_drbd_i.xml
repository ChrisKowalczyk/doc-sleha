<!DOCTYPE sect1 PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<sect1 id="sec.ha.geo.drbd">
 <title>Setting Up DRBD</title>
 <para>For a description of the overall scenario, see <xref linkend="sec.ha.geo.oview"/>. Assuming
  that you have two cluster sites that are connected with a routed IPv4 or IPv6 connection and a
  transmission speed ranging from a few Mbit/sec up to 10Gbit/sec, using a cluster file system
  across the sites will not be possible, because of the high latency. But you can use DRBD to
  replicate the data for a quick failover in case on of the sites goes down (active/passive setup).
  DRBD is a software for replicating storage data by mirroring the content of block devices (hard
  disks, partitions, logical volumes etc.) between hosts located on different sites. Failover is
  managed via the booth services, see <xref linkend="vle.ha.geo.components.booth"/>.</para>
 
 <para><xref linkend="fig.ha.geo.drbd.setup"/> shows a graphical representation of the setup and
  the resources that we will configure in the following.</para>
 <figure id="fig.ha.geo.drbd.setup">
  <title>DRBD Setup and Resources</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ha_geo_drbd.png" width="50%"
     format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ha_geo_drbd.png" width="50%"
     format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <itemizedlist id="il.ha.geo.drbd.scenario">
  <title>Basic Setup</title>
  <listitem>
   <para>A file system is to be served across the &geo; cluster via NFS. </para>
  </listitem>
  <listitem>
   <para>On site &cluster1;, DRBD is running in protocol
     <literal>C</literal>, a synchronous replication protocol. It uses local IP
    addresses in a LAN.</para>
  </listitem>
  <listitem>
   <para>The upper layer DRDB runs on one node per site, and is responsible for replicating the
    data to the other site of the &geo; cluster.</para></listitem>
  <listitem>
   <para>The lower layer DRBD is responsible for the local replication of data (between the nodes
    of one cluster site). After activating one of the lower DRBD devices on one node per site, the
    service IP (to be configured as a cluster resource) will be started. </para>
  </listitem>
  <listitem>
   <para>The service IP is not only used for the service as such, but also as a fixed point that
    can be accessed by the upper DRBD device (which runs in secondary state) for
    replication.</para>
  </listitem>
  <listitem>
   <para>On the site that should run the file system service, the upper layer DRBD gets set
    <literal>primary</literal><remark>taroth 2014-12-03: DEVs, where does this happen? on cluster
     resource level? and if yes, will this be done automatically by one of the multi-state
     resources? or does this need manual interaction? </remark>. This means that the file system
    therein can be mounted and used by applications.</para>
  </listitem>
  <listitem>
   <para>Optionally, the DRBD connection to the other site of the &geo; cluster may use a DRBD
    Proxy in between.</para>
  </listitem>
 </itemizedlist>
 
 <sect2 id="sec.ha.geo.drbd.cfg">
  <title>DRBD Configuration</title>
  <para>Beginning with DRBD 8.3, the DRBD configuration file is split into
   separate files. They must be located in the <filename>/etc/drbd.d/</filename>
   directory. The following DRBD configuration snippets show a basic DRBD
   configuration for the scenario mentioned in <xref
    linkend="il.ha.geo.drbd.scenario"/>. All snippets can be added to a single
   DRBD resource configuration file, for example,
    <filename>/etc/drbd.d/nfs.res</filename>. This file can then by synchronized using
   &csync; as described in 
   <xref linkend="sec.ha.geo.booth.sync.csync2.setup"/>. Note that the DRBD
   configuration snippets below are bare-bones&mdash;they do not include
   any performance tuning options or similar. </para>
  <example>
   <title>DRBD Configuration Snippet for Site 1 (&cluster1;)</title>
   <screen>resource nfs-lower-&cluster1; <co id="co.geo.drbd.config.rsc"/>{
         disk        /dev/volgroup/lv-nfs; <co id="co.geo.drbd.config.disk"/>
         meta-disk   internal; <co id="co.geo.drbd.config.meta-disk"/>
         device      /dev/drbd0; <co id="co.geo.drbd.config.device"/>
         protocol    C;  <co id="co.geo.drbd.config.protocol"/>
         net {
                      shared-secret  "2a9702a6-8747-11e3-9ebb-782bcbd0c11c; <co id="co.geo.drbd.config.shared-secret"/>
         }

         on &node1; { <co id="co.geo.drbd.config.resname"/>
                      address         192.168.201.111:7900; <co id="co.geo.drbd.config.address"/>
         }
         on &node2; { <xref linkend="co.geo.drbd.config.resname" xrefstyle="selec:nopage"/>
                      address         192.168.201.112:7900; <xref linkend="co.geo.drbd.config.address" xrefstyle="selec:nopage"/>
         } 
}</screen>
   <calloutlist>
    <callout arearefs="co.geo.drbd.config.rsc">
     <para> Choose a resource name that allows some association to the
      respective service (here: NFS). By including the site's name, too (here:
      &cluster1;) the complete DRBD configuration can be synchronized across
      the sites without causing name conflicts. </para>
    </callout>
    <callout arearefs="co.geo.drbd.config.disk">
     <para> The device that is replicated between the nodes. In our example, LVM
      is used as a storage layer below DRBD, and the volume group's name is
       <literal>volgroup</literal>. </para>
    </callout>
    <callout arearefs="co.geo.drbd.config.meta-disk">
     <para> The meta-disk parameter usually contains the value
       <literal>internal</literal>. It is also possible to specify an explicit
      device to hold the meta data. See <ulink
       url="http://www.drbd.org/users-guide-emb/ch-internals.html#s-metadata"/>
      for more information. </para>
    </callout>
    <callout arearefs="co.geo.drbd.config.device">
     <!--taroth 2010-08-16: fix for bnc#623524-->
     <para> The device name for DRBD and its minor number. To differentiate
      between the lower layer DRBD for the local replication and the upper layer
      DRBD for replication between the &geo; cluster sites, the device minor
      numbers 0 respectively 10 are used. </para>
    </callout>
    <callout arearefs="co.geo.drbd.config.protocol">
     <para>DRBD is running in protocol C, a synchronous replication protocol.
      Local write operations on the primary node are considered completed only
      after both the local and the remote disk write have been confirmed. As a
      result, loss of a single node is guaranteed not to lead to any data loss.
      Data loss is, of course, inevitable even with this replication protocol if
      both nodes (or their storage subsystems) are irreversibly destroyed at the
      same time.</para>
    </callout>
    <callout arearefs="co.geo.drbd.config.shared-secret">
     <para>A shared-secret is used to validate connection pairs. For example, 
      you can get unique values with the UUID program.</para>
    </callout>
    <callout arearefs="co.geo.drbd.config.resname">
     <para> The <literal>on</literal> section states which host this
      configuration statement applies to. </para>
    </callout>
    <callout arearefs="co.geo.drbd.config.address">
     <para> The local IP address and port number of the respective node. Each
      DRBD resource needs an individual port. </para>
    </callout>
   </calloutlist>
  </example>
 </sect2>
 
 
 
</sect1>
