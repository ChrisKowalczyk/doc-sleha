<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!-- Fates #310356,  #311881, #311882 -->
<chapter id="cha.ha.geo">
 <title>Multi-site Clusters</title>
 <abstract>
  <para>Apart from local clusters and metro area clusters, &productname;
   &productnumber; also supports multi-site clusters. That means you can
   have multiple, geographically dispersed, sites with a local cluster each.
   Failover between these clusters is coordinated by a higher level entity, the
   so-called <literal>booth</literal>. 
  </para>
  </abstract>
<!--FIXME: discriminate between local, metro and multi-site clusters-->
  <para>
   Those factors differentiate a multi-site cluster from a
   <quote>stretched</quote> local cluster, where a cluster might spawn the
   whole campus in the city. But a <quote>stretched</quote> cluster is still
   a single cluster, using unicast or multicast for communication between
   the nodes and managing failover internally.
  </para>
 <sect1 id="sec.ha.geo.challenges">
  <title>Challenges for Multi-site Clusters</title>

  <para>
   Typically, multi-site environments are too far apart to support
   synchronous communication between the sites and synchronous data
   replication. That leads to the following challenges:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     How to make sure that resources are only started once?
    </para>
   </listitem>
   <listitem>
    <para>
     How to make sure that quorum can be reached between the different
     sites?
    </para>
   </listitem>
<!--<listitem>
    <para>How to keep the CIB up-to-date on all nodes and sites?</para>
   </listitem>
   -->
   <listitem>
    <para>
     How to manage failover between the sites?
    </para>
   </listitem>
   <listitem>
    <para>
     How to deal with high latency in case of resources that need to be
     stopped?
    </para>
    <remark>toms 2011-10-04: Just some further ideas:
     (1) How to distinguish between high latency and a dead cluster/node?
     (2) How to avoid split-brain situations?
     </remark>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.geo.concept">
  <title>Conceptual Overview</title>

  <para>
   Multi-site clusters based on &productname; can be considered as
   <quote>overlay</quote> clusters where each cluster site corresponds to a
   cluster node in a traditional cluster. The overlay cluster is managed by
   the booth mechanism. It guarantees that the cluster resources will be
   highly available across different cluster sites takes. This is achieved
   by using so-called tickets that are treated as failover domain between
   cluster sites, in case a site should be down.
  </para>

  <para>
   <remark>toms 2011-10-04: IHMO this para could be merged with the
  Ticket entry in the variablelist. Maybe reduce this para and introduce
  the list with the sentence: "The following terms are used through this
  chapter:"</remark>
   A ticket within an overlay cluster is similar to a resource in a
   traditional cluster. But in contrast to traditional clusters, tickets are
   the only type of resource in an overlay cluster. They are primitive
   resources that do not need to be configured nor cloned. The following
   list explains the components and concepts introduced for multi-site
   clusters in more detail.
  </para>

  <figure>
   <title>Multi-site Cluster</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_geocluster.svg" width="80%" format="SVG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_geocluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <variablelist id="vl.ha.geo.components">
   <title>Components and Concepts of Multi-site Clusters</title>
   <varlistentry id="vle.ha.geo.components.ticket">
<!--taroth 2011-09-20: mail by jjzhang (ha-devel, 2011-07-26): ... we decide to 
     change the name "token" to "ticket" in order to avoid confusion with "totem token" 
     in corosync-->
    <term>Ticket</term>
    <listitem>
     <para>
      A ticket grants the right to run certain resources on a specific
      cluster site. Initially, none of the sites has a ticket&mdash;each
      ticket must be granted once by the cluster administrator. After that,
      tickets are managed by the booth for automatic failover of resources,
      but administrators may also intervene and grant or revoke tickets
      manually.
     </para>
     <para>
      Resources can be bound to a certain ticket by dependencies. Only if
      the defined ticket is available at a site, the respective resources
      are started. Vice versa, if the ticket is removed, the resources
      depending on that ticket are automatically stopped.
     </para>
     <para>
      The presence or absence of tickets for a site is stored in the CIB as
      a cluster status. With regards to a certain ticket, there are only two
      states for a site: <literal>true</literal> (the site has the ticket)
      or <literal>false</literal> (the site does not have the ticket). The
      absence of a certain ticket (during the initial state of the
      multi-site cluster) is not treated differently from the situation
      after the ticket has been revoked: both are reflected by the value
      <literal>false</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.booth">
    <term>Booth</term>
    <listitem>
     <para>
      The instance managing the ticket distribution and thus, the failover
      process between the sites of a multi-site cluster. Each of the
      participating clusters and arbitrators run a service, the
      <systemitem
       class="resource">boothd</systemitem>. It connects
      to the booth daemons running at the other sites and exchanges
      connectivity details. Once a ticket is granted to a site, the booth
      mechanism will manage the ticket automatically: If the site which
      holds the ticket is out of service, the booth daemons vote which of
      the other sites will get the ticket. To protect against brief
      connection failures, sites that lose the vote (either explicitly or
      implicitly by being disconnected from the voting body) need to
      relinquish the ticket after a time-out. Thus, it is made sure that a
      ticket will only be re-distributed after it has been relinquished by
      the previous site. See also
      <xref
       linkend="vle.ha.geo.components.deadman"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.arbitrator">
    <term>Arbitrator</term>
    <listitem>
     <para>
      Each site runs one booth instance that is responsible for
      communicating with the other sites. If you have an even number of
      sites, you need an additional instance to reach consensus about
      decisions such as failover of resources across sites. In this case,
      add one or more arbitrators running at additional sites. Arbitrators
      are single machines that run a booth instance in a special mode that
      prevents them from being granted tickets. As all booth instances
      communicate with each other, arbitrators help to make more reliable
      decisions about granting or revoking tickets.
     </para>
     <para>
      An arbitrator is especially important for a two-site scenario: For
      example, if site <literal>A</literal> can no longer communicate with
      site <literal>B</literal>, there are two possible causes for that:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        A network failure between <literal>A</literal> and
        <literal>B</literal>.
       </para>
      </listitem>
      <listitem>
       <para>
        Site <literal>B</literal> is down.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      However, if site <literal>C</literal> (the arbitrator) can still
      communicate with site <literal>A</literal>, site <literal>A</literal>
      must still be up and running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.geo.components.deadman">
    <term>Dead Man Dependency (<literal>loss-policy</literal>)</term>
    <listitem>
     <para>
      After a ticket is revoked, it can take a long time until all resources
      depending on that ticket are stopped, especially in case of cascaded
      resources. To cut that process short, the cluster administrator can
      configure a <literal>loss-policy</literal> (together with the ticket
      dependencies) for the case that a ticket gets revoked from a site. The
      loss-policy defines what happens to the resources that depend on that
      ticket. For example, the nodes that are hosting dependent resources
      can be fenced (which would considerably speed up the recovery process
      of the cluster).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.geo.req">
  <title>Requirements</title>

  <itemizedlist>
   <title>Software Requirements</title>
   <listitem>
    <para>
     The <systemitem class="resource">booth</systemitem> package must be
     installed on all cluster nodes and on all arbitrators that will be part
     of the multi-site clusters.
    </para>
   </listitem>
   <listitem>
    <para>
     <remark>taroth 2011-09-21: DEVs, is that correct? I guess we don't support
      any mixed clusters... </remark>
     All clusters that will be part of the multi-site cluster must be based
     on &productname; &productnumber;.
    </para>
   </listitem>
   <listitem>
    <para>
     <remark>taroth 2011-09-21: DEVs, what about the arbitrator? I guess it must
      have SLE 11 SP2, too? (but no HA extension, only the booth
      package?)</remark>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   <remark>taroth 2011-09-21: DEVs, are there any more prerequisites? e.g.
    network configuration? storage configuration?</remark>
  </para>

  <para>
   The most common scenario is probably a multi-site cluster with two sites
   and a single arbitrator on a third site. However, technically, there are
   no limitations with regards to the number of sites and the number of
   arbitrators involved.
  </para>

  <para>
   Nodes belonging to the same cluster site should be synchronized via NTP.
   However, time synchronization is not required between the individual
   cluster sites.
  </para>
 </sect1>
 <sect1 id="sec.ha.geo.setup">
  <title>Basic Setup</title>

  <para>
   Configuring a multi-site cluster takes the following basic steps:
  </para>

  <variablelist>
   <varlistentry>
<!--Configuring Cluster Resources and Constraints-->
    <term><xref linkend="sec.ha.geo.setup.resources" xrefstyle="select:title"/>
    </term>
    <listitem>
     <orderedlist>
      <listitem>
<!--ticket dependencies-->
       <para>
        <xref linkend="pro.ha.geo.setup.rsc.constraints" xrefstyle="select:title"/>
       </para>
      </listitem>
      <listitem>
<!--configuring a resource group for boothd-->
       <para>
        <xref linkend="pro.ha.geo.setup.rsc.boothd" xrefstyle="select:title"/>
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
<!--Setting Up the Booth Services-->
    <term><xref linkend="sec.ha.geo.setup.booth" xrefstyle="select:title"/>
    </term>
    <listitem>
     <orderedlist>
      <listitem>
<!--editing + copying booth config-->
       <para>
        <xref linkend="pro.ha.geo.setup.booth.config" xrefstyle="select:title"/>
       </para>
      </listitem>
      <listitem>
<!--starting boothd-->
       <para>
        <xref linkend="pro.ha.geo.setup.booth.service" xrefstyle="select:title"
        />
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 id="sec.ha.geo.setup.resources">
   <title>Configuring Cluster Resources and Constraints</title>
   <para>
<!--FIXME: add some blurb-->
   </para>
   <procedure id="pro.ha.geo.setup.rsc.constraints">
    <title>Configuring Ticket Dependencies</title>
    <para>
     The <command>crm configure rsc_ticket</command> command lets you
     specify the resources depending on a certain ticket. Together with the
     constraint, you can set a <literal>loss-policy</literal> that defines
     what should happen to the respective resources if the ticket is
     revoked. The attribute <literal>loss-policy</literal> can have the
     following values: <literal>fence</literal>, <literal>stop</literal>,
     <literal>freeze</literal>, <literal>demote</literal>.
     <remark>taroth
     2011-09-27: DEVs, what happens for "demote"?</remark>
     <remark>toms 2011-10-04: Maybe short explanations about the
        different options?</remark>
    </para>
    <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
      <remark>toms 2011-10-04: Shorten it to "Start a root shell on one
        of the cluster nodes"? I
        guess we deal with admin-minded people here</remark>
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      shell.
     </para>
    </step>
    <step id="step.ha.geo.setup.rsc.constraints">
     <para>
      Configure a constraint that defines which resources depend on a
      certain ticket. For example, enter:
     </para>
     <screen>rsc_ticket rsc1-req-ticketA ticketA: rsc1:Master loss-policy="fence" </screen>
     <remark>toms 2011-10-04: I would propose to follow the examples in
        the "ha_config_cli.xml" file and add a prompt. This makes it
        easier to see where the user is.</remark>
     <para>
      This command creates a constraint with the ID
      <literal>rsc1-req-ticketA</literal>. It defines that the resource
      <literal>rsc1</literal> depends on <literal>ticketA</literal> and that
      the node running the resource should be fenced in case
      <literal>ticketA</literal> is revoked.
     </para>
    </step>
    <step>
     <para>
      If you want other resources to depend on further tickets, create as
      many constraints as necessary with <command>rsc_ticket</command>.
     </para>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
     <para>
      The constraints are saved to the CIB. For example, the command in
      <xref linkend="step.ha.geo.setup.rsc.constraints"/> would result in
      the following constraint configuration in the CIB:
     </para>
     <screen>&lt;rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" role="Master" ticket="ticketA" loss-policy="fence"/></screen>
     <para>
      <remark>toms 2011-10-04: Is the output really needed
        anymore? Really interesting for our users?</remark>
     </para>
    </step>
   </procedure>
   <procedure id="pro.ha.geo.setup.rsc.boothd">
    <title>Configuring a Resource Group for <systemitem class="daemon"
      >boothd</systemitem></title>
    <para>
     Each site and arbitrator needs to run one instance of
     <systemitem
      class="daemon">boothd</systemitem>. The daemon can be
     started on any node, therefore it should be configured as primitive
     resource. As the daemons
     <remark>toms 2011-10-04: Plural?</remark>
     need a persistent IP address to communicate with each other, configure
     another primitive that manages the IP address for each booth daemon as
     virtual IP address. Group booth primitives:
    </para>
    <step>
     <para>
      On one of the cluster nodes, start a shell and log in as &rootuser; or
      equivalent.
     </para>
    </step>
    <step>
     <para>
      Enter <command>crm configure</command> to switch to the interactive
      shell.
     </para>
    </step>
    <step>
     <para>
      Enter the following commands to create both primitive resources and to
      add them to one group, <literal>g-booth</literal>:
     </para>
     <screen>primitive booth-ip ocf:heartbeat:IPaddr2 params ip="<replaceable>IP_ADDRESS</replaceable>"
primitive booth ocf:pacemaker:booth-site
group g-booth booth-ip booth</screen>
     <remark>toms 2011-10-04: Again, add the CRM prompt</remark>
    </step>
    <step>
     <para>
      Review your changes with <command>show</command>.
     </para>
    </step>
    <step>
     <para>
      If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.
     </para>
    </step>
    <step>
     <para>
      Repeat the resource group configuration on the other cluster sites,
      using a different IP address for each boothd resource group.
     </para>
     <para>
      With this configuration, each booth daemon will be available at its
      individual IP address, independent of the node the daemon is running
      on.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.geo.setup.booth">
   <title>Setting Up the Booth Services</title>
   <para>
    After having configured the resource group for the
    <systemitem
     class="daemon">boothd</systemitem> and the ticket
    dependencies, complete the booth setup:
   </para>
   <remark>taroth 2011-09-28: DEVs, what's the correct filename: booth? (as
    cited in the mail on ha-devel or booth.conf as mentioned by jjzhang while in
    Nuremberg</remark>
   <procedure id="pro.ha.geo.setup.booth.config">
    <title>Editing The Booth Configuration File</title>
    <step>
     <para>
      Log in to a cluster node as &rootuser; or equivalent.
     </para>
    </step>
    <step>
     <para>
      Create <filename>/etc/sysconfig/booth.conf</filename> and edit it
      according to the example below:
      <remark>taroth 2011-08-28: todo: mention
       example file if it should be available with one of the betas</remark>
     </para>
     <example>
      <title>Example Booth Configuration File</title>
      <screen>transport="UDP" <co id="co.ha.geo.booth.config.transport"/>
port="6666" <co id="co.ha.geo.booth.config.port"/>
arbitrator="147.2.207.14" <co id="co.ha.geo.booth.config.arbitrator"/>
site="147.4.215.19" <co id="co.ha.geo.booth.config.site"/>
site="147.18.2.1"  <xref linkend="co.ha.geo.booth.config.site" xrefstyle="select:label nopage"/>
ticket="ticketA;<co id="co.ha.geo.booth.config.ticket"/>1000<co id="co.ha.geo.booth.config.expiry"/>"
ticket="ticketB;<xref linkend="co.ha.geo.booth.config.ticket" xrefstyle="select:label nopage"/>1200<xref linkend="co.ha.geo.booth.config.expiry" xrefstyle="select:label nopage"/>"     </screen>
      <calloutlist>
       <callout arearefs="co.ha.geo.booth.config.transport">
        <para>
         Defines the transport protocol used for communication between the
         sites.
         <remark>taroth 2011-09-28: DEVs, is that correct? what values
          are allowed here?</remark>
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.port">
        <para>
         Defines the port used for communication between the sites.
         <remark>taroth 2011-09-28: DEVs, if I got it right, any port will do?
          the port must be opened in the firewall, too I guess?</remark>
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.site">
        <para>
         Defines the IP address used for the
         <systemitem class="daemon"
          >boothd</systemitem> on each
         site. Make sure to insert the correct IP addresses for each site,
         otherwise the booth mechanism won't work correctly.
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.ticket">
        <para>
         Defines the ticket to be managed by the booth. For each ticket, add
         a <literal>ticket</literal> entry.
        </para>
       </callout>
       <callout arearefs="co.ha.geo.booth.config.ticket">
        <para>
         Defines the ticket's expiry time in seconds.
         <remark>taroth 2011-09-27: DEVs, the following is not clear to me:
          "ticketA;1200" means a ticket named 'ticketA', and expire every 1000
          seconds. A site already grant the ticket will renew the ticket every
          800 seconds." -why are there 3 different values mentioned? jjzhang
          mentioned some quotient (4/5 expiry time), but I can't recall the
          details... how does the renewal/failover process work: who renews the
          ticket? if a site is down, the remaining booth daemons decide which
          site will get the ticket - what's the basis of their
          decision?</remark>
        </para>
       </callout>
      </calloutlist>
     </example>
    </step>
    <step>
     <para>
      Verify your changes and save the file.
     </para>
    </step>
    <step>
     <para>
      Copy <filename>/etc/sysconfig/booth.conf</filename> to all sites and
      arbitrators. In case of any changes, make sure to update the file
      accordingly on all parties.
     </para>
     <note>
      <title>Synchronize Booth Configuration to All Sites and Arbitrators</title>
      <para>
       All cluster nodes and arbitrators within the multi-site cluster must
       use the same booth configuration. While you may need to copy the
       files manually to the arbitrators and to one cluster node per site,
       you can use &csync; within each cluster site to synchronize the file
       to all nodes.
      </para>
     </note>
    </step>
   </procedure>
   <procedure id="pro.ha.geo.setup.booth.service">
    <title>Starting the Booth Services</title>
    <para/>
    <step>
     <para>
      <remark>taroth 2011-09-30: that didn't work for me today though I followed
       the configuration instructions by jjzhang and ygao- asked for help on the
       ML</remark>
      Start the booth resource group on each other cluster site. It will
      start one instance of the booth service per site.
     </para>
    </step>
    <step>
     <para>
      Log in to each arbitrator and start the booth service:
     </para>
     <screen>/etc/init.d/booth-arbitrator start</screen>
     <para>
      This starts the booth service in arbitrator mode. It can communicate
      with all other booth daemons but in contrast to the booth daemons
      running on the cluster sites, it cannot be granted a ticket.
     </para>
    </step>
   </procedure>
   <para>
    After finishing the booth configuration and starting the booth services,
    you are now ready to start the ticket process.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.ha.geo.manage">
  <title>Managing Multi-site Clusters</title>

  <para>
   Before the booth can manage a certain ticket within the multi-site
   cluster, you initially need to grant it to a site manually. Several shell
   commands are available for managing tickets:
   <remark>taroth 2011-09-27: DEVs,
    is the following correct?</remark>
   The <command>booth&nbsp;client</command> command works on any machine
   where the booth daemon is running. The <command>crm_ticket</command>
   command is a CRM shell command and can only be used on cluster nodes. For
   details about the commands' usage and options, refer to the respective
   man pages.
   <remark>taroth 2011-10-04: DEVs, not sure if I should mention more details
    about crm_ticket here if we advise to use booth client anyway? what do you
    think?</remark>
  </para>

  <procedure id="pro.ha.geo.manage.tickets">
   <title>Managing Tickets Manually</title>
   <note>
    <title>Granting Tickets</title>
    <para>
     To make sure that a ticket is not granted twice among the cluster
     sites, always use <command>booth&nbsp; client</command> for granting
     tickets.
     <remark>taroth 2011-09-28: DEVs, is the following
      correct?</remark>
     This command executes a sanity check and will warn you if the same
     ticket is already granted to another site. Principally, tickets can
     also be granted with <command>crm_ticket</command> but the CRM shell
     cannot validate if the same ticket is already granted elsewhere.
    </para>
   </note>
   <step>
    <para>
     To list all tickets:
     <remark>taroth 2011-09-30: hm , this didn't work
      for me at all (even though I can see in the CIB that two tickets have been
      granted), probably because I couldn't get booth to run properly on the
      cluster... DEVs, is this command supposed to show the tickets on *all*
      sites?</remark>
    </para>
    <screen>booth client list</screen>
   </step>
   <step>
    <para>
     To grant a ticket to a site (for example <literal>ticketA</literal> to
     the site <literal>147.2.207.14</literal>):
    </para>
    <screen>booth client grant -t ticketA -s 147.2.207.14</screen>
    <para>
     When granting tickets, you can also specify the expiry time after which
     a ticket will fail over to another site if it has not been renewed. The
     default expiry time is <literal>600</literal> seconds. To specify a
     different value, use the <option>-e</option> option:
    </para>
    <screen>booth client grant -t ticketA -s 147.2.207.14 -e 1000</screen>
   </step>
   <step>
    <para>
     To revoke a ticket from a site (for example <literal>ticketA</literal>
     from the site <literal>147.2.207.14</literal>):
    </para>
    <screen>booth client revoke -t ticketA -s 147.2.207.14</screen>
   </step>
  </procedure>

  <remark>taroth 2011-09-26: DEVs, is it planned to show the ticket status also
   in the GUI or HAWK? or is it only possible to query for them using crm_ticket
   or booth client list?</remark>

  <para>
   After you have initially granted a ticket to a site, the booth mechanism
   will take over and manage the ticket automatically. If the site holding a
   ticket should be out of service, the ticket will automatically be revoked
   after the expiry time and granted to another site. The resources that
   depend on that ticket will fail over to the new site holding the ticket.
   The nodes that have run the resources before will be treated according to
   the <literal>loss-policy</literal> you set within the constraint.
  </para>
 </sect1>
<!--from 
   http://oss.clusterlabs.org/pipermail/pacemaker/2011-January/008878.html -->
<!--IntroductioN: At LPC 2010, we discussed (once more) that a key feature
 for pacemaker in 2011 would be improved support for multi-site clusters;
 by multi-site, we mean two (or more) sites with a local cluster each,
 and some higher level entity coordinating fail-over across these (as
 opposed to "stretched" clusters, where a single cluster might spawn the
 whole campus in the city).
 
 Typically, such multi-site environments are also too far apart to
 support synchronous communication/replication.
 
 There are several aspects to this that we discussed; Andrew and I first
 described and wrote this out a few years ago, so I hope he can remember
 the rest ;-)
 
 "Tokens" are, essentially, cluster-wide attributes (similar to node
  attributes, just for the whole partition). Via dependencies (similar to
  rsc_location), one can specify that certain resources require a specific
  token to be set before being started (and, vice versa, need to be
  stopped if the token is cleared). You could also think of our current
  "quorum" as a special, cluster-wide token that is granted in case of
  node majority.
 
 The token thus would be similar to a "site quorum"; i.e., the permission
 to manage/own resources associated with that site, which would be
 recorded in a rsc dependency. (It'd probably make a lot of sense if this
 would support resource sets, so one can easily list all the resources;
 also, some resources like m/s may tie their role to token ownership.)
 
 These tokens can be granted/revoked either manually (which I actually
 expect will be the default for the classic enterprise clusters), or via
 an automated mechanism described further below.
 
 
 Another aspect to site fail-over is recovery speed. A site can only
 activate the resources safely if it can be sure that the other site has
 deactivated them. Waiting for them to shutdown "cleanly" could incur
 very high latency (think "cascaded stop delays"). So, it would be
 desirable if this could be short-circuited. The idea between Andrew and
 myself was to introduce the concept of a "dead man" dependency; if the
 origin goes away, nodes which host dependent resources are fenced,
 immensely speeding up recovery.
 
 It seems to make most sense to make this an attribute of some sort for
 the various dependencies that we already have, possibly, to make this
 generally available. (It may also be something admins want to
 temporarily disable - i.e., for a graceful switch-over, they may not
 want to trigger the dead man process always.)
 
 
 The next bit is what we called the "Cluster Token Registry"; for those
 scenarios where the site switch is supposed to be automatic (instead of
 the admin revoking the token somewhere, waiting for everything to stop,
 and then granting it on the desired site). The participating clusters
 would run a daemon/service that would connect to each other, exchange
 information on their connectivity details (though conceivably, not mere
 majority is relevant, but also current ownership, admin weights, time
 of day, capacity ...), and vote on which site gets which token(s); a
 token would only be granted to a site once they can be sure that it has
 been relinquished by the previous owner, which would need to be
 implemented via a timer in most scenarios (see the dead man flag).
 
 Further, sites which lose the vote (either explicitly or implicitly by
 being disconnected from the voting body) would obviously need to perform
 said release after a sane time-out (to protect against brief connection
 issues).
 
 
 A final component is an idea to ease administration and management of
 such environments. The dependencies allow an automated tool to identify
 which resources are affected by a given token, and this could be
 automatically replicated (and possibly transformed) between sites, to
 ensure that all sites have an uptodate configuration of relevant
 resources. This would be handled by yet another extension, a CIB
 replicator service (that would either run permanently or explicitly when
 the admin calls it).
 
 Conceivably, the "inactive" resources may not even be present in the
 active CIB of sites which don't own the token (and be inserted once
 token ownership is established). This may be an (optional) interesting
 feature to keep CIB sizes under control.
-->
<!--info by jjzhang-->
<!-- Below is an introduction on "Cluster Ticket Registry":
 
 (Based on the discussion on the ML, we decide to change the name "token"
 to "ticket" in order to avoid confusion with "totem token" in corosync.)
 
 Overview:
 Cluster Ticket Registry(CTR) manages the ticket which authorizes one of
 the cluster sites located in geographical distances to run certain
 resources. It is designed to be an add-on to Pacemaker and extend it to
 support geographical clustering.
 
 Definitions:
 Site: One pacemaker cluster (traditional cluster setup with SLE HA)
 which located in one site.
 Ticket: the right to run certain resources on a specific cluster.
 In pacemaker, the user can define what resources depend on which
 ticket.
 Arbitrator: the arbitrator which helps to make decision on ticket
 authorizing. It should be at a separate site and it is not the
 must. It exists only for quorum requirement.
 
 Background:
 In order to support Geo-clustering, we need a mechanism to guarantee the
 cluster resources will be highly available within different cluster
 sites. So we introduced the ticket which are bound to certain resources
 and treat it as a fail-over domain between cluster sites. As a matter of
 fact, we have setup an overlay cluster (the CTR) on top of traditional
 cluster. In the overlay cluster, ticket is like the resource and site is
 like the node. Unlike the traditional cluster, it is much simpler,
 ticket is the only type of resource and is a primitive, we don't need to
 clone it. And the communication between different sites is not like
 traditional cluster since it is over Internet.
 
 Usage:
 Each cluster site runs a CTR daemon with the role of "site". We
 configure it as a primitive resource in the pacemaker cluster, so it is
 managed by pacemaker and will be failovered within the cluster site.
 
 Each arbitrator runs a CTR daemon with the role of "arbitrator", we
 start and stop it by a script located in /etc/init.d.
 
 The user can run different commands to talk to the CTR daemon, the
 commands include list, grant, revoke, etc.
 list: list the current status, i.e. which site holds which tickets.
 grant: grant ticket to site.
 revoke: revoke ticket from site.
 
 Once a ticket is granted on a site, CTR will manage the ticket's
 liveness automatically, that is, if the site which holds the ticket is
 out of service, the ticket will be re-granted to another site.
 
 We need the arbitrator when there are two sites. Of course we can add
 more arbitrators to improve the reliability. The arbitrator is not a
 must when there are three or more sites. In brief, the number of sites
 and the number of arbitrators are not fixed, it can be configured
 according to the user's environment.
 
 Some detail on using Paxos:
 Paxos algorithm is implemented in the CTR and will be as a underlying
 layer to provide services to CTR. Why we need it? Since no matter how
 the distributed stack was designed, different sites need to reach
 consensus on the fact that "site A has the ticket B for now" in the
 distributed environment. Paxos is a good candidate on resolving this
 issue, especially when the communication between sites is over the
 geographical distances.
-->
<!--info by jjzhang-->
<!-- A Intro of Geo Cluster to Pacemaker
 
 - Terminolgy
 Site: A pacemaker cluster
 Ticket: Ticket can be granted to or revoked from a site. Resources can
 be configured to depend on ticket.
 Booth: Booth run as the the authentica source for ticket granting.
 
 - Site:
 We use 'site' instead of 'cluster' here to refer to a normal pacemaker
 based 'HA cluster'. 2 or more 'site' can be integrated via following
 method, which is called 'Geo Cluster' here.
 
 - Ticket:
 Ticket is a newly introduced CIB feature. It's stored in cib as a
 cluster status. Together with ticket, a set of new constraints is also
 introduced. Once a resource(s) depend on a ticket, it/they can only be
 started/promoted when ticket is granted to the cluster.
 
 <tickets>
  <instance_attributes id="status-tickets">
   <nvpair id="status-tickets-granted-ticket-ticketA"
    name="granted-ticket-ticketA" value="true"/>
   <nvpair id="status-tickets-last-granted-ticketA"
    name="last-granted-ticketA" value="1314246880"/>
  </instance_attributes>
 </tickets>
 
 <rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" role="Master"
  ticket="ticketA" loss-policy="stop"/>
 
 A ticket can be granted/revoked manually via following command.
 
 > crm_ticket -t ticketA -v true # grant ticketA to this site
 > crm_ticket -t ticketA -v false # revoke ticketA from this site.  
 
 Be warned, the administrator has to be very careful if he grant the
 ticket manually. The site itself cannot do any sanity check on this.
 
 - Deadman dependency:
 Once a ticket is revoked, while resource(s) depend on it is still
 running. It can be configured that pacemaker will STONITH the node(s)
 instead of stopping the resource(s) orderly.
 
 <rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" ticket="ticketA"
  loss-policy="fence"/>
 
 The loss-policy can be fence/stop/freeze/demote.
 
 - Booth:
 There should be one booth instance in each site. If the number of sites
 is even, additional booth can run in 'arbitrator' mode on a 3rd party
 node. The difference is that, booth in 'arbitrator' mode will never be
 granted tickets.
 
 * Configuration
 Booth need a pre-configured list of peers and tickets. It reads
 /etc/sysconfig/booth for that.
 
 transport="UDP"
 port="6666"
 arbitraotor="147.2.207.14"
 site="147.4.215.19"
 site="147.18.2.1"
 ticket="ticketA;1000"
 
 "ticketA;1200" means a ticket named 'ticketA', and expire every 1000
 seconds.
 A site already grant the ticket will renew the ticket every 800 seconds.
 
 * Setup
 Booth can be started on any node inside the site, so it makes sense to
 manage it as a primitive resource. Because booth require a persistent IP
 address for communication, it's better to group it with a float IP.
 
 primitive booth-ip ocf:heartbeat:IPaddr2 ip=147.2.207.14
 primitive booth ocf:pacemaker:boot-site
 group g-booth booth-ip booth
 
 * Running
 Start the resource group as normal. Booth save current ticket status in
 CIB, so it can restarted on any node. Try not to impose any constrains
 on booth itself.
 
 * Manually override
 booth support several client command.
 > booth client grant -t ticketA -s 147.2.207.14
 > booth client revoke -t ticketA -s 147.4.215.19  
 
 We encourage the administrator to set ticket via 'booth client'. This
 will make sure ticket is never granted twice among all sites.-->
</chapter>
