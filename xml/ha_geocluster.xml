<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!-- Fates #310356,  #311881, #311882 -->
<chapter id="cha.ha.geo">
 <title>Multi-site Clusters</title>
 <para>
  <remark>taroth 2011-09-19: FIXME, new chapter</remark> Apart from local
  &ha; clusters, &productname; also supports multi-site clusters. That
  means you can have multiple (geographical) sites with a local cluster each. As
  the different sites cannot communicate via unicast or multicast, the
  communication between the sites is done via Internet. Failover between these
  clusters is coordinated by a higher level
  entity.<!--taroth 2011-09-20: whom?-->Those factors differentiate a multi-site
  cluster from a <quote>stretched</quote> local cluster, where a cluster might
  spawn the whole campus in the city. But a <quote>stretched</quote> is still a
  single cluster, using unicast or multicast for communication between the nodes
  and managing failover internally.</para>
 <sect1 id="sec.ha.geo.challenges">
  <title>Challenges for Multi-site Clusters</title>
  <para>Typically, multi-site environments are also too far apart to support
   synchronous communication between the sites and synchronous data
   replication. That leads to the following challenges:</para>
  <itemizedlist>
   <listitem>
    <para>How to make sure that resources are only started once?</para></listitem>
   <listitem>
    <para>How to make sure that quorum can be reached between the different
     sites?</para></listitem>
   <listitem>
    <para>How to keep the CIB up-to-date on all nodes and sites?</para>
   </listitem>
   <listitem>
    <para>How to manage failover between the sites?</para>
   </listitem>
   <listitem>
    <para>How to deal with high latency in case of resources that need to be
     stopped?</para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.geo.concept">
  <title>Conceptual Overview</title>
  <para>Multi-site clusters can be considered as <quote>overlay</quote> clusters
   where each cluster site corresponds to a cluster node in a traditional
   cluster. The overlay cluster is managed by a mechanism called
    <literal>booth</literal> that guarantees that the cluster resources will be
   highly available within different cluster sites and takes care of the
   failover in case a site should be down. To do so, the overlay cluster makes
   use of tickets that are treated as failover domain between cluster sites. A
   ticket within an overlay cluster is similar to a resource in a traditional
   cluster. But in contrast to traditional clusters, tickets are the only type
   of resource in an overlay cluster. They are primitive resources that do not
   need to be cloned. <xref linkend="vl.ha.geo.components"/> explains the
   components and concepts introduced for multi-site clusters in more
   detail.</para>
    <para><remark>taroth 2011-09-20: todo - a conceptual
   graphic would be great here...</remark></para>
  <variablelist id="vl.ha.geo.components">
   <varlistentry>
    <!--taroth 2011-09-20: mail by jjzhang (ha-devel, 2011-07-26): ... we decide to 
     change the name "token" to "ticket" in order to avoid confusion with "totem token" 
     in corosync-->
    <term>Ticket</term>
    <listitem>
     <para>A ticket grants the right to run certain resources on a specific
      cluster. Tickets are stored in the CIB as a cluster status. <remark>taroth
       2011-09-20: todo: add example</remark> Resources can be bound to a
      certain ticket by dependencies. Only if the defined ticket is available at
      a site, the respective resources are started. Vice versa, if the ticket is
      removed, the resources depending on that ticket are automatically stopped.
     </para>
     <para>For automatic failover of resources, tickets are usually managed by
      the booth, but they can also be granted or revoked manually by the cluster
      administrator.</para>
     <!--<para>You can also think of the quorum as a special, cluster-wide ticket
      that is granted in case of node majority. The ticket would thus be similar
      to a <quote>site quorum</quote> and grant the permission to manage and own
      resources associated with that site. </para>
     --><remark>taroth 2011-09-20: not quite clear to me: seems there may be
      several tickets for a multi-site cluster? what do they look like? where do
      they come from: do you need to configure a ticket (like you configure a
      resource)? is there a place to "see" the tickets (GUI? HAWK)? or is it
      only possible to query for them with crm_ticket?</remark>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Booth</term>
    <listitem>
     <para>The instance managing the ticket distribution and thus, the failover
      process between the sites of a multi-site cluster. The participating
      clusters run a daemon/service (<systemitem class="resource"
       >boothd</systemitem>) that connect to the booth daemons running at the
      other sites. The daemons exchange information on their connectivity
      details and vote which site gets which ticket. Once a ticket is granted to
      a site, the booth will manage the ticket automatically: If the site which
      holds the ticket is out of service, the ticket will be granted to another
      site. But re-distribution of a ticket is only done after the booth has
      made sure that the ticket has been relinquished by the previous site.
      Sites which lose the vote (either explicitly or implicitly by being
      disconnected from the voting body) need to relinquish the ticket after a
      sane time-out (to protect against brief connection issues).
      <!--FIXME: See also deadman dependency.--></para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Arbitrator</term>
    <listitem>
     <para>Each site runs one booth instance that is responsible for
      communicating with the other sites. In case you have an even number of
      sites, you need an additional instance to reach consensus about granting
      or revoking tickets. For this case, you can add one or more booth
      instances on additional sites. As all booth instances communicate with
      each other, arbitrators help to make more reliable decisions about
      granting or revoking tickets. </para>
     <para>The arbitrators can consist of one-node clusters and do not run any
      resources apart from the booth daemon. A booth in arbitrator mode will
      never be granted tickets.</para>
     <para>An arbitrator is especially important for a
     two-site scenario: For example, if site <literal>A</literal> can no longer
     communicate with site <literal>B</literal>, there are
     two possible causes for that:</para>
     <itemizedlist>
      <listitem>
       <para>A network failure between <literal>A</literal> and
         <literal>B</literal>.</para></listitem>
      <listitem>
       <para>Site <literal>B</literal> is down.</para></listitem>
     </itemizedlist>
     <para>However, if site <literal>C</literal> (the arbitrator) can still
      communicate with site <literal>A</literal>, site <literal>A</literal> must
      still be up and running.</para></listitem>
   </varlistentry>
   <varlistentry>
    <term>Deadman Dependency</term>
    <listitem><para></para></listitem>
   </varlistentry>
   
   
  </variablelist>
  
 </sect1>
 
 <!--from 
   http://oss.clusterlabs.org/pipermail/pacemaker/2011-January/008878.html -->
<!--IntroductioN: At LPC 2010, we discussed (once more) that a key feature
 for pacemaker in 2011 would be improved support for multi-site clusters;
 by multi-site, we mean two (or more) sites with a local cluster each,
 and some higher level entity coordinating fail-over across these (as
 opposed to "stretched" clusters, where a single cluster might spawn the
 whole campus in the city).
 
 Typically, such multi-site environments are also too far apart to
 support synchronous communication/replication.
 
 There are several aspects to this that we discussed; Andrew and I first
 described and wrote this out a few years ago, so I hope he can remember
 the rest ;-)
 
 "Tokens" are, essentially, cluster-wide attributes (similar to node
  attributes, just for the whole partition). Via dependencies (similar to
  rsc_location), one can specify that certain resources require a specific
  token to be set before being started (and, vice versa, need to be
  stopped if the token is cleared). You could also think of our current
  "quorum" as a special, cluster-wide token that is granted in case of
  node majority.
 
 The token thus would be similar to a "site quorum"; i.e., the permission
 to manage/own resources associated with that site, which would be
 recorded in a rsc dependency. (It'd probably make a lot of sense if this
 would support resource sets, so one can easily list all the resources;
 also, some resources like m/s may tie their role to token ownership.)
 
 These tokens can be granted/revoked either manually (which I actually
 expect will be the default for the classic enterprise clusters), or via
 an automated mechanism described further below.
 
 
 Another aspect to site fail-over is recovery speed. A site can only
 activate the resources safely if it can be sure that the other site has
 deactivated them. Waiting for them to shutdown "cleanly" could incur
 very high latency (think "cascaded stop delays"). So, it would be
 desirable if this could be short-circuited. The idea between Andrew and
 myself was to introduce the concept of a "dead man" dependency; if the
 origin goes away, nodes which host dependent resources are fenced,
 immensely speeding up recovery.
 
 It seems to make most sense to make this an attribute of some sort for
 the various dependencies that we already have, possibly, to make this
 generally available. (It may also be something admins want to
 temporarily disable - i.e., for a graceful switch-over, they may not
 want to trigger the dead man process always.)
 
 
 The next bit is what we called the "Cluster Token Registry"; for those
 scenarios where the site switch is supposed to be automatic (instead of
 the admin revoking the token somewhere, waiting for everything to stop,
 and then granting it on the desired site). The participating clusters
 would run a daemon/service that would connect to each other, exchange
 information on their connectivity details (though conceivably, not mere
 majority is relevant, but also current ownership, admin weights, time
 of day, capacity ...), and vote on which site gets which token(s); a
 token would only be granted to a site once they can be sure that it has
 been relinquished by the previous owner, which would need to be
 implemented via a timer in most scenarios (see the dead man flag).
 
 Further, sites which lose the vote (either explicitly or implicitly by
 being disconnected from the voting body) would obviously need to perform
 said release after a sane time-out (to protect against brief connection
 issues).
 
 
 A final component is an idea to ease administration and management of
 such environments. The dependencies allow an automated tool to identify
 which resources are affected by a given token, and this could be
 automatically replicated (and possibly transformed) between sites, to
 ensure that all sites have an uptodate configuration of relevant
 resources. This would be handled by yet another extension, a CIB
 replicator service (that would either run permanently or explicitly when
 the admin calls it).
 
 Conceivably, the "inactive" resources may not even be present in the
 active CIB of sites which don't own the token (and be inserted once
 token ownership is established). This may be an (optional) interesting
 feature to keep CIB sizes under control.
-->
<!--info by jjzhang-->
<!-- Below is an introduction on "Cluster Ticket Registry":
 
 (Based on the discussion on the ML, we decide to change the name "token"
 to "ticket" in order to avoid confusion with "totem token" in corosync.)
 
 Overview:
 Cluster Ticket Registry(CTR) manages the ticket which authorizes one of
 the cluster sites located in geographical distances to run certain
 resources. It is designed to be an add-on to Pacemaker and extend it to
 support geographical clustering.
 
 Definitions:
 Site: One pacemaker cluster (traditional cluster setup with SLE HA)
 which located in one site.
 Ticket: the right to run certain resources on a specific cluster.
 In pacemaker, the user can define what resources depend on which
 ticket.
 Arbitrator: the arbitrator which helps to make decision on ticket
 authorizing. It should be at a separate site and it is not the
 must. It exists only for quorum requirement.
 
 Background:
 In order to support Geo-clustering, we need a mechanism to guarantee the
 cluster resources will be highly available within different cluster
 sites. So we introduced the ticket which are bound to certain resources
 and treat it as a fail-over domain between cluster sites. As a matter of
 fact, we have setup an overlay cluster (the CTR) on top of traditional
 cluster. In the overlay cluster, ticket is like the resource and site is
 like the node. Unlike the traditional cluster, it is much simpler,
 ticket is the only type of resource and is a primitive, we don't need to
 clone it. And the communication between different sites is not like
 traditional cluster since it is over Internet.
 
 Usage:
 Each cluster site runs a CTR daemon with the role of "site". We
 configure it as a primitive resource in the pacemaker cluster, so it is
 managed by pacemaker and will be failovered within the cluster site.
 
 Each arbitrator runs a CTR daemon with the role of "arbitrator", we
 start and stop it by a script located in /etc/init.d.
 
 The user can run different commands to talk to the CTR daemon, the
 commands include list, grant, revoke, etc.
 list: list the current status, i.e. which site holds which tickets.
 grant: grant ticket to site.
 revoke: revoke ticket from site.
 
 Once a ticket is granted on a site, CTR will manage the ticket's
 liveness automatically, that is, if the site which holds the ticket is
 out of service, the ticket will be re-granted to another site.
 
 We need the arbitrator when there are two sites. Of course we can add
 more arbitrators to improve the reliability. The arbitrator is not a
 must when there are three or more sites. In brief, the number of sites
 and the number of arbitrators are not fixed, it can be configured
 according to the user's environment.
 
 Some detail on using Paxos:
 Paxos algorithm is implemented in the CTR and will be as a underlying
 layer to provide services to CTR. Why we need it? Since no matter how
 the distributed stack was designed, different sites need to reach
 consensus on the fact that "site A has the ticket B for now" in the
 distributed environment. Paxos is a good candidate on resolving this
 issue, especially when the communication between sites is over the
 geographical distances.
--> 
<!--info by jjzhang-->
<!-- A Intro of Geo Cluster to Pacemaker
 
 - Terminolgy
 Site: A pacemaker cluster
 Ticket: Ticket can be granted to or revoked from a site. Resources can
 be configured to depend on ticket.
 Booth: Booth run as the the authentica source for ticket granting.
 
 - Site:
 We use 'site' instead of 'cluster' here to refer to a normal pacemaker
 based 'HA cluster'. 2 or more 'site' can be integrated via following
 method, which is called 'Geo Cluster' here.
 
 - Ticket:
 Ticket is a newly introduced CIB feature. It's stored in cib as a
 cluster status. Together with ticket, a set of new constraints is also
 introduced. Once a resource(s) depend on a ticket, it/they can only be
 started/promoted when ticket is granted to the cluster.
 
 <tickets>
  <instance_attributes id="status-tickets">
   <nvpair id="status-tickets-granted-ticket-ticketA"
    name="granted-ticket-ticketA" value="true"/>
   <nvpair id="status-tickets-last-granted-ticketA"
    name="last-granted-ticketA" value="1314246880"/>
  </instance_attributes>
 </tickets>
 
 <rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" role="Master"
  ticket="ticketA" loss-policy="stop"/>
 
 A ticket can be granted/revoked manually via following command.
 
 > crm_ticket -t ticketA -v true # grant ticketA to this site
 > crm_ticket -t ticketA -v false # revoke ticketA from this site.  
 
 Be warned, the administrator has to be very careful if he grant the
 ticket manually. The site itself cannot do any sanity check on this.
 
 - Deadman dependency:
 Once a ticket is revoked, while resource(s) depend on it is still
 running. It can be configured that pacemaker will STONITH the node(s)
 instead of stopping the resource(s) orderly.
 
 <rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" ticket="ticketA"
  loss-policy="fence"/>
 
 The loss-policy can be fence/stop/freeze/demote.
 
 - Booth:
 There should be one booth instance in each site. If the number of sites
 is even, additional booth can run in 'arbitrator' mode on a 3rd party
 node. The difference is that, booth in 'arbitrator' mode will never be
 granted tickets.
 
 * Configuration
 Booth need a pre-configured list of peers and tickets. It reads
 /etc/sysconfig/booth for that.
 
 transport="UDP"
 port="6666"
 arbitraotor="147.2.207.14"
 site="147.4.215.19"
 site="147.18.2.1"
 ticket="ticketA;1000"
 
 "ticketA;1200" means a ticket named 'ticketA', and expire every 1000
 seconds.
 A site already grant the ticket will renew the ticket every 800 seconds.
 
 * Setup
 Booth can be started on any node inside the site, so it makes sense to
 manage it as a primitive resource. Because booth require a persistent IP
 address for communication, it's better to group it with a float IP.
 
 primitive booth-ip ocf:heartbeat:IPaddr2 ip=147.2.207.14
 primitive booth ocf:pacemaker:boot-site
 group g-booth booth-ip booth
 
 * Running
 Start the resource group as normal. Booth save current ticket status in
 CIB, so it can restarted on any node. Try not to impose any constrains
 on booth itself.
 
 * Manually override
 booth support several client command.
 > booth client grant -t ticketA -s 147.2.207.14
 > booth client revoke -t ticketA -s 147.4.215.19  
 
 We encourage the administrator to set ticket via 'booth client'. This
 will make sure ticket is never granted twice among all sites.-->
</chapter>
