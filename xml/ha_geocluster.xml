<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!-- Fates #310356,  #311881, #311882 -->
<chapter id="cha.ha.geo">
 <title>Multi-site Clusters</title>
 <para>
  <remark>taroth 2011-09-19: FIXME, new chapter</remark> Apart from local
  &ha; clusters, &productname; also supports multi-site clusters. That
  means you can have multiple (geographical) sites with a local cluster each. As
  the different sites cannot communicate via unicast or multicast, the
  communication between the sites is done via Internet. Failover between these
  clusters is coordinated by a higher level
  entity.<!--taroth 2011-09-20: whom?-->Those factors differentiate a multi-site
  cluster from a <quote>stretched</quote> local cluster, where a cluster might
  spawn the whole campus in the city. But a <quote>stretched</quote> is still a
  single cluster, using unicast or multicast for communication between the nodes
  and managing failover internally.</para>
 <sect1 id="sec.ha.geo.challenges">
  <title>Challenges for Multi-site Clusters</title>
  <para>Typically, multi-site environments are also too far apart to support
   synchronous communication between the sites and synchronous data
   replication. That leads to the following challenges:</para>
  <itemizedlist>
   <listitem>
    <para>How to make sure that resources are only started once?</para></listitem>
   <listitem>
    <para>How to make sure that quorum can be reached between the different
     sites?</para></listitem>
   <listitem>
    <para>How to keep the CIB up-to-date on all nodes and sites?</para>
   </listitem>
   <listitem>
    <para>How to manage failover between the sites?</para>
   </listitem>
   <listitem>
    <para>How to deal with high latency in case of resources that need to be
     stopped?</para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.geo.concept">
  <title>Conceptual Overview</title>
  <para>Multi-site clusters can be considered as <quote>overlay</quote> clusters
   where each cluster site corresponds to a cluster node in a traditional
   cluster. The overlay cluster is managed by a mechanism called
    <literal>booth</literal> that guarantees that the cluster resources will be
   highly available within different cluster sites and takes care of the
   failover in case a site should be down. To do so, the overlay cluster makes
   use of tickets that are treated as failover domain between cluster sites. A
   ticket within an overlay cluster is similar to a resource in a traditional
   cluster. But in contrast to traditional clusters, tickets are the only type
   of resource in an overlay cluster and do not need to be configured. They are
   primitive resources that do not need to be cloned. <xref
    linkend="vl.ha.geo.components"/> explains the components and concepts
   introduced for multi-site clusters in more detail.</para>
    <para><remark>taroth 2011-09-20: todo - a conceptual
   graphic would be great here...</remark></para>
  <variablelist id="vl.ha.geo.components">
   <varlistentry>
    <!--taroth 2011-09-20: mail by jjzhang (ha-devel, 2011-07-26): ... we decide to 
     change the name "token" to "ticket" in order to avoid confusion with "totem token" 
     in corosync-->
    <term>Ticket</term>
    <listitem>
     <para>A ticket grants the right to run certain resources on a specific
      cluster. Initially, none of the sites has a ticket&mdash;tickets must
      initially be granted by the cluster administrator. After that, tickets are
      usually managed by the booth for automatic failover of resources, but they
      can also be granted or revoked manually by the cluster administrator.
      Tickets are stored in the CIB as a cluster status. With regards to a
      certain ticket, there are only two states for a site: with ticket or
      without ticket. Thus, the absence of a certain ticket (during the initial
      state) does not differ from the status after the ticket has been revoked:
      both are reflected by <literal>false</literal>.</para>
     <para>Resources can be bound to a certain ticket by dependencies. Only if
      the defined ticket is available at a site, the respective resources are
      started. Vice versa, if the ticket is removed, the resources depending on
      that ticket are automatically stopped.</para>
        </listitem>
   </varlistentry>
   <varlistentry>
    <term>Booth</term>
    <listitem>
     <para>The instance managing the ticket distribution and thus, the failover
      process between the sites of a multi-site cluster. The participating
      clusters run a daemon/service (<systemitem class="resource"
       >boothd</systemitem>) that connect to the booth daemons running at the
      other sites. The daemons exchange information on their connectivity
      details and vote which site gets which ticket. Once a ticket is granted to
      a site, the booth will manage the ticket automatically: If the site which
      holds the ticket is out of service, the ticket will be granted to another
      site. But re-distribution of a ticket is only done after the booth has
      made sure that the ticket has been relinquished by the previous site.
      Sites which lose the vote (either explicitly or implicitly by being
      disconnected from the voting body) need to relinquish the ticket after a
      sane time-out (to protect against brief connection issues).
      <!--FIXME: See also deadman dependency.--></para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Arbitrator</term>
    <listitem>
     <para>Each site runs one booth instance that is responsible for
      communicating with the other sites. In case you have an even number of
      sites, you need an additional instance to reach consensus. For this case,
      you can add one or more booth instances on additional sites. As all booth
      instances communicate with each other, arbitrators help to make more
      reliable decisions about granting or revoking tickets. </para>
     <para>The arbitrators can consist of one-node clusters and do not run any
      resources apart from the booth daemon. A booth in arbitrator mode will
      never be granted tickets.</para>
     <para>An arbitrator is especially important for a two-site scenario: For
      example, if site <literal>A</literal> can no longer communicate with site
       <literal>B</literal>, there are two possible causes for that:</para>
     <itemizedlist>
      <listitem>
       <para>A network failure between <literal>A</literal> and
         <literal>B</literal>.</para>
      </listitem>
      <listitem>
       <para>Site <literal>B</literal> is down.</para>
      </listitem>
     </itemizedlist>
     <para>However, if site <literal>C</literal> (the arbitrator) can still
      communicate with site <literal>A</literal>, site <literal>A</literal> must
      still be up and running.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Dead Man Dependency</term>
    <listitem>
     <para>After a ticket is revoked, it can take a long time until all
      resources depending on that ticket are stopped, especially in case of
      cascaded resources. To cut that process short, the cluster administrator
      can configure a <literal>loss-policy</literal> in Pacemaker for the case
      that a ticket gets revoked from a site. The loss-policy defines what
      happens to the resources that depend on that ticket. For example, the
      nodes that are hosting dependent resources can be fenced (which would
      considerably speed up the recovery process of the cluster).</para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.geo.req">
 <title>Requirements</title>
   <itemizedlist>
    <title>Software Requirements</title>
    <listitem>
     <para>The <systemitem class="resource">booth</systemitem> package must be
      installed on all cluster nodes and on all arbitrators that will be part of
      the multi-site clusters.</para>
    </listitem>
    <listitem>
     <para>
      <remark>taroth 2011-09-21: DEVs, is that correct? I guess we don't support
       any mixed clusters... </remark>All clusters that will be part of the
      multi-site cluster must be based on &productname;
      &productnumber;.</para>
    </listitem>
    <listitem>
     <para>
      <remark>taroth 2011-09-21: DEVs, what about the arbitrator? must it be
       configured as a (one-node) cluster? is it allowed to run any more
       resources than the boothd resource?</remark>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    <remark>taroth 2011-09-21: DEVs, are there any more prerequisites? e.g.
     network configuration? storage configuration?</remark>
   </para>
   <para>The most common scenario is probably a multi-site cluster with two
    sites and a third site running an arbitrator. However, technically, there
    are no limitations with regards to the number of sites and the number of
    arbitrators involved.</para>
   <para>Whereas the nodes belonging to the same cluster site should be
    synchronized via NTP, this is not required between the individual cluster
    sites.</para>
  </sect1>
  <sect1 id="sec.ha.geo.setup">
   <title>Basic Setup</title>
   <para>Configuring a multi-site cluster takes the following basic
    steps:</para>
   <variablelist>
    <varlistentry>
     <term>Configuring Cluster Resources and Constraints</term>
     <listitem>
      <orderedlist>
       <listitem>
        <!--ticket dependencies-->
        <para>
         <xref linkend="pro.ha.geo.setup.rsc.constraints" xrefstyle="select:title"/>
        </para>
       </listitem>
       <listitem>
        <!--configuring a resource group for each site's/arbitrartors boothd-->
        <para><xref linkend="pro.ha.geo.setup.rsc.boothd" xrefstyle="select:title"/></para>
       </listitem>
      </orderedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Editing the Booth Configuration File</term>
     <listitem>
      <para/>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Starting Booth and Granting Tickets</term>
     <listitem>
      <para/>
     </listitem>
    </varlistentry>

   </variablelist>
  <sect2 id="sec.ha.geo.setup.resources">
   <title>Configuring Cluster Resources and Constraints</title>
   <para><!--FIXME: add some blurb--></para>
   <procedure id="pro.ha.geo.setup.rsc.constraints">
    <title>Configuring Ticket Dependencies</title>
    <para>The <command>crm configure rsc_ticket</command> command lets you
     specify the resources depending on a certain ticket and set a
      <literal>loss-policy</literal> that defines what should happen to the
     respective resources if the ticket is revoked. The attribute
      <literal>loss-policy</literal> can have the following values:
      <literal>fence</literal>, <literal>stop</literal>,
      <literal>freeze</literal>, <literal>demote</literal>.<remark>taroth
      2011-09-27: DEVs, what happens for "demote"?</remark></para>
    <step>
     <para>On one of the cluster nodes, start a shell and log in as
      &rootuser; or equivalent. </para>
    </step>
    <step>
     <para>Enter <command>crm configure</command> to switch to the interactive
      shell.</para>
    </step>
    <step id="step.ha.geo.setup.rsc.constraints">
     <para>Configure a constraint that defines which resources depend on a
      certain ticket. For example, the following command creates a constraint
      with the ID <literal>rsc1-req-ticketA</literal>. It defines that the
      resource <literal>rsc1</literal> depends on <literal>ticketA</literal> and
      that the node running the resource should be fenced in case
       <literal>ticketA</literal> is revoked.</para>
     <screen>rsc_ticket rsc1-req-ticketA ticketA: rsc1:Master loss-policy="fence" </screen>
    </step>
    <step>
     <para>If you want other resources to depend on further tickets, create as
      many constraints as necessary with <command>rsc_ticket</command>.</para>
    </step>
    <step>
     <para> Review your changes with <command>show</command>. </para>
    </step>
    <step>
     <para>If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.</para>
     <para>The constraints are saved to the CIB. For example, the command in
       <xref linkend="step.ha.geo.setup.rsc.constraints"/> would result in the
      following constraint configuration in the CIB:</para>
     <screen>&lt;rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" role="Master" ticket="ticketA" loss-policy="fence"/></screen>
    </step>
   </procedure>
   <remark>taroth 2011-09-26: is there a place to "see" the tickets (GUI? HAWK)?
   or is it only possible to query for them with crm_ticket?</remark>
   <procedure id="pro.ha.geo.setup.rsc.boothd">
    <title>Configuring a Resource Group For <systemitem class="daemon"
      >boothd</systemitem></title>
    <para>Each site and arbitrator needs to run one instance of <systemitem
      class="daemon">boothd</systemitem>. The daemon can be started on any node,
     therefore it should be configured as primitive resource. As the daemons
     need a persistent IP address to communicate with each other, configure
     another primitive that manages the IP address for each booth daemon and
     group booth primitives:</para>
    <step>
     <para>On one of the cluster nodes, start a shell and log in as
      &rootuser; or equivalent. </para>
    </step>
    <step>
     <para>Enter <command>crm configure</command> to switch to the interactive
      shell.</para>
    </step>
    <step>
     <para>Enter the following commands to create both primitive resources and
      to add them to one group, <literal>g-booth</literal>:</para>
     <screen>primitive booth-ip ocf:heartbeat:IPaddr2 ip=<replaceable>IP_ADDRESS</replaceable>
primitive booth ocf:pacemaker:boot-site
group g-booth booth-ip booth</screen>
    </step>
    <step>
     <para>Review your changes with <command>show</command>. </para>
    </step>
    <step>
     <para>If everything is correct, submit your changes with
      <command>commit</command> and leave the crm live configuration with
      <command>exit</command>.</para>
    </step>
    <step>
     <para>Configure a similar group with a different IP address for each
      cluster site and for each arbitrator.</para>
     <para>With this configuration, each booth daemon will be available at its
      individual IP address, independent of the node the daemon is running on.
     </para>
    </step>
   </procedure>
  </sect2>
   <sect2 id="sec.ha.geo.setup.boothconfig">
    <title>Editing the Booth Configuration File</title>
    <para>After having configured the resource group for the 
    <systemitem class="daemon">boothd</systemitem>, edit the
    booth configuration file
     <filename>/etc/sysconfig/booth.config</filename>
    <remark>taroth 2011-09-28: DEVs, what's the correct filename booth (as cited
     in the mail on ha-devel or booth.conf as mentioned by jjzhang while in
     Nuremberg</remark>to complete booth setup:</para>
    <example>
     <title>Example Booth Configuration File</title>
<screen>transport="UDP" <co id="co.ha.geo.boothconfig.transport"/>
port="6666" <co id="co.ha.geo.boothconfig.port"/>
arbitrator="147.2.207.14" <co id="co.ha.geo.boothconfig.arbitartor"/>
site="147.4.215.19" <co id="co.ha.geo.boothconfig.site"/>
site="147.18.2.1"  <xref linkend="co.ha.geo.boothconfig.site" xrefstyle="select:label nopage"/>
ticket="ticketA;1000" <co id="co.ha.geo.boothconfig.ticket"/></screen>
</example>
    
   </sect2>
   
   
   <!--<sect2>
    <title>Editing the Booth Configuration File</title>
    <procedure id="pro.ha.geo.setup.booth">
     <title>Setting Up the Booth</title>
     <step>
      <para>Make sure that the <systemitem class="resource">booth</systemitem>
       package is installed on all cluster nodes and on all arbitrators that
       will be part of the multi-site clusters.</para>
     </step>
     <step>
      <para>On a node or an arbitrator, create
        <filename>/etc/sysconfig/booth.conf</filename> with the following
       contents:</para>
      <screen/>
     </step>
    </procedure>
   </sect2>
   -->
  
 </sect1>
 <sect1 id="sec.ha.geo.manage">
  <title>Managing Multi-site Clusters</title>
  <para></para>
 </sect1>
 
 
 <!--from 
   http://oss.clusterlabs.org/pipermail/pacemaker/2011-January/008878.html -->
<!--IntroductioN: At LPC 2010, we discussed (once more) that a key feature
 for pacemaker in 2011 would be improved support for multi-site clusters;
 by multi-site, we mean two (or more) sites with a local cluster each,
 and some higher level entity coordinating fail-over across these (as
 opposed to "stretched" clusters, where a single cluster might spawn the
 whole campus in the city).
 
 Typically, such multi-site environments are also too far apart to
 support synchronous communication/replication.
 
 There are several aspects to this that we discussed; Andrew and I first
 described and wrote this out a few years ago, so I hope he can remember
 the rest ;-)
 
 "Tokens" are, essentially, cluster-wide attributes (similar to node
  attributes, just for the whole partition). Via dependencies (similar to
  rsc_location), one can specify that certain resources require a specific
  token to be set before being started (and, vice versa, need to be
  stopped if the token is cleared). You could also think of our current
  "quorum" as a special, cluster-wide token that is granted in case of
  node majority.
 
 The token thus would be similar to a "site quorum"; i.e., the permission
 to manage/own resources associated with that site, which would be
 recorded in a rsc dependency. (It'd probably make a lot of sense if this
 would support resource sets, so one can easily list all the resources;
 also, some resources like m/s may tie their role to token ownership.)
 
 These tokens can be granted/revoked either manually (which I actually
 expect will be the default for the classic enterprise clusters), or via
 an automated mechanism described further below.
 
 
 Another aspect to site fail-over is recovery speed. A site can only
 activate the resources safely if it can be sure that the other site has
 deactivated them. Waiting for them to shutdown "cleanly" could incur
 very high latency (think "cascaded stop delays"). So, it would be
 desirable if this could be short-circuited. The idea between Andrew and
 myself was to introduce the concept of a "dead man" dependency; if the
 origin goes away, nodes which host dependent resources are fenced,
 immensely speeding up recovery.
 
 It seems to make most sense to make this an attribute of some sort for
 the various dependencies that we already have, possibly, to make this
 generally available. (It may also be something admins want to
 temporarily disable - i.e., for a graceful switch-over, they may not
 want to trigger the dead man process always.)
 
 
 The next bit is what we called the "Cluster Token Registry"; for those
 scenarios where the site switch is supposed to be automatic (instead of
 the admin revoking the token somewhere, waiting for everything to stop,
 and then granting it on the desired site). The participating clusters
 would run a daemon/service that would connect to each other, exchange
 information on their connectivity details (though conceivably, not mere
 majority is relevant, but also current ownership, admin weights, time
 of day, capacity ...), and vote on which site gets which token(s); a
 token would only be granted to a site once they can be sure that it has
 been relinquished by the previous owner, which would need to be
 implemented via a timer in most scenarios (see the dead man flag).
 
 Further, sites which lose the vote (either explicitly or implicitly by
 being disconnected from the voting body) would obviously need to perform
 said release after a sane time-out (to protect against brief connection
 issues).
 
 
 A final component is an idea to ease administration and management of
 such environments. The dependencies allow an automated tool to identify
 which resources are affected by a given token, and this could be
 automatically replicated (and possibly transformed) between sites, to
 ensure that all sites have an uptodate configuration of relevant
 resources. This would be handled by yet another extension, a CIB
 replicator service (that would either run permanently or explicitly when
 the admin calls it).
 
 Conceivably, the "inactive" resources may not even be present in the
 active CIB of sites which don't own the token (and be inserted once
 token ownership is established). This may be an (optional) interesting
 feature to keep CIB sizes under control.
-->
<!--info by jjzhang-->
<!-- Below is an introduction on "Cluster Ticket Registry":
 
 (Based on the discussion on the ML, we decide to change the name "token"
 to "ticket" in order to avoid confusion with "totem token" in corosync.)
 
 Overview:
 Cluster Ticket Registry(CTR) manages the ticket which authorizes one of
 the cluster sites located in geographical distances to run certain
 resources. It is designed to be an add-on to Pacemaker and extend it to
 support geographical clustering.
 
 Definitions:
 Site: One pacemaker cluster (traditional cluster setup with SLE HA)
 which located in one site.
 Ticket: the right to run certain resources on a specific cluster.
 In pacemaker, the user can define what resources depend on which
 ticket.
 Arbitrator: the arbitrator which helps to make decision on ticket
 authorizing. It should be at a separate site and it is not the
 must. It exists only for quorum requirement.
 
 Background:
 In order to support Geo-clustering, we need a mechanism to guarantee the
 cluster resources will be highly available within different cluster
 sites. So we introduced the ticket which are bound to certain resources
 and treat it as a fail-over domain between cluster sites. As a matter of
 fact, we have setup an overlay cluster (the CTR) on top of traditional
 cluster. In the overlay cluster, ticket is like the resource and site is
 like the node. Unlike the traditional cluster, it is much simpler,
 ticket is the only type of resource and is a primitive, we don't need to
 clone it. And the communication between different sites is not like
 traditional cluster since it is over Internet.
 
 Usage:
 Each cluster site runs a CTR daemon with the role of "site". We
 configure it as a primitive resource in the pacemaker cluster, so it is
 managed by pacemaker and will be failovered within the cluster site.
 
 Each arbitrator runs a CTR daemon with the role of "arbitrator", we
 start and stop it by a script located in /etc/init.d.
 
 The user can run different commands to talk to the CTR daemon, the
 commands include list, grant, revoke, etc.
 list: list the current status, i.e. which site holds which tickets.
 grant: grant ticket to site.
 revoke: revoke ticket from site.
 
 Once a ticket is granted on a site, CTR will manage the ticket's
 liveness automatically, that is, if the site which holds the ticket is
 out of service, the ticket will be re-granted to another site.
 
 We need the arbitrator when there are two sites. Of course we can add
 more arbitrators to improve the reliability. The arbitrator is not a
 must when there are three or more sites. In brief, the number of sites
 and the number of arbitrators are not fixed, it can be configured
 according to the user's environment.
 
 Some detail on using Paxos:
 Paxos algorithm is implemented in the CTR and will be as a underlying
 layer to provide services to CTR. Why we need it? Since no matter how
 the distributed stack was designed, different sites need to reach
 consensus on the fact that "site A has the ticket B for now" in the
 distributed environment. Paxos is a good candidate on resolving this
 issue, especially when the communication between sites is over the
 geographical distances.
--> 
<!--info by jjzhang-->
<!-- A Intro of Geo Cluster to Pacemaker
 
 - Terminolgy
 Site: A pacemaker cluster
 Ticket: Ticket can be granted to or revoked from a site. Resources can
 be configured to depend on ticket.
 Booth: Booth run as the the authentica source for ticket granting.
 
 - Site:
 We use 'site' instead of 'cluster' here to refer to a normal pacemaker
 based 'HA cluster'. 2 or more 'site' can be integrated via following
 method, which is called 'Geo Cluster' here.
 
 - Ticket:
 Ticket is a newly introduced CIB feature. It's stored in cib as a
 cluster status. Together with ticket, a set of new constraints is also
 introduced. Once a resource(s) depend on a ticket, it/they can only be
 started/promoted when ticket is granted to the cluster.
 
 <tickets>
  <instance_attributes id="status-tickets">
   <nvpair id="status-tickets-granted-ticket-ticketA"
    name="granted-ticket-ticketA" value="true"/>
   <nvpair id="status-tickets-last-granted-ticketA"
    name="last-granted-ticketA" value="1314246880"/>
  </instance_attributes>
 </tickets>
 
 <rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" role="Master"
  ticket="ticketA" loss-policy="stop"/>
 
 A ticket can be granted/revoked manually via following command.
 
 > crm_ticket -t ticketA -v true # grant ticketA to this site
 > crm_ticket -t ticketA -v false # revoke ticketA from this site.  
 
 Be warned, the administrator has to be very careful if he grant the
 ticket manually. The site itself cannot do any sanity check on this.
 
 - Deadman dependency:
 Once a ticket is revoked, while resource(s) depend on it is still
 running. It can be configured that pacemaker will STONITH the node(s)
 instead of stopping the resource(s) orderly.
 
 <rsc_ticket id="rsc1-req-ticketA" rsc="rsc1" ticket="ticketA"
  loss-policy="fence"/>
 
 The loss-policy can be fence/stop/freeze/demote.
 
 - Booth:
 There should be one booth instance in each site. If the number of sites
 is even, additional booth can run in 'arbitrator' mode on a 3rd party
 node. The difference is that, booth in 'arbitrator' mode will never be
 granted tickets.
 
 * Configuration
 Booth need a pre-configured list of peers and tickets. It reads
 /etc/sysconfig/booth for that.
 
 transport="UDP"
 port="6666"
 arbitraotor="147.2.207.14"
 site="147.4.215.19"
 site="147.18.2.1"
 ticket="ticketA;1000"
 
 "ticketA;1200" means a ticket named 'ticketA', and expire every 1000
 seconds.
 A site already grant the ticket will renew the ticket every 800 seconds.
 
 * Setup
 Booth can be started on any node inside the site, so it makes sense to
 manage it as a primitive resource. Because booth require a persistent IP
 address for communication, it's better to group it with a float IP.
 
 primitive booth-ip ocf:heartbeat:IPaddr2 ip=147.2.207.14
 primitive booth ocf:pacemaker:boot-site
 group g-booth booth-ip booth
 
 * Running
 Start the resource group as normal. Booth save current ticket status in
 CIB, so it can restarted on any node. Try not to impose any constrains
 on booth itself.
 
 * Manually override
 booth support several client command.
 > booth client grant -t ticketA -s 147.2.207.14
 > booth client revoke -t ticketA -s 147.4.215.19  
 
 We encourage the administrator to set ticket via 'booth client'. This
 will make sure ticket is never granted twice among all sites.-->
</chapter>
