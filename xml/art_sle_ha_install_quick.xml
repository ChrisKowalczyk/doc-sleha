<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
  type="text/xml" 
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>

<?provo dirname="install_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art.ha.install.quick"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
<?suse-quickstart columns="no" version="2"?>
  <!--https://fate.suse.com/320823: [DOC] HA quick start document-->
  <title>&instquick;</title>
 <subtitle>&productname; &productnumber;</subtitle>
 <info>
      <productnumber>&productnumber;</productnumber>
      <productname>&productname;</productname>
      <abstract>
       <para>
        This document shows you how to install &productname; and how to set up a
        very basic cluster automatically, using the bootstrap scripts provided
        by the <systemitem class="resource">ha-cluster-bootstrap</systemitem>
        package. The automatic setup includes using the SBD fencing mechanism
        to protect your shared storage.
       </para>
      </abstract>
      <dm:docmanager>
        <dm:bugtracker>
          <dm:url>https://bugzilla.suse.com/enter_bug.cgi</dm:url>
          <dm:product>SUSE Linux Enterprise High Availability Extension 12 SP2</dm:product>
          <dm:component>Documentation</dm:component>
        </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
    </info>

  <sect1 xml:id="sec.ha.inst.quick.intro">
   <title>Introduction</title>
   <remark>taroth 2016-07-08: FIXME: add more details about the setup later on</remark>
   <remark>toms 2016-07-13: What about target group(s)?</remark>
   <para>
    Following the steps will lead to a minimal two node cluster setup. Use it
    for testing or as a basic configuration that you can extended and adjust
    later on, according to your requirements for using an &ha; cluster in a
    production environment.
   </para>
   <para>
    Find more documentation for this product at <link
     xlink:href="http://www.suse.com/documentation/sle-ha-12"/>. The
    documentation also includes a comprehensive &admin; for
    &productname;. Refer to it for further configuration and administration
    tasks.
   </para>
  </sect1>

  <sect1 xml:id="sec.ha.inst.quick.usage-scenario">
   <title>Usage Scenario and Workflow</title>
   <para>
    This guide assumes that you want to create a high availability cluster
    with the following properties:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Two nodes, &node1; and &node2;, connected to each other via network.
     </para>
    </listitem>
    <listitem>
     <para>
      Failover from one to the other if the active host breaks down (also
      named an <emphasis>active/passive</emphasis> setup).
     </para>
    </listitem>
    <listitem>
     <para>
      a floating, virtual IP address which allows clients to connect to the
      service no matter which physical node it is running on.
     </para>
    </listitem>
   </itemizedlist>

   <remark>toms 2016-07-15: Not sure if we should split this section into two:
   "Usage Scenario" and "Workflow".
   </remark>

   <para>
    To setup a basic HA cluster, the following steps are described in this
    document:
   </para>
   <orderedlist>
    <listitem>
     <para>
      Installation of &sls; and &ha; extension on all of your nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      Automatic cluster setup with the ha-cluster-bootstrap scripts.
      This leads to a one-node cluster.
     </para>
    </listitem>
    <listitem>
     <remark>toms 2016-07-15: Hmn, not sure if we combine this step and the
     last step into a general  "Creation of the two-node cluster".
     Thoughts?</remark>
     <para>
      Addition of a node to your existing cluster. After this step,
      you have a two-node cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      Configuration of virtual IP address and &stonith; resource
     </para>
    </listitem>
   </orderedlist>
  </sect1>

  <sect1 xml:id="sec.ha.inst.quick.installation">
    <title>Installation as Extension</title>
    <remark>toms 2016-07-14: @taroth: should we have a different title?
     Something like "Prerequisites and Installation"? It would be consistent
     with the NFS Quick Start Guide.
    </remark>
    <para>
      The packages needed for configuring and managing a cluster with the
      &hasi; are included in the <literal>&ha;</literal> installation
      pattern. This pattern is only available after &productname; has been
      installed as an extension to &slsreg;. For information on how to install
      extensions, see the <citetitle>&sle; &productnumber;
        &deploy;</citetitle>, available at
      <link xlink:href="http://www.suse.com/doc"/>. Refer to chapter
      <citetitle>Installing Modules, Extensions, and Third Party Add-On Products</citetitle>.
      <!--taroth: need to use hard-coded link here as the target is not included in the same set-->
    </para>

    <procedure xml:id="pro.ha.inst.quick.pattern">
      <title>Installing the &ha; Pattern</title>
      <step>
        <para>
          Start &yast; as &rootuser; user and select <menuchoice>
            <guimenu>Software</guimenu> <guimenu>Software Management</guimenu>
          </menuchoice>.
        </para>
        <para>
          Alternatively, start the &yast; module as &rootuser; on a command
          line with <command>yast2&nbsp;sw_single</command>.
        </para>
      </step>
      <step>
        <para>
          From the <guimenu>Filter</guimenu> list, select
          <guimenu>Patterns</guimenu> and activate the <guimenu>High
            Availability</guimenu> pattern in the pattern list.
          <!--SLE HA GEO: <guimenu>GEO Clustering for High Availability</guimenu>-->
        </para>
      </step>
      <step>
        <para>
          Click <guimenu>Accept</guimenu> to start installing the packages.
        </para>
        <note>
          <title>Installing Software Packages on All Parties</title>
          <para>
            The software packages needed for &ha; clusters are
            <emphasis>not</emphasis> automatically copied to the cluster nodes.
          </para>
        </note>
      </step>
      <step>
        <para>
          Install the &ha; pattern on <emphasis>all</emphasis> machines that
          will be part of your cluster.
        </para>
        <para>
          If you do not want to install &sls; &productnumber; and
          &productname; &productnumber; manually on all nodes that will be
          part of your cluster, use &ay; to clone existing nodes. For more
          information, refer to <remark>FIXME Install Quick</remark>
          <!--<xref linkend="sec.ha.installation.autoyast"/>-->.
        </para>
      </step>
    </procedure>
  </sect1>
  <sect1 xml:id="sec.ha.inst.quick.setup.auto">
  <title>Automatic Cluster Setup (ha-cluster-bootstrap)</title>

  <para>
   The <systemitem class="resource">ha-cluster-bootstrap</systemitem>
   package provides everything you need to get a one-node cluster up and
   running, to make other nodes join, and to remove nodes from an existing
   cluster:
  </para>

  <variablelist xml:id="vl.ha.inst.quick.common.cmds">
   <title>Common Commands</title>
   <varlistentry>
    <term><xref linkend="pro.ha.inst.quick.setup.ha-cluster-init" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      With <command>ha-cluster-init</command>, define the basic parameters
      needed for cluster communication and (optionally) set up a
      &stonith; mechanism to protect your shared storage. This leaves you
      with a running one-node cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="pro.ha.inst.quick.setup.ha-cluster-join" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      With <command>ha-cluster-join</command>, add more nodes to your
      cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="pro.ha.inst.quick.setup.ha-cluster-remove" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      With <command>ha-cluster-remove</command>, remove nodes from your
      cluster.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.ha.inst.quick.setup.1st-node">
  <title>Setting Up the First Node</title>
  <para>
   All commands from <xref linkend="vl.ha.inst.quick.common.cmds"/> execute
   bootstrap scripts that require only a minimum of
   time and manual intervention. The bootstrap scripts for initialization
   and joining automatically open the ports in the firewall that are needed
   for cluster communication. The configuration is written to
   <filename>/etc/sysconfig/SuSEfirewall2.d/services/cluster</filename>. Any
   options set during the bootstrap process can be modified later with the
   &yast; cluster module.
  </para>

  <para>
   Before starting the automatic setup, make sure that the following
   prerequisites are fulfilled on all nodes that will participate in the
   cluster:
  </para>

  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     The requirements listed in <remark>FIXME Install Quick</remark> <!--<xref linkend="sec.ha.requirements.sw"/>--> and
     <remark>FIXME Install Quick</remark> <!--<xref linkend="sec.ha.requirements.other"/>--> are fulfilled.
    </para>
   </listitem>
   <listitem>
    <para>
     The <systemitem class="resource">ha-cluster-bootstrap</systemitem>
     package is installed.
    </para>
   </listitem>
   <listitem>
    <para>
     The network is configured according to your needs. For example, a
     private network is available for cluster communication and network
     device bonding is configured. For information on bonding, refer to
     <remark>FIXME Install Quick</remark> <!--<xref linkend="cha.ha.netbonding"/>-->.
    </para>
   </listitem>
   <listitem>
    <para>
     If you want to use SBD for your shared storage, you need one shared
     block device for SBD. The block device need not be formatted. In
     addition, you will need a suitable hardware watchdog device. For more
     information, refer to <remark>FIXME Install Quick</remark> <!--<xref linkend="cha.ha.storage.protect"/>-->.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes must be able to see the shared storage via the same paths
     (<filename>/dev/disk/by-path/...</filename> or
     <filename>/dev/disk/by-id/...</filename>).
    </para>
   </listitem>
  </itemizedlist>

  <procedure xml:id="pro.ha.inst.quick.setup.ha-cluster-init">
   <title>Automatically Setting Up the First Node</title>
   <para> The <command>ha-cluster-init</command> command checks for
    configuration of NTP and guides you through configuration of the cluster
    communication layer (&corosync;). Optionally, it lets you configure the
    following:
   </para>
   <variablelist>
    <varlistentry>
     <term>SBD</term>
     <listitem>
      <para>A fencing mechanism to protect your shared storage. For details on
       SBD, see <remark>FIXME Install Quick</remark> <!--<xref linkend="cha.ha.storage.protect"/>-->.</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Virtual IP</term>
     <listitem>
      <para>A virtual IP address for &hawk2;. For details on &hawk2;, see
        <remark>FIXME Install Quick</remark> <!--<xref linkend="cha.conf.hawk2"/>-->.</para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Shared Storage with OCFS2</term>
     <listitem><!--taroth 2014-07-02: https://bugzilla.novell.com/show_bug.cgi?id=821123-->
      <para>When running the script with the <option>-t</option> option, it can
       perform additional cluster configuration based on templates. For example,
       <command>ha-cluster-init -t ocfs2</command> will partition some shared
       storage into two pieces: 1 MB for SBD, and the remainder for OCFS2. </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>For details about the script's range of functions, its options,
    and an overview of the files it can create and modify, refer to the
    <command>ha-cluster-init</command> man page.
   </para>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine you want to
     use as cluster node.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing
    </para>
<screen>&prompt.root;<command>ha-cluster-init</command></screen>
    <para>
     If NTP has not been configured to start at boot time, a message
     appears. The script also checks for a hardware watchdog device (which
     is important in case you want to configure SBD) and warns you if none
     is present.
    </para>
    <para>
     If you decide to continue anyway, the script will automatically
     generate keys for SSH access and for the &csync; synchronization
     tool and start the services needed for both.
    </para>
   </step>
   <step>
    <para>
     To configure the cluster communication layer (&corosync;):
    </para>
    <substeps>
     <step>
      <para>
       Enter a network address to bind to. By default, the script will
       propose the network address of <systemitem>eth0</systemitem>.
       Alternatively, enter a different network address, for example the
       address of <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast address. The script proposes a random address that
       you can use as default.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast port. The script proposes <literal>5405</literal>
       as default.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To configure SBD (optional), enter a persistent path to the partition
     of your block device that you want to use for SBD. The path must be
     consistent across all nodes in the cluster.
    </para>
    <!--taroth 2015-07-24: NEW FEATURE: Configure Administration IP Address:
     Optionally configure an administration virtual IP
     address. The purpose of this IP address is to
     provide a single IP that can be used to interact
     with the cluster, rather than using the IP address
     of any specific cluster node.
    -->
    <para>
     Finally, the script will start the &pace; service to bring the
     one-node cluster online and enable the Web management interface
     &hawk2;. The URL to use for &hawk2; is displayed on the screen.
    </para>
   </step>
   <!-- taroth 2015-09-22: fate#318549: cluster-init: configure virtual IP for HAWK  -->
   <step>
    <para>To configure a virtual IP address for cluster administration, enter
      <literal>yes</literal> when prompted and enter an unused IP address that
     you want to use as administration IP for the cluster. Instead of logging in
     to an individual cluster node with &hawk2;, you can then connect to the
     virtual IP address.</para>
   </step>
   <step>
    <para>
     For any details of the setup process, check
     <filename>/var/log/ha-cluster-bootstrap.log</filename>.
    </para>
   </step>
  </procedure>

  <para>
   You now have a running one-node cluster. Check the cluster status with
   <command>crm&nbsp;status</command>:
  </para>

<screen>&prompt.root;crm status
   Last updated: Thu Jul  3 11:04:10 2014
   Last change: Thu Jul  3 10:58:43 2014
   Current DC: alice (175704363) - partition with quorum
   1 Nodes configured
   0 Resources configured

   Online: [ alice ]</screen>

  <important>
   <title>Secure Password</title>
   <para>
    The bootstrap procedure creates a Linux user named
    <systemitem class="username">hacluster</systemitem> with the password
    <literal>linux</literal>. You need it for logging in to &hawk2;.
    Replace the default password with a secure one as soon as possible:
   </para>
<screen>&prompt.root;<command>passwd</command> hacluster</screen>
  </important>
 </sect1>

 <sect1 xml:id="sec.ha.inst.quick.setup.add-nodes">
  <title>Adding Nodes to an Existing Cluster</title>
  <para>
   To add a new node to your existing cluster, proceed as follows:
  </para>
  <procedure xml:id="pro.ha.inst.quick.setup.ha-cluster-join">
   <title>Adding Nodes to an Existing Cluster</title>
   <para>
    If you have a cluster up and running (with one or more nodes), add more
    cluster nodes with the <command>ha-cluster-join</command> bootstrap
    script. The script only needs access to an existing cluster node and
    will complete the basic setup on the current machine automatically.
    Follow the steps below. For details, refer to the
    <command>ha-cluster-join</command> man page.
   </para>
   <para>
    If you have configured the existing cluster nodes with the &yast;
    cluster module, make sure the following prerequisites are fulfilled
    before you run <command>ha-cluster-join</command>:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      The &rootuser; user on the existing nodes has SSH keys in place for
      passwordless login.
     </para>
    </listitem>
    <listitem>
     <para>
      &csync; is configured on the existing nodes. For details, refer to
      <remark>FIXME Install Quick</remark> <!--<xref linkend="pro.ha.installation.setup.csync2.yast"/>-->.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If you are logged in to the first node via &hawk2;, you can follow the
    changes in cluster status and view the resources being activated in the
    Web interface.
   </para>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine supposed to
     join the cluster.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
<screen>&prompt.root;<command>ha-cluster-join</command></screen>
    <para>
     If NTP has not been configured to start at boot time, a message
     appears. The script also checks for a hardware watchdog device (which
     is important in case you want to configure SBD) and warns you if none
     is present.
    </para>
   </step>
   <step>
    <para>
     If you decide to continue anyway, you will be prompted for the IP
     address of an existing node. Enter the IP address.
    </para>
   </step>
   <step>
    <para>
     If you have not already configured a passwordless SSH access between
     both machines, you will also be prompted for the &rootuser; password
     of the existing node.
    </para>
    <para>
     After logging in to the specified node, the script will copy the
     &corosync; configuration, configure SSH and &csync;, and will
     bring the current machine online as new cluster node. Apart from that,
     it will start the service needed for &hawk2;. If you have configured
     shared storage with OCFS2, it will also automatically create the mount
     point directory for the OCFS2 file system.
    </para>
   </step>
   <step>
    <para>
     Repeat the steps above for all machines you want to add to the cluster.
    </para>
   </step>
   <step>
    <para>
     For details of the process, check
     <filename>/var/log/ha-cluster-bootstrap.log</filename>.
    </para>
   </step>
  </procedure>

  <para>
   Check the cluster status with <command>crm&nbsp;status</command>. If
   you have successfully added a second node, the output will be similar to
   the following:
  </para>

<screen>&prompt.root;crm status
   Last updated: Thu Jul  3 11:07:10 2014
   Last change: Thu Jul  3 10:58:43 2014
   Current DC: alice (175704363) - partition with quorum
   2 Nodes configured
   0 Resources configured

   Online: [ alice bob ]</screen>

  <important>
   <title>Check <systemitem>no-quorum-policy</systemitem></title>
   <para>
    After adding all nodes, check if you need to adjust the
    <systemitem>no-quorum-policy</systemitem> in the global cluster options.
    This is especially important for two-node clusters. For more
    information, refer to <remark>FIXME Install Quick</remark> <!--
    <xref linkend="sec.ha.config.basics.global.quorum"/>-->.
   </para>
  </important>
 </sect1>

 <sect1 xml:id="sec.ha.inst.quick.setup.rm-nodes">
  <title>Removing Nodes From An Existing Cluster</title>
  <para>
   To remove a node from an existing cluster, proceed as follows:
  </para>
  <procedure xml:id="pro.ha.inst.quick.setup.ha-cluster-remove">
   <title>Removing Nodes From An Existing Cluster</title>
   <para>
    If you have a cluster up and running (with at least two nodes), you can
    remove single nodes from the cluster with the
    <command>ha-cluster-remove</command> bootstrap script. You need to know
    the IP address or host name of the node you want to remove from the
    cluster. Follow the steps below. For details, refer to the
    <command>ha-cluster-remove</command> man page.
   </para>
<!--taroth 2013-03-12: is it required that rcopenais is running one the node I 
    want to take down? taroth 2013-03-12: according to tserong: no, does not
    matter-->
   <step>
    <para>
     Log in as &rootuser; to one of the cluster nodes.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
<screen>&prompt.root;<command>ha-cluster-remove</command> <option>-c</option> <replaceable>IP_ADDR_OR_HOSTNAME</replaceable></screen>
    <para>
     The script enables the <systemitem class="daemon">sshd</systemitem>,
     stops the &pace; service on the specified node, and propagates the
     files to synchronize with &csync; across the remaining nodes.
    </para>
<!--information taken from bnc#810292, c#5-->
    <para>
     If you specified a host name and the node to remove cannot be contacted
     (or the host name cannot be resolved), the script will inform you and
     ask whether to remove the node anyway. If you specified an IP address
     and the node cannot be contacted, you will be asked to enter the host
     name and to confirm whether to remove the node anyway.
    </para>
   </step>
   <step>
    <para>
     To remove more nodes, repeat the step above.
    </para>
   </step>
   <step>
    <para>
     For details of the process, check
     <filename>/var/log/ha-cluster-bootstrap.log</filename>.
    </para>
   </step>
  </procedure>

  <para>
   If you need to re-add the removed node at a later point in time, add it
   with <command>ha-cluster-join</command>. For details, refer to
   <xref linkend="pro.ha.inst.quick.setup.ha-cluster-join"/>.
  </para>
  </sect1>

  <!--taroth 2016-07-08: @toms: sorry, I did not have time to add further sections yet,
  please refer to https://fate.suse.com/320823 (feature description *and* the
  respective comments by Kai, Kristoffer, and Antoine-->
 <xi:include href="common_legal.xml"/>
</article>