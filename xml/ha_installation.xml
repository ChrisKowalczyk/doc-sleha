<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!--taroth 2010-02-01: todo: https://fate.novell.com/307503 (guided setup, shell
                                                 templates)
                                                 https://fate.novell.com/308359 (csync2)-->
<chapter id="cha.ha.installation.yast">
 <title>Installation and Basic Setup with &yast;</title>
 <abstract>
  <para> There are several ways to install the software needed for &ha; clusters: either from a
   command line, using <command>zypper</command>, or with &yast; which provides a graphical user
   interface. After installing the software on all nodes that will be part of your cluster, the next
   step is to initially configure the cluster so that the nodes can communicate with each other and
   to start the services needed to bring the cluster online. The initial cluster setup can either be
   done manually (be editing and copying the configuration files) or with the &yast; cluster
   module. </para>
 </abstract>

 <sect1 id="sec.ha.installation.inst">
  <title>Installing the &hasi;</title>

  <para> The packages needed for configuring and managing a cluster with the &hasi; are included
   in the <literal>&ha;</literal> installation pattern. This pattern is only available after
   &productnamereg; has been installed as add-on. <remark>taroth 090115: need to use hard-coded
    link here as the target is not included in the same set</remark> For information on how to
   install add-on products, see the &sle; &productnumber; &deploy;, available at <ulink
    url="http://www.novell.com/documentation"/>. Refer to chapter <citetitle>Installing Add-On
    Products</citetitle>. </para>

  <note>
   <title>Installing the Software Packages</title>
   <para>The software packages needed for &ha; clusters are not automatically copied to the
    cluster nodes. If you do not want to install &slsreg; &productnumber; and
    &productnamereg; &productnumber; manually on all nodes that will be part of your
    cluster, use &ay; to clone existing nodes. For more information, refer to <xref
     linkend="sec.ha.installation.autoyast"/>.</para>
  </note>
  <procedure>
   <title>Installing the &ha; Pattern</title>
   <step>
    <para> Start &yast; as &rootuser; user and select <menuchoice>
      <guimenu>Software</guimenu>
      <guimenu>Software Management</guimenu>
     </menuchoice>.</para>
    <para>Alternatively, start the &yast; package manager as &rootuser; on a command line
     with <command>yast2&nbsp;sw_single</command>. </para>
   </step>
   <step>
    <para>From the <guimenu>Filter</guimenu> list, select <guimenu>Patterns</guimenu> and activate
     the <guimenu>High Availability</guimenu> pattern in the pattern list. </para>
   </step>
   <step>
    <para> Click <guimenu>Accept</guimenu> to start the installation of the packages. </para>
   </step>
  </procedure>
  <para>
   <remark>taroth 2010-02-08: depending on the answers on [ha-devel], add link to next step
    (autoyast or basic config)</remark>
  </para>
 </sect1>

 <sect1 id="sec.ha.installation.setup">
  <title>Initial Cluster Setup</title>

  <para> After having installed the HA packages, proceed with the initial cluster setup.
   <!--taroth 2010-02-02: if there is time left, add
    procedure for manual config: <xref  linkend="pro.ha.installatiom.setup.manual"/>-->
   This includes the following basic steps: <remark>taroth 2010-02-08: DEVs, what about setting
    quorum/fencing options (I guess they are also vital for basic cluster setup)? when and how to
    set these options?</remark></para>
  <procedure>
   <step>
    <para>
     <xref linkend="sec.ha.installation.setup.channels" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="sec.ha.installation.setup.security" xrefstyle="select:title"/>
    </para>
   </step>
   <step>
    <para>
     <xref linkend="sec.ha.installation.setup.csync2" xrefstyle="select:title"/>
    </para>
   </step>
  </procedure>
  <para>The following procedures guide you through each of the steps, using the &yast; cluster
   module. To access the cluster configuration dialog, start &yast; as &rootuser; user and
   select <menuchoice>
    <guimenu>Miscellaneous</guimenu>
    <guimenu>Cluster</guimenu>
   </menuchoice>. Alternatively, start the &yast; cluster module as &rootuser; on a command
   line with <command>yast2&nbsp;cluster</command> . </para>
  <para>Click the categories on the left to access the configuration options for each step. </para>

  <sect2 id="sec.ha.installation.setup.channels">
   <title>Defining the Communication Channels</title>

   <para>For successful communication between the cluster nodes, define at least one communication
    channel. However, it is recommended to set up the communication via two or more redundant paths
    (either by using network device bonding or by adding a second communication channel with
    &corosync;). For each communication channel, you need to define the following parameters: </para>

   <variablelist>
    <varlistentry>
     <term>Bind Network Address (<literal>bindnetaddr</literal>)
    </term>
     <listitem>
      <para>The network address to bind to. To ease sharing configuration files across the cluster,
       &ais; uses network interface netmask to mask only the address bits that are used for
       routing the network. Set the value to the subnet you will use for cluster multicast. </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Multicast Address (<literal>mcastaddr</literal>)
    </term>
     <listitem>
      <para>Can be an IPv4 or IPv6 address. </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Multicast Port (<literal>mcastport</literal>)
    </term>
     <listitem>
      <para> The UDP port specified for <literal>mcastaddr</literal>. </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <!--(Setting up two or more
    clusters that use the same multicast address, but a different port, also works, 
    but is less efficient).-->
   <para> All nodes in the cluster will know each other from using the same multicast address and
    the same port number. For different clusters, use a different multicast address. </para>

   <para>
    <remark>taroth 02010-0208: find out if and how the ringnumbers can be addressed with yast!
    </remark>To configure redundant communication with &corosync;, you need to define multiple
    interface sections in <filename>/etc/corosync/corosync.conf</filename>, each with a different
    ringnumber. Use the Redundant Ring Protocol (RRP) to tell the cluster how to use these
    interfaces. RRP can have three modes (<literal>rrp_mode</literal>): if set to
     <literal>active</literal>, &corosync; uses all interfaces actively. If set to
     <literal>passive</literal>, &corosync; uses the second interface only if the first ring
    fails. If rrp_mode is set to<literal>none</literal>, RRP is disabled. With RRP, two physically
    separate networks are used for communication. In case one network fails, the cluster nodes can
    still communicate via the other network. </para>

   <para>If several rings are configured, each node can have multiple IP addresses. As soon as
    rpp_mode is enabled, the Stream Control Transmission Protocol (SCTP) is used for communication
    between the nodes by default (instead of TCP). </para>

   <procedure id="pro.ha.installation.setup.channels">
    <title>Defining the Communication Channels</title>
    <step>
     <para>In the &yast; cluster module, switch to the <guimenu>Communication Channels</guimenu>
      category.</para>
    </step>
    <step>
     <para>Define the <guimenu>Bind Network Address</guimenu>, the <guimenu>Multicast
       Address</guimenu> and the <guimenu>Multicast Port</guimenu> to use for all cluster nodes. </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_communication.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_communication.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <!--taroth 2010-02-05: https://fate.novell.com/307371-->
    <step>
     <para>If you want to define a second channel:</para>
     <substeps>
      <step>
       <para>Activate <guimenu>Redundant Channel</guimenu>. </para>
      </step>
      <step>
       <para>Define the <guimenu>Bind Network Address</guimenu>, the <guimenu>Multicast
         Address</guimenu> and the <guimenu>Multicast Port</guimenu> for the redundant channel.
       </para>
      </step>
      <step>
       <para>Select the <guimenu>RRP Mode</guimenu> you want to use. To disable the RRP, select
         <guimenu>None</guimenu>. For more information about the modes, click
         <guimenu>Help</guimenu>.</para>
      </step>
     </substeps>
    </step>
    <step>
     <para> Specify a unique <guimenu>Node ID</guimenu> for every cluster node. It is recommended to
      start at <literal>1</literal>. </para>
    </step>
    <step>
     <para>If you only wanted to modify the communication channels for an existing cluster, click
       <guimenu>Finish</guimenu> to write the configuration to
       <filename>/etc/corosync/corosync.conf</filename> and to close the &yast; cluster module.
      &yast; then automatically also adjusts the firewall settings and opens the UDP port used
      for multicast. </para>
    </step>
    <step>
     <para>For further cluster configuration, proceed with <xref
       linkend="pro.ha.installation.setup.security"/>.</para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.installation.setup.security">
   <title>Defining Authentication Settings</title>
   <para>As next step, define the authentication settings for the cluster. You can use HMAC/SHA1
    authentication which requires a shared secret, used to protect and authenticate messages. The
    authentication key (password) you specify will be used on all nodes in the cluster. </para>
   <procedure id="pro.ha.installation.setup.security">
    <title>Enabling Secure Authentication</title>
    <step>
     <para>In the &yast; cluster module, switch to the <guimenu>Security</guimenu>
      category.</para>
    </step>
    <step>
     <para>Activate <guimenu>Enabling Security Auth</guimenu>. </para>
    </step>
    <step>
     <para>For a newly created cluster, click <guimenu>Generate Auth Key File</guimenu>. This
      creates an authentication key that is written to <filename>/etc/corosync/authkey</filename>. </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>If you only wanted to modify the authentication settings, click <guimenu>Finish</guimenu>
      to write the configuration to <filename>/etc/corosync/corosync.conf</filename> and to close
      the &yast; cluster module.</para>
    </step>
    <step>
     <para>For further cluster configuration, proceed with <xref
       linkend="pro.ha.installation.setup.csync2"/>.</para>
    </step>
   </procedure>
  </sect2>

  <!--taroth 2010-02-02: https://fate.novell.com/308359 (csync2)-->
  <sect2 id="sec.ha.installation.setup.csync2">
   <title>Transferring the Configuration to All Nodes</title>
   <para> Instead of copying the resulting configuration files to all nodes manually, use the
    <command>csync2</command> tool for replication across all nodes in the cluster. The tool is not
    restricted to a two-host-setup but can handle any number of host. It is also suitable for
    complex setups where not all configuration files are shared among all members of a cluster, but
    some only among a subgroup. </para>
   <procedure id="pro.ha.installation.setup.csync2">
    <title>Synchronizing Configuration Files</title>
    <para>For the use of Csync2, specify both your cluster nodes and the configuration files to be
     synchronized among the cluster nodes: </para>
    <step>
     <para>In the &yast; cluster module, switch to the <guimenu>Csync2</guimenu>
      category.</para>
    </step>
    <!--taroth 2010-02-02: https://fate.novell.com/308359 (csync2)-->
    <step>
     <para>
      <remark>taroth 2010-02-02: ygao, from the help text I guess that entering IP addresses instead
       of hostnames wouldn't work here, is that correct?</remark> Use the <guimenu>Add</guimenu>
      button in the <guimenu>Sync Host</guimenu> group to enter the local hostnames of all nodes in
      your cluster. You must use exactly the same string that is returned by the
      <command>hostname</command> command. </para>
    </step>
    <step>
     <para> FIXME <remark>taroth 2010-02-02: ygao, what about the pre-shared-keys? is this something
       csync2 requires for synchronization? and the key group file itself can't be synced with
       csync2 because it is needed as prerequisite for csnyc2, is that right? at first glance, that
       still doesn't look like a very good deal to me since there is still one file that needs to be
       copied manually, but at least it's better than copying multiple files... </remark>
     </para>
    </step>
    <step>
     <para> To populate the <guimenu>Sync File</guimenu> list with the files that usually need to be
      synchronized among all nodes, click <guimenu>Add Suggested Files</guimenu>. </para>
    </step>
    <!--<informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_cluster_sync.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_cluster_sync.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>-->
    <step>
     <para> If you want to edit, add or remove files from the list of files to be synchronized use
      the respective buttons. You must enter the absolute pathname for each file. </para>
    </step>
    <step>
     <para> To start the file synchronization among the specified hosts, click <guimenu>Turn Csync2
       On</guimenu>. <remark>taroth 2010-02-02: ygao, what happens when the admin stops csync2? are
       any files removed or is the current status just kept until csync2 is turned on
       again?</remark>
     </para>
    </step>
    <step>
     <para>If you only wanted to synchronize the configuration files among the nodes, click
       <guimenu>Finish</guimenu> to close the &yast; cluster module.</para>
    </step>
    <step>
     <para>To start the basic services needed to bring the cluster online, proceed with <xref
       linkend="sec.ha.installation.start"/>.</para>
    </step>
   </procedure>
  </sect2>
  <sect2 id="sec.ha.installation.setup.services">
   <title>Starting Services</title>
   <para>Optionally, the &yast; cluster module lets you define if to start certain services on a
    node at boot time. You can also use the module to start and stop the services manually (in case
    you do not want to use the command line for that). In order to bring the cluster nodes online
    and to start the cluster resource manager, &ais; must be started as a service.</para>
   <procedure id="pro.ha.installation.setup.services">
    <title>Starting or Stopping Services</title>
    <step>
     <para>In the &yast; cluster module, switch to the <guimenu>Service</guimenu>
      category.</para>
    </step>
    <step>
     <para>To start &ais; each time this cluster node is booted, select the respective option in
      the <guimenu>Booting</guimenu> group. </para>
    </step>
    <step>
     <para>If you want to use the &hbgui; for configuring, managing and monitoring cluster
      resources, activate <guimenu>Start mgmtd as well</guimenu>. This daemon is needed for the
      GUI.</para>
    </step>
    <step>
     <para>To start or stop &ais; immediately, click the respective button.</para>
    </step>
    <step>
     <para>Click <guimenu>Finish</guimenu> to close the &yast; cluster module.</para>
    </step>
   </procedure>
   <para>If you selected <guimenu>Off</guimenu> in the <guimenu>Booting</guimenu> group, you must
    start &ais; manually each time this node is booted. To start &ais; manually, use the
    <command>rcopenais&nbsp; start</command> command. </para>
  </sect2>

  <!-- <procedure id="pro.ha.installatiom.setup.manual"><title>Configuring the Cluster Manually</title>
   <step>
   <para>FIXME: possible contents: /etc/corosync/corosync.conf.example - The easiest way to do so
   is to copy the <filename>/etc/corosync/corosync.conf</filename> file to the other nodes in the
   cluster. As each node needs to have a unique node ID, make sure to adjust the node ID
   accordingly after copying the file. - what about synchronization with csnyc2? -  To enable 
   RRP make the following changes to corosync.conf:
   1.
   In the totem section, add rrp_mode=active or rrp_mode=passive
   2.
   Add a second interface section with a different bindnetaddr for your second network. </para>
   </step>
   </procedure>-->
 </sect1>
 <sect1 id="sec.ha.installation.start">
  <title>Bringing the Cluster Online</title>
  <para> After the initial cluster configuration is done, you can now start the service needed to
   bring the stack online.</para>
  <procedure>
   <title>Starting &ais; and Checking the Status</title>
   <step>
    <para> Run the following command on each of the cluster nodes to start &ais;: </para>
    <screen>rcopenais start</screen>
   </step>
   <step>
    <para> On one of the nodes, check the cluster status with the following command: </para>
    <screen>crm_mon</screen>
    <para>If all nodes are online, the output should be similar to the following: <remark>taroth
      2010-02-08: update screen to display current output</remark></para>
    <screen>============
      Last updated: Thu Feb  5 18:30:33 2009
      Current DC: d42 (d42)
      Version: 1.0.1-node: b7ffe2729e3003ac8ff740bebc003cf237dfa854
      3 Nodes configured.
      0 Resources configured.
      ============
      
      Node: d230 (d230): online
      Node: d42 (d42): online
      Node: e246 (e246): online</screen>
    <para>This output indicates that the cluster resource manager is started and is ready to manage
     resources.</para>
   </step>
  </procedure>

  <para>After the basic configuration is done and the nodes are online, you can now start to
   configure cluster resources. Use either the <command>crm</command> command line tool or the
   graphical user interface. For more information, refer to <xref linkend="cha.ha.configuration.gui"
   /> or <xref linkend="cha.ha.manual_config"/>. </para>
 </sect1>
 
 <sect1 id="sec.ha.installation.autoyast">
  <title>Mass Deployment with &ay;</title>
  <remark>taroth 2010-02-04: not sure about the correct sequence for HA nodes (create the autoyast
   profile *after* sec.ha.installation.setup to also include the basic cluster configuration files??
   how does this relate to https://fate.novell.com/308359 (csync2)??</remark>

  <para>&ay; is a system for installing one or more &sle; systems automatically and without
   user intervention. &sle; lets you create a &ay; profile that contains installation and
   configuration data. The profile tells &ay; what to install and how to configure the installed
   system to get a completely ready-to-use system in the end. This profile can then be used for mass
   deployment in different ways.</para>
  <para>&ay; is suitable for mass installations on machines with the same hardware as well for
   machines with heterogeneous hardware (rule-based autoinstallation). For detailed instructions of
   how to make use of &ay; in various scenarios, see the &sle; &productnumber;
   &deploy;, available at <ulink url="http://www.novell.com/documentation"/>. Refer to chapter
    <citetitle>Automated Installation</citetitle>.</para>

  <procedure>
   <title>Cloning a Cluster Node with &ay;</title>
   <para>The following procedure is suitable for deploying cluster nodes which are clones of an
    already existing node. The cloned nodes will have the same packages installed and the same
    system configuration (network settings, &ais;/&corosync; etc.). After deployment, the
    cloned nodes will join the cluster because the same
     <filename>/etc/corosync/corosync.config</filename> has been applied to the clones.</para>

   <important>
    <title>Identical Hardware</title>
    <para> This scenario assumes you are rolling out &productname; &productnumber; to a set
     of machines with exactly the same hardware configuration. </para>
   </important>

   <step>
    <para>Make sure the node you want to clone is correctly installed and configured to suit as
     source for the other nodes.</para>
   </step>
   <step>
    <para>Follow the description outlined in the &sle; &productnumber; &deploy; for
     simple mass installation. This includes the following basic steps:</para>
    <substeps>
     <step>
      <para>Creating an &ay; profile. Use the &ay; GUI to create and modify a profile from
       the existing system configuration. </para>
     </step>
     <step>
      <para>Determining the source of the &ay; profile and the parameter to pass to the
       installation routines for the other nodes.</para>
     </step>
     <step>
      <para>Determining the source of the &sls; and &productname; installation data.</para>
     </step>
     <step>
      <para>Determining and setting up the boot scenario for autoinstallation.</para>
     </step>
     <step>
      <para> Passing the command line to the installation routines, either by adding the parameters
       manually or by creating an <filename>info</filename> file.</para>
     </step>
     <step>
      <para>Starting and monitoring the autoinstallation process.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     <remark>taroth 2010-02-04: lmb, I didn't test it but I guess the following is required as last
      step, correct?</remark>After the clones have been successfully installed, start the &ais;
     service on each node as described in <xref linkend="sec.ha.installation.start"/> to bring
     the cluster online.</para>
    <para>The CIB is automatically synchronized among the cluster nodes.</para>
   </step>
  </procedure>
  <remark>taroth 2010-02-05: todo - add sentence about different autoyast setup for non-identical
   hardware and link to autoyast chapter for more information </remark>
 </sect1>


</chapter>
