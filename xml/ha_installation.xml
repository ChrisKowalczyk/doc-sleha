<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!--taroth 2010-02-01: todo: https://fate.novell.com/305251 (autoyast)
                                                 https://fate.novell.com/307503 (guided setup, shell
                                                 templates)
                                                 https://fate.novell.com/308359 (csync2)-->
<chapter id="cha.ha.installation.yast">
 <title>Installation and Basic Setup with &yast;</title>
 <abstract>
  <para>
   There are several ways to install the software needed for &ha; clusters:
   either from a command line, using <command>zypper</command>, or with
   &yast; which provides a graphical user interface. After installing the
   software on all nodes that will be part of your cluster, the next step is
   to initially configure the cluster so that the nodes can communicate with
   each other. This can either be done manually (by editing a configuration
   file) or with the &yast; cluster module.
  </para>
 </abstract>
 <remark>taroth 2010-02-04: is the following note deprecated by
  https://fate.novell.com/305251 (autoyast)???</remark>
 <note>
    <title>Installing the Software Packages</title>
  <para>
   The software packages needed for &ha; clusters are not automatically
   copied to the cluster nodes. Install &slsreg; &productnumber; and
   &productnamereg; &productnumber; on all nodes that will be part of your
   cluster.
  </para>
 </note>
 <sect1 id="sec.ha.installation.inst">
  <title>Installing the &hasi;</title>

  <para>
   The packages needed for configuring and managing a cluster with the
   &hasi; are included in the <literal>High Availability</literal>
   installation pattern. This patterns is only available after
   &productnamereg; has been installed as add-on.
   <remark>taroth 090115: need to use
    hard-coded link here as the target is not included in the same set</remark>
   For information on how to install add-on products, see the &sle;
   &productnumber; &deploy;, available at
   <ulink url="http://www.novell.com/documentation"/>. Refer to chapter
   <citetitle>Installing Add-On Products</citetitle>.
  </para>

  <procedure>
   <step>
    <para>
     Start &yast; and select <menuchoice> <guimenu>Software</guimenu>
     <guimenu>Software Management</guimenu> </menuchoice> to open the &yast;
     package manager.
    </para>
   </step>
   <step>
    <para>
     From the <guimenu>Filter</guimenu> list, select
     <guimenu>Patterns</guimenu> and activate the <guimenu>High
     Availability</guimenu> pattern in the pattern list.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Accept</guimenu> to start the installation of the
     packages.
    </para>
   </step>
  </procedure>
  
  </sect1>
 <sect1 id="sec.ha.installation.setup">
  <title>Initial Cluster Setup</title>

  <para> After having installed the HA packages, you can configure the initial cluster setup with
   &yast; as described in <xref linkend="pro.ha.installation.setup.yast"/>. Alternatively,
   manually edit and copy the configuration files.
   <!--taroth 2010-02-02: if there is time left, add
    procedure for manual config: as described in <xref
    linkend="pro.ha.installatiom.setup.manual"/>-->
   The basic setup always includes defining the communication channels between the nodes, security
   aspects (like using encrypted communication), transferring the configuration to all nodes in the
   cluster and starting the needed services. </para>

  <para>You need at least one communication channel. Optionally, you can define a second, redundant
   channel. Regarding the communication channels, you need to define the following parameters: </para>

  <variablelist>
   <varlistentry>
    <term>Bind Network Address (<literal>bindnetaddr</literal>)
    </term>
    <listitem>
     <para>The network address to bind to. To ease sharing configuration files across the cluster,
      &ais; uses network interface netmask to mask only the address bits that are used for
      routing the network. Set the value to the subnet you will use for cluster multicast. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multicast Address (<literal>mcastaddr</literal>)
    </term>
    <listitem>
     <para>Can be an IPv4 or IPv6 address. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multicast Port (<literal>mcastport</literal>)
    </term>
    <listitem>
     <para> The UDP port specified for <literal>mcastaddr</literal>. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <!--taroth 2010-02-05: https://fate.novell.com/307371-->
    <term>Redundant Ring Protocol (<literal>rrp_mode</literal>)</term>
    <listitem>
     <para>
      <remark>taroth 02010-02-05: DEVs, is the following correct?</remark>Only if you have defined a
      redundant communication channel.</para>
     <para>If you activate the redundant ring protocol, two physically separate networks are used
      for communication. In case one network fails, the cluster nodes can still communicate via the
      other network. RRP can have three modes: <literal>active replication</literal>,
       <literal>passive replication</literal> or <literal>none</literal>. If several rings are
      configured, the Stream Control Transmission Protocol (SCTP) is automatically enabled.</para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para> The nodes in the cluster will know each other from using the same multicast address and the
   same port number. For different clusters, use a different multicast address.
   <!--(Setting up two or more
   clusters that use the same multicast address, but a different port, also works, 
   but is less efficient).--></para>

  <para> Instead of copying the resulting configuration files manually, you can now also use the
   <command>csync2</command> tool for replication of the configuration files across all nodes in the
   cluster. The tool is not restricted to a two-host-setup but can handle any number of host. It is
   also suitable for complex setups where not all configuration files are shared among all members
   of a cluster, but some only among a subgroup. </para>

  <procedure id="pro.ha.installation.setup.yast">
   <title>Configuring the Cluster with &yast;</title>
   <step>
    <para> Start &yast; and select <menuchoice>
      <guimenu>Miscellaneous</guimenu>
      <guimenu>Cluster</guimenu>
     </menuchoice> or run <command>yast2
     cluster</command> on a command line to start the
     initial cluster configuration dialog. </para>
   </step>
   <step>
    <para> In the <guimenu>Communication Channels</guimenu> category, configure the channels used
     for communication between the cluster nodes. This information is written to the
      <filename>/etc/corosync/corosync.conf</filename> configuration file. </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_cluster_communication.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_cluster_communication.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
    <para> Define the <guimenu>Bind Network Address</guimenu>, the <guimenu>Multicast
      Address</guimenu> and the <guimenu>Multicast Port</guimenu> to use for all cluster nodes.
    </para>
   </step>
   <!--taroth 2010-02-05: https://fate.novell.com/307371-->
   <step>
    <para>If you want to define a second channel:</para>
    <substeps>
     <step>
      <para>Activate <guimenu>Redundant Channel</guimenu>. </para>
     </step>
     <step>
      <para>Define the <guimenu>Bind Network Address</guimenu>, the <guimenu>Multicast
        Address</guimenu> and the <guimenu>Multicast Port</guimenu> for the redundant channel.
      </para>
     </step>
     <step>
      <para>Select the <guimenu>RRP Mode</guimenu> you want to use. To disable the RRP, select
        <guimenu>None</guimenu>. For more information about the modes, click
       <guimenu>Help</guimenu>.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para> Specify a unique <guimenu>Node ID</guimenu> for every cluster node. It is recommended to
     start at <literal>1</literal>. </para>
   </step>
   <step>
    <para>In the <guimenu>Security</guimenu> category, define the authentication settings for the
     cluster. If <guimenu>Enable Security Authentication</guimenu> is activated, HMAC/SHA1
     authentication is used for communication between the cluster nodes. </para>
    <para> This authentication method requires a shared secret, which is used to protect and
     authenticate messages. The authentication key (password) you specify will be used on all nodes
     in the cluster. For a newly created cluster, click <guimenu>Generate Auth Key File</guimenu> to
     create an authentication key that is written to <filename>/etc/corosync/authkey</filename>. </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_cluster_security.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_cluster_security.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para> In the <guimenu>Service</guimenu> category, choose whether you want to start &ais; on
     this cluster server each time it is booted. </para>
    <para> In order to use the &hbgui;, activate <guimenu>Start mdmtd as well</guimenu>, as this
     daemon is needed for the GUI. </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_cluster_service.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_cluster_service.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
    <para> If you select <guimenu>Off</guimenu>, you must start &ais; manually each time this
     cluster server is booted. To start &ais; manually, use the
     <command>rcopenais start</command> command. </para>
    <para> To start &ais; immediately, click <guimenu>Start &ais; Now</guimenu>. </para>
   </step>
   <!--taroth 2010-02-02: https://fate.novell.com/308359 (csync2)-->
   <step>
    <para> In the <guimenu>Csync2</guimenu> category, specify both your cluster nodes and the
     configuration files to be synchronized among the cluster nodes: </para>
    <substeps>
     <step>
      <para>
       <remark>taroth 2010-02-02: ygao, from the help text I guess that entering IP addresses
        instead of hostnames wouldn't work here, is that correct?</remark> Use the
        <guimenu>Add</guimenu> button in the <guimenu>Sync Host</guimenu> group to enter the local
       hostnames of all nodes in your cluster. You must use exactly the same string that is returned
       by the <command>hostname</command> command. </para>
     </step>
     <step>
      <para> FIXME <remark>taroth 2010-02-02: ygao, what about the pre-shared-keys? is this
        something csync2 requires for synchronization? and the key group file itself can't be synced
        with csync2 because it is needed as prerequisite for csnyc2, is that right? at first glance,
        that still doesn't look like a very good deal to me since there is still one file that needs
        to be copied manually, but at least it's better than copying multiple files... </remark>
      </para>
     </step>
     <step>
      <para> To populate the <guimenu>Sync File</guimenu> list with the files that usually need to
       be synchronized among all nodes, click <guimenu>Add Suggested Files</guimenu>. </para>
     </step>
     <step>
      <para> If you want to edit, add or remove files from the list of files to be synchronized use
       the respective buttons. You must enter the absolute pathname for each file. </para>
     </step>
     <step>
      <para> To start the file synchronization among the specified hosts, click <guimenu>Turn Csync2
        On</guimenu>. <remark>taroth 2010-02-02: ygao, what happens when the admin stops csync2? are
        any files removed or is the current status just kept until csync2 is turned on
        again?</remark>
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para> If all parameters for basic cluster setup are set according to your wishes, click
      <guimenu>Finish</guimenu>. &yast; then automatically also adjusts the firewall settings
     and opens the UDP port used for multicast. </para>
   </step>
  </procedure>

  <!-- <procedure id="pro.ha.installatiom.setup.manual"><title>Configuring the Cluster Manually</title>
   <step>
    <para>FIXME: possible contents: /etc/corosync/corosync.conf.example - The easiest way to do so
     is to copy the <filename>/etc/corosync/corosync.conf</filename> file to the other nodes in the
     cluster. As each node needs to have a unique node ID, make sure to adjust the node ID
     accordingly after copying the file. - what about synchronization with csnyc2? -  To enable 
     RRP make the following changes to corosync.conf:
     1.
     In the totem section, add rrp_mode=active or rrp_mode=passive
     2.
     Add a second interface section with a different bindnetaddr for your second network. </para>
   </step>
  </procedure>-->
 </sect1>
 <sect1 id="sec.ha.installation.autoyast">
  <title>Mass Deployment with &ay;</title>
  <remark>taroth 2010-02-04: not sure about the correct sequence for HA nodes (create the autoyast
   profile *after* sec.ha.installation.setup to also include the basic cluster configuration files??
   how does this relate to https://fate.novell.com/308359 (csync2)??</remark>

  <para>&ay; is a system for installing one or more &sle; systems automatically and without
   user intervention. &sle; lets you create a &ay; profile that contains installation and
   configuration data. The profile tells &ay; what to install and how to configure the installed
   system to get a completely ready-to-use system in the end. This profile can then be used for mass
   deployment in different ways.</para>
  <para>&ay; is suitable for mass installations on machines with the same hardware as well for
   machines with heterogeneous hardware (rule-based autoinstallation). For detailed instructions of
   how to make use of &ay; in various scenarios, see the &sle; &productnumber;
   &deploy;, available at <ulink url="http://www.novell.com/documentation"/>. Refer to chapter
    <citetitle>Automated Installation</citetitle>.</para>

  <procedure>
   <title>Cloning a Cluster Node with &ay;</title>
   <para>The following procedure is suitable for deploying cluster nodes which are clones of an
    already existing node. The cloned nodes will have the same packages installed and the same
    system configuration (network settings, &ais;/&corosync; etc.). After deployment, the
    cloned nodes will join the cluster because the same
     <filename>/etc/corosync/corosync.config</filename> has been applied to the clones.</para>

   <important>
    <title>Identical Hardware</title>
    <para> This scenario assumes you are rolling out &productname; &productnumber; to a set
     of machines with exactly the same hardware configuration. </para>
   </important>

   <step>
    <para>Make sure the node you want to clone is correctly installed and configured to suit as
     source for the other nodes.</para>
   </step>
   <step>
    <para>Follow the description outlined in the &sle; &productnumber; &deploy; for
     simple mass installation. This includes the following basic steps:</para>
    <substeps>
     <step>
      <para>Creating an &ay; profile. Use the &ay; GUI to create and modify a profile from
       the existing system configuration. </para>
     </step>
     <step>
      <para>Determining the source of the &ay; profile and the parameter to pass to the
       installation routines for the other nodes.</para>
     </step>
     <step>
      <para>Determining the source of the &sls; and &productname; installation data.</para>
     </step>
     <step>
      <para>Determining and setting up the boot scenario for autoinstallation.</para>
     </step>
     <step>
      <para> Passing the command line to the installation routines, either by adding the parameters
       manually or by creating an <filename>info</filename> file.</para>
     </step>
     <step>
      <para>Starting and monitoring the autoinstallation process.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     <remark>taroth 2010-02-04: lmb, I didn't test it but I guess the following is required as last
      step, correct?</remark>After the clones have been successfully installed, start the &ais;
     service on each node as described in <xref linkend="sec.ha.installation.start"/> to bring the
     cluster online.</para>
    <para>The CIB is automatically synchronized among the cluster nodes.</para>
   </step>
  </procedure>
 </sect1>
 
 <sect1 id="sec.ha.installation.start">
  <title>Bringing the Cluster Online</title>

  <para>
   After the basic configuration, you can bring the stack online and check
   the status.
  </para>

  <procedure>
   <step>
    <para>
     Run the following command on each of the cluster nodes to start &ais;:
    </para>
<screen>rcopenais start</screen>
   </step>
   <step>
    <para>
     On one of the nodes, check the cluster status with the following
     command:
    </para>
<screen>crm_mon</screen>
    <para>
     If all nodes are online, the output should be similar to the following:
    </para>
<screen>============
Last updated: Thu Feb  5 18:30:33 2009
Current DC: d42 (d42)
Version: 1.0.1-node: b7ffe2729e3003ac8ff740bebc003cf237dfa854
3 Nodes configured.
0 Resources configured.
============
     
Node: d230 (d230): online
Node: d42 (d42): online
Node: e246 (e246): online</screen>
   </step>
  </procedure>

  <para>
   After the basic configuration is done and the nodes are online, you can
   now start to configure cluster resources, either with the
   <command>crm</command> command line tool or with a graphical user
   interface. For more information, refer to
   <xref linkend="cha.ha.configuration.gui"
   /> or
   <xref linkend="cha.ha.manual_config"/>.
  </para>
 </sect1>
</chapter>
