<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!--taroth 2010-02-01: todo: https://fate.novell.com/305251 (autoyast)
                                                 https://fate.novell.com/307503 (guided setup, shell
                                                 templates)
                                                 https://fate.novell.com/308359 (csync2)-->
<chapter id="cha.ha.installation.yast">
 <title>Installation and Basic Setup with &yast;</title>
 <abstract>
  <para> There are several ways to install the software needed for &ha; clusters: either from a
   command line, using <command>zypper</command>, or with &yast; which provides a graphical user
   interface. After installing the software on all nodes that will be part of your cluster, the next
   step is to initially configure the cluster so that the nodes can communicate with each other and
   start the services needed to bring the cluster online. The initial cluster setup can either be
   done manually (be editing and copying the configuration files) or with the &yast; cluster
   module. </para>
 </abstract>
 <remark>taroth 2010-02-04: is the following note deprecated by
  https://fate.novell.com/305251 (autoyast)???</remark>
 <note>
    <title>Installing the Software Packages</title>
  <para>
   The software packages needed for &ha; clusters are not automatically
   copied to the cluster nodes. Install &slsreg; &productnumber; and
   &productnamereg; &productnumber; on all nodes that will be part of your
   cluster.
  </para>
 </note>
 <sect1 id="sec.ha.installation.inst">
  <title>Installing the &hasi;</title>

  <para>
   The packages needed for configuring and managing a cluster with the
   &hasi; are included in the <literal>High Availability</literal>
   installation pattern. This patterns is only available after
   &productnamereg; has been installed as add-on.
   <remark>taroth 090115: need to use
    hard-coded link here as the target is not included in the same set</remark>
   For information on how to install add-on products, see the &sle;
   &productnumber; &deploy;, available at
   <ulink url="http://www.novell.com/documentation"/>. Refer to chapter
   <citetitle>Installing Add-On Products</citetitle>.
  </para>

  <procedure>
   <step>
    <para>
     Start &yast; and select <menuchoice> <guimenu>Software</guimenu>
     <guimenu>Software Management</guimenu> </menuchoice> to open the &yast;
     package manager.
    </para>
   </step>
   <step>
    <para>
     From the <guimenu>Filter</guimenu> list, select
     <guimenu>Patterns</guimenu> and activate the <guimenu>High
     Availability</guimenu> pattern in the pattern list.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Accept</guimenu> to start the installation of the
     packages.
    </para>
   </step>
  </procedure>
  
  </sect1>
 <sect1 id="sec.ha.installation.setup">
  <title>Initial Cluster Setup</title>

  <para> After having installed the HA packages, proceed with the initial cluster setup.
   <!--taroth 2010-02-02: if there is time left, add
    procedure for manual config: as described in <xref
    linkend="pro.ha.installatiom.setup.manual"/>-->
   This includes the following basic steps:</para>
  <procedure>
   <step>
    <para>Defining the Communication Channels</para>
   </step>
   <step>
    <para>Considering Security Aspects</para>
   </step>
   <step>
    <para>Transferring the Configuration to All Nodes</para>
   </step>
   <step>
    <para>Starting Services</para>
   </step>
  </procedure>

  <para>The following procedures guide you through each of the steps, using the &yast; cluster
   module. To access the cluster configuration dialog, start &yast; as &rootuser; user and
   select <menuchoice>
    <guimenu>Miscellaneous</guimenu>
    <guimenu>Cluster</guimenu>
   </menuchoice>. Alternatively, start the &yast; cluster module as &rootuser; with
   <command>yast2
   cluster</command> on a command line. </para>
  <figure>
   <title>&yast; Cluster Configuration Dialog</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="yast2_cluster_communication.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="yast2_cluster_communication.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>Click the categories on the left to access the configuration options for each step. </para>

  <sect2 id="sec.ha.installation.setup.channels">
   <title>Defining the Communication Channels</title>

   <para>For successful communication between the cluster nodes, define at least one communication
    channel. Optionally, configure a second, redundant channel. For each channel, you need to define
    the following parameters: </para>

   <variablelist>
    <varlistentry>
     <term>Bind Network Address (<literal>bindnetaddr</literal>)
    </term>
     <listitem>
      <para>The network address to bind to. To ease sharing configuration files across the cluster,
       &ais; uses network interface netmask to mask only the address bits that are used for
       routing the network. Set the value to the subnet you will use for cluster multicast. </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Multicast Address (<literal>mcastaddr</literal>)
    </term>
     <listitem>
      <para>Can be an IPv4 or IPv6 address. </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Multicast Port (<literal>mcastport</literal>)
    </term>
     <listitem>
      <para> The UDP port specified for <literal>mcastaddr</literal>. </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <!--(Setting up two or more
    clusters that use the same multicast address, but a different port, also works, 
    but is less efficient).-->
   <para> All nodes in the cluster will know each other from using the same multicast address and
    the same port number. For different clusters, use a different multicast address. </para>

   <para>If you have defined a second, redundant communication channel, decide if you want to use
    the Redundant Ring Protocol (RRP) or not. If you activate the redundant ring protocol, two
    physically separate networks are used for communication. In case one network fails, the cluster
    nodes can still communicate via the other network. RRP can have three modes: <literal>active
     replication</literal>, <literal>passive replication</literal> or <literal>none</literal>. </para>

   <para>If several rings are configured (redundant channel), each node can have multiple IP
    addresses. As soon as rrp mode is enabled, the communication between the nodes is thus handled
    by the Stream Control Transmission Protocol (SCTP) instead of TCP. </para>

   <procedure id="pro.ha.installation.setup.channels">
    <title>Defining the Communication Channels</title>
    <step>
     <para>In the &yast; cluster module, switch to the <guimenu>Communication Channels</guimenu>
      category.</para>
    </step>
    <step>
     <para>Define the <guimenu>Bind Network Address</guimenu>, the <guimenu>Multicast
       Address</guimenu> and the <guimenu>Multicast Port</guimenu> to use for all cluster nodes. </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_communication.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_communication.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <!--taroth 2010-02-05: https://fate.novell.com/307371-->
    <step>
     <para>If you want to define a second channel:</para>
     <substeps>
      <step>
       <para>Activate <guimenu>Redundant Channel</guimenu>. </para>
      </step>
      <step>
       <para>Define the <guimenu>Bind Network Address</guimenu>, the <guimenu>Multicast
         Address</guimenu> and the <guimenu>Multicast Port</guimenu> for the redundant channel.
       </para>
      </step>
      <step>
       <para>Select the <guimenu>RRP Mode</guimenu> you want to use. To disable the RRP, select
         <guimenu>None</guimenu>. For more information about the modes, click
         <guimenu>Help</guimenu>.</para>
      </step>
     </substeps>
    </step>
    <step>
     <para> Specify a unique <guimenu>Node ID</guimenu> for every cluster node. It is recommended to
      start at <literal>1</literal>. </para>
    </step>
    <step>
     <para>For initial cluster configuration, proceed with <xref
       linkend="pro.ha.installation.setup.security"/>.</para>
     <para>If you have just modified the communication channels for an existing cluster, click
       <guimenu>Finish</guimenu> to write the configuration to
       <filename>/etc/corosync/corosync.conf</filename>. &yast; then automatically also adjusts
      the firewall settings and opens the UDP port used for multicast. </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.installation.setup.security">
   <title>Considering Security Aspects</title>
   <para>As next step, define the authentication settings for the cluster. You can use HMAC/SHA1
    authentication which requires a shared secret, used to protect and authenticate messages. The
    authentication key (password) you specify will be used on all nodes in the cluster. </para>
   <procedure id="pro.ha.installation.setup.security">
    <title>Enabling Secure Authentication</title>
    <step>
     <para>In the &yast; cluster module, switch to the <guimenu>Security</guimenu>
      category.</para>
    </step>
    <step>
     <para>Activate the respective checkbox. </para>
    </step>
    <step>
     <para>For a newly created cluster, click <guimenu>Generate Auth Key File</guimenu> to create an
      authentication key that is written to <filename>/etc/corosync/authkey</filename>. </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para> For initial cluster configuration, proceed with <xref
       linkend="pro.ha.installation.setup.csync2"/>.</para>
     <para>If you have just modified the authentication settings, click <guimenu>Finish</guimenu> to
      write the configuration to
      <!--check!--><filename>/etc/corosync/corosync.conf</filename>.</para>
    </step>
   </procedure>
  </sect2>
  
  <!--taroth 2010-02-02: https://fate.novell.com/308359 (csync2)-->
  <sect2 id="sec.ha.installation.setup.csnyc2">
   <title>Transferring the Configuration to All Nodes</title>
   <para> Instead of copying the resulting configuration files to all nodes manually, use the
    <command>csync2</command> tool for replication across all nodes in
    the cluster. The tool is not restricted to a two-host-setup but can handle any number of host.
    It is also suitable for complex setups where not all configuration files are shared among all
    members of a cluster, but some only among a subgroup. </para>
   <procedure id="pro.ha.installation.setup.csync2">
    <title>Synchronizing Configuration Files</title>
    <step>
     <para>In the &yast; cluster module, switch to the <guimenu>Csync2</guimenu>
      category.</para>
    </step>
    <!--taroth 2010-02-02: https://fate.novell.com/308359 (csync2)-->
    <step>
     <para>Specify both your cluster nodes and the configuration files to be synchronized among the
      cluster nodes: </para>
     <substeps>
      <step>
       <para>
        <remark>taroth 2010-02-02: ygao, from the help text I guess that entering IP addresses
         instead of hostnames wouldn't work here, is that correct?</remark> Use the
         <guimenu>Add</guimenu> button in the <guimenu>Sync Host</guimenu> group to enter the local
        hostnames of all nodes in your cluster. You must use exactly the same string that is
        returned by the <command>hostname</command> command. </para>
      </step>
      <step>
       <para> FIXME <remark>taroth 2010-02-02: ygao, what about the pre-shared-keys? is this
         something csync2 requires for synchronization? and the key group file itself can't be
         synced with csync2 because it is needed as prerequisite for csnyc2, is that right? at first
         glance, that still doesn't look like a very good deal to me since there is still one file
         that needs to be copied manually, but at least it's better than copying multiple files...
        </remark>
       </para>
      </step>
      <step>
       <para> To populate the <guimenu>Sync File</guimenu> list with the files that usually need to
        be synchronized among all nodes, click <guimenu>Add Suggested Files</guimenu>. </para>
      </step>
      <step>
       <para> If you want to edit, add or remove files from the list of files to be synchronized use
        the respective buttons. You must enter the absolute pathname for each file. </para>
      </step>
      <step>
       <para> To start the file synchronization among the specified hosts, click <guimenu>Turn
         Csync2 On</guimenu>. <remark>taroth 2010-02-02: ygao, what happens when the admin stops
         csync2? are any files removed or is the current status just kept until csync2 is turned on
         again?</remark>
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para> If all parameters for basic cluster setup are set according to your wishes, click
       <guimenu>Finish</guimenu>. </para>
    </step>
    <step>
     <para>Proceed with FIXME <!--<xref linkend=""/>--> to start the basic services needed to bring
      the cluster online.</para></step>
   </procedure>
    
  </sect2>

  <!-- <procedure id="pro.ha.installatiom.setup.manual"><title>Configuring the Cluster Manually</title>
   <step>
   <para>FIXME: possible contents: /etc/corosync/corosync.conf.example - The easiest way to do so
   is to copy the <filename>/etc/corosync/corosync.conf</filename> file to the other nodes in the
   cluster. As each node needs to have a unique node ID, make sure to adjust the node ID
   accordingly after copying the file. - what about synchronization with csnyc2? -  To enable 
   RRP make the following changes to corosync.conf:
   1.
   In the totem section, add rrp_mode=active or rrp_mode=passive
   2.
   Add a second interface section with a different bindnetaddr for your second network. </para>
   </step>
   </procedure>-->
  
  <sect2 id="sec.ha.installation.setup.start">
   <title>Bringing the Cluster Online</title>
   <!--check if contents from original procedure can be integrated here-->
        <!--<step>
   <para> In the <guimenu>Service</guimenu> category, choose whether you want to start &ais;
     on this cluster server each time it is booted. </para>
    <para> In order to use the &hbgui;, activate <guimenu>Start mdmtd as well</guimenu>, as
     this daemon is needed for the GUI. </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="yast2_cluster_service.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="yast2_cluster_service.png" width="75%" format="png"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
    <para> If you select <guimenu>Off</guimenu>, you must start &ais; manually each time this
     cluster server is booted. To start &ais; manually, use the
     <command>rcopenais start</command> command. </para>
    <para> To start &ais; immediately, click <guimenu>Start &ais; Now</guimenu>. </para>
   </step>-->
   <para> After the basic configuration, you can bring the stack online and check the status. </para>

   <procedure>
    <step>
     <para> Run the following command on each of the cluster nodes to start &ais;: </para>
     <screen>rcopenais start</screen>
    </step>
    <step>
     <para> On one of the nodes, check the cluster status with the following command: </para>
     <screen>crm_mon</screen>
     <para> If all nodes are online, the output should be similar to the following: </para>
     <screen>============
      Last updated: Thu Feb  5 18:30:33 2009
      Current DC: d42 (d42)
      Version: 1.0.1-node: b7ffe2729e3003ac8ff740bebc003cf237dfa854
      3 Nodes configured.
      0 Resources configured.
      ============
      
      Node: d230 (d230): online
      Node: d42 (d42): online
      Node: e246 (e246): online</screen>
    </step>
   </procedure>

   <para> After the basic configuration is done and the nodes are online, you can now start to
    configure cluster resources, either with the <command>crm</command> command line tool or with a
    graphical user interface. For more information, refer to <xref
     linkend="cha.ha.configuration.gui"/> or <xref linkend="cha.ha.manual_config"/>. </para>
  </sect2>
 </sect1>
 <sect1 id="sec.ha.installation.autoyast">
  <title>Mass Deployment with &ay;</title>
  <remark>taroth 2010-02-04: not sure about the correct sequence for HA nodes (create the autoyast
   profile *after* sec.ha.installation.setup to also include the basic cluster configuration files??
   how does this relate to https://fate.novell.com/308359 (csync2)??</remark>

  <para>&ay; is a system for installing one or more &sle; systems automatically and without
   user intervention. &sle; lets you create a &ay; profile that contains installation and
   configuration data. The profile tells &ay; what to install and how to configure the installed
   system to get a completely ready-to-use system in the end. This profile can then be used for mass
   deployment in different ways.</para>
  <para>&ay; is suitable for mass installations on machines with the same hardware as well for
   machines with heterogeneous hardware (rule-based autoinstallation). For detailed instructions of
   how to make use of &ay; in various scenarios, see the &sle; &productnumber;
   &deploy;, available at <ulink url="http://www.novell.com/documentation"/>. Refer to chapter
    <citetitle>Automated Installation</citetitle>.</para>

  <procedure>
   <title>Cloning a Cluster Node with &ay;</title>
   <para>The following procedure is suitable for deploying cluster nodes which are clones of an
    already existing node. The cloned nodes will have the same packages installed and the same
    system configuration (network settings, &ais;/&corosync; etc.). After deployment, the
    cloned nodes will join the cluster because the same
     <filename>/etc/corosync/corosync.config</filename> has been applied to the clones.</para>

   <important>
    <title>Identical Hardware</title>
    <para> This scenario assumes you are rolling out &productname; &productnumber; to a set
     of machines with exactly the same hardware configuration. </para>
   </important>

   <step>
    <para>Make sure the node you want to clone is correctly installed and configured to suit as
     source for the other nodes.</para>
   </step>
   <step>
    <para>Follow the description outlined in the &sle; &productnumber; &deploy; for
     simple mass installation. This includes the following basic steps:</para>
    <substeps>
     <step>
      <para>Creating an &ay; profile. Use the &ay; GUI to create and modify a profile from
       the existing system configuration. </para>
     </step>
     <step>
      <para>Determining the source of the &ay; profile and the parameter to pass to the
       installation routines for the other nodes.</para>
     </step>
     <step>
      <para>Determining the source of the &sls; and &productname; installation data.</para>
     </step>
     <step>
      <para>Determining and setting up the boot scenario for autoinstallation.</para>
     </step>
     <step>
      <para> Passing the command line to the installation routines, either by adding the parameters
       manually or by creating an <filename>info</filename> file.</para>
     </step>
     <step>
      <para>Starting and monitoring the autoinstallation process.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     <remark>taroth 2010-02-04: lmb, I didn't test it but I guess the following is required as last
      step, correct?</remark>After the clones have been successfully installed, start the &ais;
     service on each node as described in <xref linkend="sec.ha.installation.setup.start"/> to bring the
     cluster online.</para>
    <para>The CIB is automatically synchronized among the cluster nodes.</para>
   </step>
  </procedure>
  <remark>taroth 2010-02-05: todo - add sentence about different autoyast setup for non-identical
   hardware and link to autoyast chapter for more information </remark>
 </sect1>
 
 
</chapter>
