<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!--taroth 2011-08-22: todo: fates #305251, #309677, #310284, #311084 -->
<chapter id="cha.ha.installation">
 <title>Installation and Basic Setup</title>
 <abstract>
  <para>This chapter describes how to install and set up &productnamereg; &productnumber; from
   scratch. Choose between different deployment methods that allow you to have a cluster up and
   running within a few minutes (with the choice to adjust any options later on) or decide for a
   manual setup, allowing you to set your individual options right at the beginning.</para>
   <!--https://bugzilla.novell.com/show_bug.cgi?id=573817#c6-->
  <para>Refer to chapter <xref linkend="cha.ha.migration"/> if you want to migrate an existing
   cluster that runs an older version of &productname; or if you want to update any software
   packages on nodes that are part of a running cluster.</para>
 </abstract>
 <sect1 id="sec.ha.installation.terms">
  <!--taroth 2011-10-26: move this to glossary?-->
  <title>Definition of Terms</title>
 <variablelist>
  <varlistentry>
   <term>Existing Cluster</term>
   <listitem>
     <para>The term <quote>existing cluster</quote> is used to refer to any cluster which consists
      of at least one node. Existing clusters have a basic &corosync; configuration that defines
      the communication channels, but they do not necessarily have resource configuration
      yet.</para></listitem>
  </varlistentry>
  <varlistentry>
   <term>Multicast</term>
   <listitem>
     <para>A technology used for a one-to-many communication within a network that can be used for
      cluster communication. If multicast does not comply with your corporate IT policy, use unicast
      instead. </para>
    <note>
      <title>Switches and Multicast</title>
      <para>If you want to use multicast for cluster communication, make sure your switches support
       multicast.</para></note></listitem>
  </varlistentry>
  <varlistentry id="vle.ha.mcastaddr">
   <term>Multicast Address (<systemitem>mcastaddr</systemitem>)
   </term>
   <listitem>
     <para>IP address to use for multicasting (either IPv4 or IPv6). You can use any multicast address in your private
      network. </para>
   </listitem>
  </varlistentry>
   <varlistentry>
   <term>Unicast</term>
   <listitem>
     <para>A technology for sending messages to a single network destination. In &corosync;,
      unicast is implemented as UDP-unicast (UDPU). </para></listitem>
   </varlistentry>
  <varlistentry>
   <term>Multicast Port (<systemitem>mcastport</systemitem>)
     
   </term>
   <listitem>
    <para>The port to use for cluster communication. </para>
   </listitem>
  </varlistentry>
   <varlistentry>
    <term>Bind Network Address (<systemitem>bindnetaddr</systemitem>)
  </term>
    <listitem>
     <para>The network address to bind to. To ease sharing configuration files across the cluster,
      &ais; uses network interface netmask to mask only the address bits that are used for
      routing the network. </para>
     <note>
      <title>Network Address for All Nodes </title>
      <para>As the same &corosync; configuration will be used on all nodes, make sure to use a
       network address as bindnetaddr, not the address of a specific network interface.</para>
     </note>
    </listitem>
   </varlistentry>
  <varlistentry id="vle.ha.rrp">
   <term>Redundant Ring Protocol (RRP)</term>
   <listitem>
     <para>&corosync; supports the Totem Redundant Ring Protocol. It allows the use of multiple
      redundant local-area networks for resilience against partial or total network faults. This
      way, cluster communication can still be kept up as long as a single network is operational. A
      logical token-passing ring is imposed on all participating nodes to deliver messages in a
      reliable and sorted manner. A node is allowed to broadcast a message only if it holds the
      token. For more information, refer to <ulink url="http://www.rcsc.de/pdf/icdcs02.pdf"
       ></ulink>.</para>
     <para>When having defined redundant communication channels in &corosync;, use RRP to tell
      the cluster how to use these interfaces. RRP can have three modes
      (<literal>rrp_mode</literal>): if set to <literal>active</literal>, &corosync; uses both
      interfaces actively. If set to <literal>passive</literal>, &corosync; uses the second
      interface only if the first ring fails. If rrp_mode is set to <literal>none</literal>, RRP is
      disabled.
      <!--With RRP, two physically separate networks are used for communication. In case one
      network fails, the cluster nodes can still communicate via the other network.--></para>
    </listitem>
  </varlistentry>
  <varlistentry>
   <term>&csync;</term>
   <listitem>
     <para>A synchronization tool that can be used to replicate configuration files across all nodes
      in the cluster. &csync; can handle any number of hosts, sorted into synchronization
      groups. Each synchronization group has its own list of member hosts and its include/exclude
      patterns that define which Ô¨Åles should be synchronized in the synchronization group. The
      groups, the hostnames belonging to each group, and the include/exclude rules for each group
      are specified in the &csync; configuration file,
       <filename>/etc/csync2/csync2.cfg</filename>.</para>
    <para>For authentication, &csync; uses the IP addresses and pre-shared keys within a
     synchronization group. You need to generate one key file for each synchronization group and copy
     it to all group members. </para>
     <para> For more information about &csync;, refer to <ulink
       url="http://oss.linbit.com/csync2/paper.pdf"/>
     </para></listitem>
  </varlistentry>
  
 </variablelist>
  
 </sect1>
 
 
 <sect1 id="sec.ha.installation.overview">
  <title>Overview</title>
  <para>The following basic steps are needed for installation and initial cluster setup.
   <!--They can
   either be executed manually or automatically, using one the methods mentioned in <xref
    linkend="sec.ha.installation.methods"/>.--></para>

  <variablelist>
   <varlistentry>
    <term><xref linkend="sec.ha.installation.add-on" xrefstyle="select:title"/></term>
    <listitem>
     <para>Install the software packages with &yast;. Alternatively, you can install them from
      the command line with <command>zypper</command>.</para>
    </listitem>
   </varlistentry>
   <varlistentry id="vle.ha.installation.setup.oview">
    <term>Initial Cluster Setup</term>
    <listitem>
     <para>After installing the software on all nodes that will be part of your cluster, the
      following steps are needed to initially configure the cluster:</para>
     <remark>taroth 2010-10-20: todo - modify and extend for SP2</remark>
     <orderedlist>
      <listitem>
       <para>
        <xref linkend="sec.ha.installation.setup.channels" xrefstyle="select:title"/>
       </para>
      </listitem>
      <listitem>
       <para>
        <xref linkend="sec.ha.installation.setup.security" xrefstyle="select:title"/>
       </para>
      </listitem>
      <listitem>
       <para>
        <xref linkend="sec.ha.installation.setup.csync2" xrefstyle="select:title"/>
       </para>
      </listitem>
     </orderedlist>
     <para>This steps above can either be executed automatically (with bootstrap scripts) or
      manually (with the &yast; cluster module or from command line).</para>
     <itemizedlist>
      <listitem>
       <para>If you decide for an automatic cluster setup, refer to <xref
         linkend="sec.ha.installation.setup.auto"/>. </para>
      </listitem>
      <listitem>
       <para>For a manual setup (or for adjusting any options after the automatic setup), refer to
         <xref linkend="sec.ha.installation.setup.manual"/>.</para>
      </listitem>
     </itemizedlist>
     <para>You can also use a mixture of both setup methods, for example: set up one node with
      &yast; cluster and then use <command>sleha-join</command> to integrate more nodes.</para>
     <para>Finally, you can clone existing nodes for mass deployment with &ay;. The cloned nodes
      will have the same packages installed and the same system configuration. For details, refer to
       <xref linkend="sec.ha.installation.autoyast"/>. </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 
 <!--taroth 2011-10-25: commenting for now as deployment as applicance is not fully settled yet-->
 <!--<sect1 id="sec.ha.installation.methods">
  <title>Deployment Methods</title>
  <para>The following installation and setup methods are available for &productname;:</para>
  <variablelist>
   <varlistentry>
    <term>Installation as Add-on</term>
    <listitem>
     <para>
      <xref linkend="sec.ha.installation.add-on"/> describes how to install the software packages with
      &yast;. Alternatively, you can install them from the command line with
      <command>zypper</command>.</para>
     <para>After installing the software on all nodes that will be part of your cluster, the next
      steps are to initially configure the cluster so that the nodes can communicate with each other
      and to start the services needed to bring the cluster online. This can either be done
      automatically (with a bootstrap script) or manually. The automatic setup requires only a
      minimum of time and manual intervention to get a one-node cluster up and running and to make
      other nodes join. Any options can be modified later with the &yast; cluster module.</para>
     <itemizedlist>
      <listitem>
       <para>If you decide for an automatic cluster setup, refer to <xref linkend="sec.ha.installation.setup.auto"/>.
       </para>
      </listitem>
      <listitem>
       <para>For a manual setup (or for adjusting any options after the automatic setup), refer to
        <xref linkend="sec.ha.installation.setup.manual"/>.</para>
      </listitem>
     </itemizedlist>
     <para>You can also use a mixture of both setup methods, for example: set up one node with
      &yast; cluster and then use <command>sleha-join</command> to integrate more nodes.</para>
     <para>Finally, you can clone existing nodes for mass deployment with &ay;. The cloned nodes
      will have the same packages installed and the same system configuration. For details, refer to
       <xref linkend="sec.ha.installation.autoyast"/>. </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Installation as Appliance</term>
    <listitem>
     <para><remark>taroth 2011-10-13: waiting for feedback if this scenario will be supported for
       SP2 (todo: if yes, add xref)</remark> The &hasi; is also available as software appliance,
      containing a preconfigured set of applications, their basic configuration and an operating
      system (&slsreg;). All these parts are integrated into a single image, deployable on
      industry standard hardware or in a virtual environment.</para>
     <para>This installation method provides both automated installation and cluster setup.
       <remark>taroth 2011-10-13: DEVs, is the following true?</remark>Any options can be modified
      later with the &yast; cluster module.</para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>-->

 <sect1 id="sec.ha.installation.add-on">
  <title>Installation as Add-on</title>
  <para>The packages needed for configuring and managing a cluster with the &hasi; are included
   in the <literal>&ha;</literal> installation pattern. This pattern is only available after
   &productname; has been installed as add-on to &sls;. For information on how to install
   add-on products, see the &sle; &productnumber; &deploy;, available at <ulink
    url="http://www.suse.com/documentation/sles11"/>. Refer to chapter <citetitle>Installing Add-On
    Products</citetitle>.
   <!--taroth: need to use hard-coded link here as the target is not included in the same set-->
  </para>

  <procedure id="pro.ha.install.pattern">
   <title>Installing the &ha; Pattern</title>
   <step>
    <para> Start &yast; as &rootuser; user and select <menuchoice>
      <guimenu>Software</guimenu>
      <guimenu>Software Management</guimenu>
     </menuchoice>. </para>
    <para> Alternatively, start the &yast; package manager as &rootuser; on a command line
     with <command>yast2&nbsp;sw_single</command>. </para>
   </step>
   <step>
    <para> From the <guimenu>Filter</guimenu> list, select <guimenu>Patterns</guimenu> and activate
     the <guimenu>High Availability</guimenu> pattern in the pattern list. </para>
   </step>
   <step>
    <para> Click <guimenu>Accept</guimenu> to start the installation of the packages. </para>
    <note>
     <para>The software packages needed for &ha; clusters are <emphasis>not</emphasis>
      automatically copied to the cluster nodes.</para>
    </note>
   </step>
   <step>
    <para>Install the &ha; pattern on all machines that will be part of your cluster.</para>
    <para>If you do not want to install &slsreg; &productnumber; and &productname;
     &productnumber; manually on all nodes that will be part of your cluster, use &ay; to
     clone existing nodes. For more information, refer to <xref
      linkend="sec.ha.installation.autoyast"/>.</para>
   </step>
  </procedure>
 </sect1>
  <!--taroth 2011-10-25: commenting for now as deployment as applicance is not fully settled yet-->
  
 <!--<sect1 id="sec.ha.installation.appliance">
  <title>Installation as Appliance</title>
  <para>
   <remark>taroth 2011-10-17: FIXME:  if this is supported for SP2, add more information</remark></para>
 </sect1>-->
 
 <sect1 id="sec.ha.installation.setup.auto">
  <title>Automatic Cluster Setup (sleha-bootstrap)</title>
  <para>The 
   <systemitem class="resource">sleha-bootstrap</systemitem> package provides everything
   you need to get a one-node cluster up and running and to make other nodes join with the following
   basic steps:</para>
  <variablelist>
   <varlistentry>
    <term><xref linkend="pro.ha.installation.setup.sleha-init" xrefstyle="select:title"/></term>
    <listitem>
     <para>With <command>sleha-init</command>, define the basic parameters needed for cluster
      communication and (optionally) set up a &stonith; mechanism to protect your shared
      storage. This leaves you with a running one-node cluster.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="pro.ha.installation.setup.sleha-join" xrefstyle="select:title"/></term>
    <listitem>
     <para>With <command>sleha-join</command>, add more nodes to your cluster.</para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>Both commands execute bootstrap scripts that require only a minimum of time and manual
   intervention. Any options set during the bootstrap process can be modified later with the
   &yast; cluster module.</para>
 
  <para>Before starting the automatic setup, make sure that the following prerequisites are
   fulfilled on all nodes that will participate in the cluster:</para>
  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>The requirements listed in <xref linkend="sec.ha.requirements.sw"/> and <xref
      linkend="sec.ha.requirements.other"/> are fulfilled. </para>
   </listitem>
   <listitem>
    <para>The <systemitem class="resource">sleha-bootstrap</systemitem> package is installed.
     Optionally, also install the <systemitem class="resource">haveged</systemitem> package to speed
     up generation of keys for SSH and &csync;.</para>
   </listitem>
   <listitem>
    <para>The network is configured according to your needs. For example, a private network is
     available for cluster communication and network device bonding is configured. For information
     on bonding, refer to <xref linkend="cha.ha.netbonding"/>.</para>
   </listitem>
   <listitem>
    <para>If you want to use Split Brain Detection (SBD) for your shared storage, you need at least
     two shared storage block devices: one for SBD and one for the OCFS2 file system. The block
     devices need not be formatted. For more information, refer to <xref
      linkend="cha.ha.storage.protect"/>.</para>
   </listitem>
   <listitem>
    <para>All nodes must be able to see the shared storage via the same paths
      (<filename>/dev/disk/by-path/...</filename> or
     <filename>/dev/disk/by-id/...</filename>).</para>
   </listitem>
  </itemizedlist>

  <procedure id="pro.ha.installation.setup.sleha-init">
   <title>Automatically Setting Up the First Node</title>
   <para>The <command>sleha-init</command> command checks for configuration of NTP and guides you
    through configuration of the cluster communication layer (&corosync;), and (optionally)
    through the configuration of SBD to protect your shared storage:</para>
   <step>
    <para>Log in as &rootuser; to the physical or virtual machine you want to use as cluster
     node.</para>
   </step>
   <step>
    <para>Start the bootstrap script by executing</para>
    <screen>sleha-init</screen>
    <para>If NTP has not been configured to start at boot time, a message appears. </para>
    <para>If you decide to continue anyway, the script will automatically generate keys for SSH
     access and for the &csync; synchronization tool and start the services needed for
     both.</para>
   </step>
   <step>
    <para>To configure the cluster communication layer (&corosync;):</para>
    <substeps>
     <step>
      <para>Enter a network address to bind to. By default, the script will propose the network
       address of <systemitem>eth0</systemitem>. Alternatively, enter a different network address,
       for example the address of <literal>bond0</literal>.</para>
     
     </step>
     <step>
      <para>Enter a multicast address. The script proposes a random address that you can use as
       default.</para>
     </step>
     <step>
      <para>Enter a multicast port. The script proposes <literal>5404</literal> as default.</para>
     </step>
    </substeps>
   </step>
   <step>
    <para>To configure SBD (optional), enter a persistent path to the partition of your block device
     that you want to use for SBD. The path must be consistent across all nodes in the
     cluster.</para>
    <para><!--FIXME: what happens in the background? any resources added?--></para>
    <para>Finally, the script will start the &ais; service to bring the one-node cluster online
     and enable the Web management interface, the &hawk;. The URL to use for the &hawk; is displayed
     on the screen.</para>
   </step>
   <step>
    <para>For any details of the setup process, check
      <filename>var/log/sleha-bootstrap.log</filename>.</para>
   </step>
  </procedure>
  <para>You now have a running one-node cluster. If you want to check the cluster status or start
   managing resources, proceed by logging in to one of the user interfaces, &hawk; or the
   &hbgui;. For more information, refer to <xref linkend="cha.ha.configuration.hawk"/> and <xref
    linkend="cha.ha.configuration.gui"/>.</para>
  <important>
   <title>Secure Password</title>
   <para>The bootstrap procedure creates a linux user named <systemitem class="username"
     >hacluster</systemitem> with the password <literal>linux</literal>. You need it for logging in
    to the &hbgui; or &hawk;. Replace the default password with a secure one as soon as
    possible.</para>
  </important>
  
  <procedure id="pro.ha.installation.setup.sleha-join">
   <title>Adding Nodes to an Existing Cluster</title>
   
   <para>If you have a cluster up and running (with one or more nodes), add more cluster nodes with
    the <command>sleha-join</command> bootstrap script. The script only needs access to an existing
    cluster node and will complete the basic setup on the current machine automatically. </para>
   <para>If you are logged in to the first node via the &hawk;, you can follow the changes in
    cluster status and view the resources being activated in the Web interface.</para>
   <step>
    <para>Log in as &rootuser; to the physical or virtual machine you want to join the
     cluster.</para>
   </step>
   <step>
    <para>Start the bootstrap script by executing</para>
    <screen>sleha-join</screen>
    <para>If NTP has not been configured to start at boot time, a message appears. </para>
   </step>
   <step>
    <para>If you decide to continue anyway, you will be prompted for the IP address of an existing
     node. Enter the IP address. </para>
   </step>
   <step>
    <para>If you have not already configured a passwordless SSH access between both machines, you
     will also be prompted for the &rootuser; password of the existing node. </para>
    <para>After logging in to the specified node, the script will copy the &corosync;
     configuration, configure SSH and &csync;, and will bring the current machine online as new
     cluster node. Apart from that, it will start the service needed for &hawk;. If you have
     configured shared storage with OCFS2, it will also automatically create the mountpoint
     directory for the OCFS2 file system.</para>
   </step>
   <step>
    <para>Repeat the steps above for as many machines as you want to add to the cluster.</para>
   </step>
   <step>
    <para>For any details of the process, check
     <filename>var/log/sleha-bootstrap.log</filename>.</para>
   </step>
  </procedure>
  <important>
   <title>Check <systemitem>no-quorum-policy</systemitem></title>
   <para>After all nodes are added, check if you need to adjust the
     <systemitem>no-quorum-policy</systemitem> in the global cluster options. This is especially
    important for two-node clusters. For more information, refer to <xref
     linkend="sec.ha.configuration.basics.global.quorum"/>.</para></important>
 </sect1>
 
 <sect1 id="sec.ha.installation.setup.manual">
  <title>Manual Cluster Setup (&yast;)</title>

  <remark>taroth 2011-03-09: todo: http://doccomments.provo.novell.com/admin/viewcomment/15680#:
   Second, you should make clear what to do in which order on what node, e.g.
   xinetd must be started on all nodes. Also, I think, openais must be started on all nodes. 
    - taroth 2011-05-04:  most of the topics are already mentioned in the following sections of the chapter, will have a
   look again for next revision </remark>

  <para>See <xref linkend="vle.ha.installation.setup.oview"/> for an overview of all steps. The
   following sections provide detailed guidance for each step.</para>

  <sect2 id="sec.ha.installation.setup.yast2cluster">
   <title>&yast; Cluster Module</title>

   <para>The following sections guide you through each of the setup steps, using the &yast; cluster
    module. To access the cluster configuration dialog, start &yast; as &rootuser; and
    select <menuchoice>
     <guimenu>&ha;</guimenu>
     <guimenu>Cluster</guimenu>
    </menuchoice>. Alternatively, start the &yast; cluster module as &rootuser; on a command
    line with <command>yast2&nbsp;cluster</command>. </para>

   <para> If you start the cluster module for the first time, it appears as wizard, guiding you
    through all the steps necessary for basic setup. Otherwise, click the categories on the left
    panel to access the configuration options for each step. </para>
   <figure>
    <title>&yast; Cluster Module&mdash;Overview</title>
    <mediaobject>
    <imageobject role="fo">
    <imagedata fileref="yast2_cluster_main.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
    <imagedata fileref="yast2_cluster_main.png" width="75%" format="png"/>
    </imageobject>
    </mediaobject>
    </figure>
  </sect2>

  <sect2 id="sec.ha.installation.setup.channels">
   <title>Defining the Communication Channels</title>
   <para>For successful communication between the cluster nodes, define at least one communication
    channel.</para>
   <important>
    <title>Redundant Communication Paths</title>
    <para>However, it is highly recommended to set up cluster communication via two or more
     redundant paths. This can be done via:</para>
    <itemizedlist>
     <listitem>
      <para>
       <xref linkend="cha.ha.netbonding" xrefstyle="select:title"/>. 
      </para>
     </listitem>
     <listitem>
      <para>A second communication channel in &corosync;. For details, see 
       <xref linkend="pro.ha.installation.setup.channel2"/>.</para>
     </listitem>
    </itemizedlist>
    <para>If possible, prefer network device bonding.</para>
   </important>
   
 
    
   <procedure id="pro.ha.installation.setup.channel1">
    <title>Defining the First Communication Channel</title>
    <para>For communication between the cluster nodes, use either multicast (UDP) or unicast (UDPU).
    </para>
   
    <step>
     <para>In the &yast; cluster module, switch to the <guimenu>Communication
       Channels</guimenu> category. </para>
    </step>
    <step>
     <para>To use multicast:</para>
     <!--<informalfigure>
      <mediaobject>
      <imageobject role="fo">
      <imagedata fileref="yast2_cluster_communication.png" width="75%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
      <imagedata fileref="yast2_cluster_communication.png" width="75%" format="png"/>
      </imageobject>
      </mediaobject>
      </informalfigure>-->
     <substeps>
      <step>
       <para>Set the <guimenu>Transport</guimenu> protocol to <literal>UDP</literal>.</para>
      </step>
      <step>
       <para>Define the <guimenu>Bind Network Address</guimenu>. Set the value to the subnet you
        will use for cluster multicast.</para>
      </step>
      <step>
       <para>Define the <guimenu>Multicast Address</guimenu>.</para>
      </step>
      <step>
       <para>Define the <guimenu>Multicast Port</guimenu>.</para>
       <para>Wit the values entered above you have now defined <emphasis>one</emphasis>
        communication channel for the cluster. In multicast mode, the same bindnetaddr, mcastaddr,
        and mcastport will be used for all cluster nodes. All nodes in the cluster will know each
        other from using the same multicast address. For different clusters, use different multicast
        addresses.
        <!--taroth 2011-10-26: for the records a statement by lmb: Setting up two or more
         clusters that use the same multicast address, but a different port, also works, 
         but is less efficient)--></para>
      </step>
     </substeps>
    </step>
    <step>
     <para>To use unicast:</para>
     <substeps>
      <step>
       <para>Set the <guimenu>Transport</guimenu> protocol to <literal>UDPU</literal>.</para>
      </step>
      <step>
       <para>Define the <guimenu>Bind Network Address</guimenu>. Set the value to the subnet you
        will use for cluster unicast.</para>
      </step>
      <step>
       <!--taroth 2011-10-26: check if the portname is also "mcastport" in corosync if UDPU is used,
       otherwise it's a bug in the UI!-->
       <para>Define the <guimenu>Multicast Port</guimenu>.</para>
      </step>
      <step>
       <para>For unicast communication, &corosync; needs to know the IP addresses of all nodes
        in the cluster. For each node that will be part of the cluster, click <guimenu>Add</guimenu>
        and enter its IP address <remark>taroth 2011-10-26: DEVs, would hostname also
        work?</remark>. To modify or remove any addresses of cluster members, use the
         <guimenu>Edit</guimenu> or <guimenu>Del</guimenu> buttons.</para>
         </step>
     </substeps>
    </step>
    <step>
     <para>Activate <guimenu>Auto Generate Node ID</guimenu> to automatically generate a unique ID
      for every cluster node. </para>
    </step>
    <step>
     <para>If you modified any options for an existing cluster, confirm your
      changes and close the cluster module. &yast; writes the configuration to
       <filename>/etc/corosync/corosync.conf</filename>.</para>
    </step>
    <step>
     <para>Otherwise proceed with <xref linkend="pro.ha.installation.setup.channel2"/>. Or click <guimenu>Next</guimenu>and proceed
      with <xref linkend="pro.ha.installation.setup.security"/>.</para></step>
   </procedure>
   
   <!--taroth 2010-02-05: https://fate.novell.com/307371-->
   <procedure id="pro.ha.installation.setup.channel2">
    <title>Defining a Redundant Communication Channel</title>
    <para>If network device bonding cannot be used for any reason, the second best choice is to
     define a redundant communication channel (a second ring) in &corosync;. That way, two
     physically separate networks can be used for communication. In case one network fails, the
     cluster nodes can still communicate via the other network.</para>
    <important>
     <title>Redundant Rings and <filename>/etc/hosts</filename>
     </title>
     <para>If multiple rings are configured, each node can have multiple IP addresses. This needs to
      be reflected in the <filename>/etc/hosts</filename> of all nodes.</para>
    </important>
    <step>
     <para>In the &yast; cluster module, switch to the <guimenu>Communication Channels</guimenu>
      category. </para>
    </step>
    <step>
     <para>Activate <guimenu>Redundant Channel</guimenu>. <remark>taroth 2011-10-26: DEVs, is the
       following correct?</remark>The redundant channel must use the same protocol like the first
      communication channel you defined.</para>
    </step>
    <step>
     <para>If you use multicast, define the <guimenu>Bind Network Address</guimenu>, the
       <guimenu>Multicast Address</guimenu> and the <guimenu>Multicast Port</guimenu> for the
      redundant channel. </para>
     <para>If you use unicast, define the <guimenu>Bind Network Address</guimenu> and the
       <guimenu>Multicast Port</guimenu>, and enter the IP addresses of all nodes that will be part
      of the cluster.</para>
     <para>With that, you have defined an additional communication channel in &corosync; that
      will form a second token-passing ring. In <filename>/etc/corosync/corosync.conf</filename>,
      the primary ring (the first channel you have configured) gets the ringnumber
       <literal>0</literal>, the second ring (redundant channel) the ringnumber
      <literal>1</literal>. </para>
    </step>
    <step>
     <para>To tell &corosync; how and when to use the different channels, select the
       <guimenu>rrp_mode</guimenu> you want to use (<literal>active</literal> or
       <literal>passive</literal>). For more information about the modes, refer to <xref
       linkend="vle.ha.rrp"/> or click <guimenu>Help</guimenu>. As soon as RRP is used, the Stream
      Control Transmission Protocol (SCTP) is used for communication between the nodes (instead of
      TCP).</para>
     <para>If only one communication channel is defined, <guimenu>rrp_mode</guimenu> is
      automatically disabled (value <literal>none</literal>).</para>
    </step>
    <step>
     <para>If you modified any options for an existing cluster, confirm your changes and close the
      cluster module. &yast; writes the configuration to
       <filename>/etc/corosync/corosync.conf</filename>.</para>
    </step>
    <step>
     <para> For further cluster configuration, click <guimenu>Next</guimenu> and proceed with <xref
       linkend="pro.ha.installation.setup.services"/>. </para>
    </step>
    <!--taroth:adjust! - &yast; then automatically also adjusts the firewall settings and opens the UDP port used
      for multicast.-->
   </procedure>
  </sect2>

  
  <sect2 id="sec.ha.installation.setup.security">
   <title>Defining Authentication Settings</title>
   <para> As next step, define the authentication settings for the cluster. You can use HMAC/SHA1
    authentication which requires a shared secret, used to protect and authenticate messages. The
    authentication key (password) you specify will be used on all nodes in the cluster. </para>
   <procedure id="pro.ha.installation.setup.security">
    <title>Enabling Secure Authentication</title>
    <step>
     <para> In the &yast; cluster module, switch to the <guimenu>Security</guimenu> category.
     </para>
    </step>
    <step>
     <para> Activate <guimenu>Enable Security Auth</guimenu>. </para>
    </step>
    <step>
     <para> For a newly created cluster, click <guimenu>Generate Auth Key File</guimenu>. This
      creates an authentication key that is written to <filename>/etc/corosync/authkey</filename>. </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>To make the current machine join an existing cluster, copy the
       <filename>/etc/corosync/authkey</filename> from one of the nodes to the current machine
      (either manually or with &csync;).</para></step>
    <step>
     <para>If you modified any options for an existing cluster, confirm your changes and close the
      cluster module. &yast; writes the configuration to
      <filename>/etc/corosync/corosync.conf</filename>.</para>
    </step>
    <step>
     <para> For further cluster configuration, proceed with <xref
       linkend="sec.ha.installation.setup.services"/>. </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.installation.setup.services">
   <title>Starting Services</title>
   <para>The &yast; cluster module lets you define if to start certain services on a node at
    boot time. You can also use the module to start and stop the services manually. In order to
    bring the cluster nodes online and to start the cluster resource manager, &ais; must be
    started as a service. </para>
   <procedure id="pro.ha.installation.setup.services">
    <title>Starting or Stopping Services</title>
    <step>
     <para> In the &yast; cluster module, switch to the <guimenu>Service</guimenu> category.
     </para>
    </step>
    <step>
     <para> To start &ais; each time this cluster node is booted, select the respective option
      in the <guimenu>Booting</guimenu> group. If you select <guimenu>Off</guimenu> in the
      <guimenu>Booting</guimenu> group, you must start &ais; manually each time this node is
      booted. To start &ais; manually, use the <command>rcopenais&nbsp;start</command>
      command. </para>
    </step>
    <step>
     <para> If you want to use the &hbgui; for configuring, managing and monitoring cluster
      resources, activate <guimenu>Enable mgmtd</guimenu>. This daemon is needed for the GUI.
     </para>
    </step>
    <step>
     <para> To start or stop &ais; immediately, click the respective button. </para>
    </step>
    <step>
     <para>If you modified any options for an existing cluster, confirm your changes and close the
      cluster module.</para>
    </step>
    <step>
         <para>For further cluster configuration, click <guimenu>Next</guimenu> and proceed with <xref
      linkend="sec.ha.installation.setup.csync2"/>. </para>
    </step>
   </procedure>
  </sect2>
  
  <!--taroth 2010-02-02: https://fate.novell.com/308359 (csync2)-->
  <sect2 id="sec.ha.installation.setup.csync2">
   <title>Transferring the Configuration to All Nodes</title>
   <para> Instead of copying the resulting configuration files to all nodes manually, use the
    <command>csync2</command> tool for replication across all nodes in the cluster. </para>
   <para>This requires the following basic steps:</para>
   <procedure>
    <step><para><xref linkend="pro.ha.installation.setup.csync2.yast" xrefstyle="select:title"/></para></step>
    <step><para><xref linkend="pro.ha.installation.setup.csync2.start" xrefstyle="select:title"/></para></step>
   </procedure>
        
   <procedure id="pro.ha.installation.setup.csync2.yast">
     <title>Configuring &csync; with &yast;</title>
    
    <step>
     <para> In the &yast; cluster module, switch to the <guimenu>&csync;</guimenu> category.
     </para>
    </step>
    <step>
     <para>To specify the synchronization group, click <guimenu>Add</guimenu> in the <guimenu>Sync
       Host</guimenu> group and enter the local hostnames of all nodes in your cluster. For each
      node, you must use exactly the strings that are returned by the <command>hostname</command>
      command. </para>
    </step>
    <step id="step.csync2.generate.key">
     <para> Click <guimenu>Generate Pre-Shared-Keys</guimenu> to create a key file for the
      synchronization group. The key file is written to
      <filename>/etc/csync2/key_hagroup</filename>. After it has been created, it must be copied
      manually to all members of the cluster. </para>
    </step>
    <step>
     <para> To populate the <guimenu>Sync File</guimenu> list with the files that usually need to be
      synchronized among all nodes, click <guimenu>Add Suggested Files</guimenu>. </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_sync.png" width="75%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_sync.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para> If you want to <guimenu>Edit</guimenu>, <guimenu>Add</guimenu> or
       <guimenu>Remove</guimenu> files from the list of files to be synchronized use the respective
      buttons. You must enter the absolute pathname for each file. </para>
    </step>
    <step>
     <para> Activate &csync; by clicking <guimenu>Turn &csync; On</guimenu>. This will
      execute <command>chkconfig&nbsp;csync2</command> to start &csync; automatically
      at boot time. 
     </para>
    </step>
    <step>
     <para>If you modified any options for an existing cluster, confirm your changes and close the
      cluster module. &yast; then writes the &csync; configuration to 
      <filename>/etc/csync2/csync2.cfg</filename>. To start the synchronization process now,
      proceed with <xref linkend="pro.ha.installation.setup.csync2.start"/>.</para>
    </step>
    <step>
     <para>For further cluster configuration, click <guimenu>Next</guimenu> and proceed with <!--FIXME-->. </para>
    </step>
    </procedure>
   <procedure id="pro.ha.installation.setup.csync2.start">
    <title>Synchronizing the Configuration Files with &csync;</title>
    <para>To successfully synchronize the files with &csync;, make sure that the following
     prerequisites are met: </para>
    <itemizedlist>
     <listitem>
      <para>The same &csync; configuration is available on all nodes. Copy the file
        <filename>/etc/csync2/csync2.cfg</filename> manually to all nodes after you have configured
       it as described <xref linkend="pro.ha.installation.setup.csync2.yast"/>. Also, it is
       recommended to include this file in the list of files to be synchronized with &csync;.
      </para>
     </listitem>
     <listitem>
      <para>Copy the <filename>/etc/csync2/key_hagroup</filename> file you have generated on one
       node in <xref linkend="step.csync2.generate.key"/> to <emphasis>all</emphasis> nodes in the
       cluster as it is needed for authentication by &csync;. However, do not regenerate
       the file on the other nodes as it needs to be the same file on all nodes. </para>
     </listitem>
     <listitem>
      <para><remark>taroth 2011-09-19: todo: bnc# 718424</remark> Make sure that <systemitem
        class="daemon">xinetd</systemitem> is running on <emphasis>all</emphasis> nodes, as
       &csync; depends on that daemon. Start <systemitem class="daemon">xinetd</systemitem> as
       &rootuser; with the following command: </para>
      <screen>rcxinetd start</screen>
      <note>
       <title>Starting Services at Boot Time</title>
       <para> If you want &csync; and <systemitem class="daemon">xinetd</systemitem> to start
        automatically at boot time, execute the following command on all nodes: </para>
       <screen>chkconfig csync2 on
chkconfig xinetd on</screen>
      </note>
     </listitem>
    </itemizedlist>
      <step>
     <para> Start the initial file synchronization by executing the following command on
       <emphasis>one</emphasis> of the nodes: </para>
     <screen>csync2 <option>-xv</option></screen>
     <para> This will synchronize all the files once. If all files can be synchronized successfully,
      &csync; will finish with no errors. </para>
     <para> If one or several files that are to be synchronized have also been modified on other
      nodes (not only on the current one), &csync; will report a conflict. You will get an
      output similar to the one below: </para>
     <screen>While syncing file /etc/corosync/corosync.conf:
ERROR from peer hex-14: File is also marked dirty here!
Finished with 1 errors.</screen>
    </step>
    <step>
     <para> If you are sure that the file version on the current node is the <quote>best</quote>
      one, you can resolve the conflict by forcing this file and resynchronizing: </para>
     <screen>csync2 -f /etc/corosync/corosync.conf
csync2 -x</screen>
    </step>
   </procedure>
   <para> For more information on the &csync; options run
      <command>csync2&nbsp;<option>-help</option></command>. </para>
   <note>
    <title>Pushing Synchronization After Any Changes</title>
    <para> Be aware that &csync; only pushes changes and it does <emphasis>not</emphasis>
     continuously synchronize files between the nodes. </para>
    <para> Each time you have updated any of the files that need to be synchronized, you need to
     push the changes to the other nodes: Run
     <command>csync2&nbsp;<option>-xv</option></command> on the node where you did the changes.
     If you run the command on any of the other nodes (that have no changes for the synchronized
     files), nothing will happen. </para>
   </note>
     </sect2>
  <sect2 id="sec.ha.installation.setup.conntrackd">
 <title>Synchronizing Firewall Status Between Cluster Nodes</title>
   <para> <systemitem class="resource">conntrackd</systemitem>is a daemon that helps to duplicate
    the firewall status between cluster nodes.</para>
</sect2>
  
  
  </sect1>
 <sect1 id="sec.ha.installation.start">
  <title>Bringing the Cluster Online</title>

  <para> After the initial cluster configuration is done, you can now start the service needed to
   bring the stack online. </para>

  <procedure>
   <title>Starting &ais;/&corosync; and Checking the Status</title>
   <step>
    <para> Run the following command on each of the cluster nodes to start &ais;/&corosync;: </para>
    <screen>rcopenais start</screen>
   </step>
   <step>
    <para> On one of the nodes, check the cluster status with the following command: </para>
    <screen>crm_mon</screen>
    <para> If all nodes are online, the output should be similar to the following: </para>
    <screen>============
Last updated: Tue Mar  2 18:35:34 2010
Stack: openais
Current DC: e229 - partition with quorum
Version: 1.1.1-530add2a3721a0ecccb24660a97dbfdaa3e68f51
2 Nodes configured, 2 expected votes
0 Resources configured.
============
     
 Online: [ e231 e229 ]</screen>
    <para> This output indicates that the cluster resource manager is started and is ready to manage
     resources. </para>
   </step>
  </procedure>

  <para> After the basic configuration is done and the nodes are online, you can now start to
   configure cluster resources. Use either the <command>crm</command> command line tool or the
   graphical user interface. For more information, refer to <xref linkend="cha.ha.configuration.gui"
   /> or <xref linkend="cha.ha.manual_config"/>. </para>
 </sect1>
 <sect1 id="sec.ha.installation.autoyast">
  <title>Mass Deployment with &ay;</title>

  <!--from lmb on [ha-devel] 2010-02-19: 
    That wouldn't be included - that doesn't seem to be something that
    autoyast2 can do, since it installs only a node image, replicated
    configuration files are difficult.

    However, if you clone a base install using autoyast2, I'd expect (I've
    not tested this myself!) that with SP1, it comes up to the state where
    it can receive files via csync2 from the already configured nodes, which
   should get it to the state where it can automatically join the cluster.-->

  <para> &ay; is a system for installing one or more &sle; systems automatically and without
   user intervention. &sle; lets you create a &ay; profile that contains installation and
   configuration data. The profile tells &ay; what to install and how to configure the installed
   system to get a completely ready-to-use system in the end. This profile can then be used for mass
   deployment in different ways. </para>

  <para> For detailed instructions of how to make use of &ay; in various scenarios, see the
   &sle; &productnumber; &deploy;, available at <ulink
    url="http://www.suse.com/documentation"/>. Refer to chapter <citetitle>Automated
    Installation</citetitle>. </para>

  <procedure id="pro.ha.installation.clone.node">
   <title>Cloning a Cluster Node with &ay;</title>
   <para> The following procedure is suitable for deploying cluster nodes which are clones of an
    already existing node. The cloned nodes will have the same packages installed and the same
    system configuration. </para>
   <para> If you need to deploy cluster nodes on non-identical hardware, refer to the
     <citetitle>Rule-Based Autoinstallation</citetitle> section in the &sle; &productnumber;
    &deploy;, available at <ulink url="http://www.suse.com/documentation"/>. </para>
   <important>
    <title>Identical Hardware</title>
    <para> This scenario assumes you are rolling out &productname; &productnumber; to a set
     of machines with exactly the same hardware configuration. </para>
   </important>
   <step>
    <para> Make sure the node you want to clone is correctly installed and configured as described
     in <!--taroth 2011-10-17: FIXME <xref linkend="sec.ha.installation.inst"/>--> and <!--taroth 2011-10-17: FIXME <xref
      linkend="sec.ha.installation.setup"/>-->.
    </para>
   </step>
   <step>
    <para> Follow the description outlined in the &sle; &productnumber; &deploy; for
     simple mass installation. This includes the following basic steps: </para>
    <substeps>
     <step>
      <para> Creating an &ay; profile. Use the &ay; GUI to create and modify a profile from
       the existing system configuration. In &ay;, choose the <guimenu>&ha;</guimenu> module
       and click the <guimenu>Clone</guimenu> button. If needed, adjust the configuration in the
       other modules and save the resulting control file as XML. </para>
     </step>
     <step>
      <para> Determining the source of the &ay; profile and the parameter to pass to the
       installation routines for the other nodes. </para>
     </step>
     <step>
      <para> Determining the source of the &sls; and &productname; installation data.
      </para>
     </step>
     <step>
      <para> Determining and setting up the boot scenario for autoinstallation. </para>
     </step>
     <step>
      <para> Passing the command line to the installation routines, either by adding the parameters
       manually or by creating an <filename>info</filename> file. </para>
     </step>
     <step>
      <para> Starting and monitoring the autoinstallation process. </para>
     </step>
    </substeps>
   </step>
  </procedure>

  <para> After the clone has been successfully installed, execute the following steps to make the
   cloned node join the cluster: </para>

  <procedure id="pro.ha.installation.clone.start">
   <title>Bringing the Cloned Node Online</title>
   <step>
    <para> Transfer the key configuration files from the already configured nodes to the cloned node
     with &csync; as described in <xref linkend="sec.ha.installation.setup.csync2"/>. </para>
   </step>
   <step>
    <para> Start the &ais; service on the cloned node as described in <xref
      linkend="sec.ha.installation.start"/> to bring the node online. </para>
   </step>
  </procedure>

  <para> The cloned node will now join the cluster because the
    <filename>/etc/corosync/corosync.conf</filename> file has been applied to the cloned node via
   &csync;. The CIB is automatically synchronized among the cluster nodes. </para>
 </sect1>
</chapter>
