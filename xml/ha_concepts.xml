<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>

<chapter id="cha.ha.concepts">
 <title>Conceptual Overview</title>
 <abstract>
  <para>
   &productnamereg; is an integrated suite of open source clustering
   technologies that enables you to implement highly available physical and
   virtual Linux clusters, and to eliminate single points of failure. It
   ensures the high availability and manageability of critical network
   resources including data, applications, and services. Thus, it helps you
   maintain business continuity, protect data integrity, and reduce
   unplanned downtime for your mission-critical Linux workloads.
  </para>

  <para>
   It ships with essential monitoring, messaging, and cluster resource
   management functionality (supporting failover, failback, and migration
   (load balancing) of individually managed cluster resources). The &hasi;
   is available as add-on to &sls; &productnumber;.
<!--and also provides you with the means to make virtual machines (containing
   services) highly available. -->
  </para>
<!--  <para>
   This chapter introduces the main product features and benefits of the
   &hasi;. Inside you will find several example clusters and learn about the
   components making up a cluster. The last section provides an overview of
   the architecture, describing the individual architecture layers and
   processes within the cluster.
  </para>-->
 </abstract>
 <sect1 id="sec.ha.features">
  <title>Product Features</title>

  <para>
   &productnamereg; helps you ensure and manage the availability of your
   network resources. The following list highlights some of the key
   features:
  </para>

  <variablelist>
   <varlistentry>
    <term>Support for a Wide Range of Clustering Scenarios</term>
    <listitem>
     <para>
      Including active/active and active/passive (N+1, N+M, N to 1, N to M)
      scenarios, as well as hybrid physical and virtual clusters (allowing
      virtual servers to be clustered with physical servers to improve
      services availability and resource utilization).
     </para>
     <para>
      Multi-node active cluster, containing up to 16 Linux servers. Any
      server in the cluster can restart resources (applications, services,
      IP addresses, and file systems) from a failed server in the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Flexible Solution</term>
    <listitem>
     <para><remark>taroth 2010-01-25: replace openAIS with corosync now
      (in entity-decl)?? - sent mail to ha-devel about this</remark>
      The &hasi; ships with &corosync;/&ais; messaging and membership layer and
      Pacemaker Cluster Resource Manager. Using Pacemaker, administrators
      can continually monitor the health and status of their resources,
      manage dependencies, and automatically stop and start services based
      on highly configurable rules and policies. The &hasi; allows you to
      tailor a cluster to the specific applications and hardware
      infrastructure that fit your organization. Time-dependent
      configuration enables services to automatically migrate back to
      repaired nodes at specified times.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Storage and Data Replication</term>
    <listitem>
     <para>
      With the &hasi; you can dynamically assign and reassign server storage
      as needed. It supports Fibre Channel or iSCSI storage area networks
      (SANs). Shared disk systems are also supported, but they are not a
      requirement. &productname; also comes with a cluster-aware file system
      (Oracle Cluster File System, OCFS2) and volume manager (clustered
      Logical Volume Manager, cLVM). For replication of your data, the
      &hasi; also delivers DRBD (Distributed Replicated Block Device) which
      you can use to mirror the data of a high availably service from the
      active node of a cluster to its standby node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Support for Virtualized Environments</term>
    <listitem>
     <para>
      &productname; supports the mixed clustering of both physical and
      virtual Linux servers. &sls; &productnumber; ships with Xen, an open
      source virtualization hypervisor. The cluster resource manager in the
      &hasi; is able to recognize, monitor and manage services running
      within virtual servers created with Xen, as well as services running
      in physical servers. Guest systems can be managed as services by the
      cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Resource Agents</term>
    <listitem>
     <para>
      &productname; includes a huge number of resource agents to manage
      resources such as Apache, IPv4, IPv6 and many more. It also ships with
      resource agents for popular third party applications such as IBM
      WebSphere Application Server. For a list of Open Cluster Framework
      (OCF) resource agents included with your product, refer to
      <xref linkend="app.agents.details"/>. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>User-friendly Administration</term>
    <listitem>
     <para>
      For easy configuration and administration, the &hasi; ships with both
      a graphical user interface (like &yast; and the &hbgui;) and a
      powerful unified command line interface. Both approaches provide a
      single point of administration for effectively monitoring and
      administrating your cluster. Learn how to do so in the following
      chapters.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 id="sec.ha.benefits">
  <title>Product Benefits</title>

  <para>
     The &hasi; allows you to configure up to 16 Linux servers into a
   high-availability cluster (HA cluster), where resources can be
   dynamically switched or moved to any server in the cluster. Resources can
   be configured to automatically migrate in the event of a server failure,
   or they can be moved manually to troubleshoot hardware or balance the
   workload.
  </para>

  <para>
   The &hasi; provides high availability from commodity components. Lower
   costs are obtained through the consolidation of applications and
   operations onto a cluster. The &hasi; also allows you to centrally manage
   the complete cluster and to adjust resources to meet changing workload
   requirements (thus, manually <quote>load balance</quote> the cluster).
   Allowing clusters of more than two nodes also provides savings by
   allowing several nodes to share a <quote>hot spare</quote>.
  </para>

  <para>
   An equally important benefit is the potential reduction of unplanned
   service outages as well as planned outages for software and hardware
   maintenance and upgrades.
  </para>

  <para>
   Reasons that you would want to implement a cluster include:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Increased availability
    </para>
   </listitem>
   <listitem>
    <para>
     Improved performance
    </para>
   </listitem>
   <listitem>
    <para>
     Low cost of operation
    </para>
   </listitem>
   <listitem>
    <para>
     Scalability
    </para>
   </listitem>
   <listitem>
    <para>
     Disaster recovery
    </para>
   </listitem>
   <listitem>
    <para>
     Data protection
    </para>
   </listitem>
   <listitem>
    <para>
     Server consolidation
    </para>
   </listitem>
   <listitem>
    <para>
     Storage consolidation
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Shared disk fault tolerance can be obtained by implementing RAID on the
   shared disk subsystem.
  </para>

  <para>
   The following scenario illustrates some of the benefits the &hasi; can
   provide.
  </para>

  <bridgehead>Example Cluster Scenario</bridgehead>

  <para>
   Suppose you have configured a three-server cluster, with a Web server
   installed on each of the three servers in the cluster. Each of the
   servers in the cluster hosts two Web sites. All the data, graphics, and
   Web page content for each Web site are stored on a shared disk subsystem
   connected to each of the servers in the cluster. The following figure
   depicts how this setup might look.
  </para>

  <figure>
   <title>Three-Server Cluster</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example1.png" width="90%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example1.png" width="90%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   During normal cluster operation, each server is in constant communication
   with the other servers in the cluster and performs periodic polling of
   all registered resources to detect failure.
  </para>

  <para>
   Suppose Web Server 1 experiences hardware or software problems and the
   users depending on Web Server 1 for Internet access, e-mail, and
   information lose their connections. The following figure shows how
   resources are moved when Web Server 1 fails.
  </para>

  <figure>
   <title>Three-Server Cluster after One Server Fails</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example2.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example2.png" width="75%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Web Site A moves to Web Server 2 and Web Site B moves to Web Server 3. IP
   addresses and certificates also move to Web Server 2 and Web Server 3.
  </para>

  <para>
   When you configured the cluster, you decided where the Web sites hosted
   on each Web server would go should a failure occur. In the previous
   example, you configured Web Site A to move to Web Server 2 and Web Site B
   to move to Web Server 3. This way, the workload once handled by Web
   Server 1 continues to be available and is evenly distributed between any
   surviving cluster members.
  </para>

  <para>
   When Web Server 1 failed, the &hasi; software
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Detected a failure and verified with &stonith; that Web Server 1 was
     really dead
    </para>
   </listitem>
   <listitem>
    <para>
     Remounted the shared data directories that were formerly mounted on Web
     server 1 on Web Server 2 and Web Server 3.
    </para>
   </listitem>
   <listitem>
    <para>
     Restarted applications that were running on Web Server 1 on Web Server
     2 and Web Server 3
    </para>
   </listitem>
   <listitem>
    <para>
     Transferred IP addresses to Web Server 2 and Web Server 3
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In this example, the failover process happened quickly and users regained
   access to Web site information within seconds, and in most cases, without
   needing to log in again.
  </para>

  <para>
   Now suppose the problems with Web Server 1 are resolved, and Web Server 1
   is returned to a normal operating state. Web Site A and Web Site B can
   either automatically fail back (move back) to Web Server 1, or they can
   stay where they are. This is dependent on how you configured the
   resources for them. Migrating the services back to Web Server 1 will
   incur some down-time, so the &hasi; also allows you to defer the
   migration until a period when it will cause little or no service
   interruption. There are advantages and disadvantages to both
   alternatives.
  </para>

  <para>
   The &hasi; also provides resource migration capabilities. You can move
   applications, Web sites, etc. to other servers in your cluster as
   required for system management.
  </para>

  <para>
   For example, you could have manually moved Web Site A or Web Site B from
   Web Server 1 to either of the other servers in the cluster. You might
   want to do this to upgrade or perform scheduled maintenance on Web Server
   1, or just to increase performance or accessibility of the Web sites.
  </para>
 </sect1>
 <sect1 id="sec.ha.clusterconfig">
  <title>Cluster Configurations</title>

  <para>
   Cluster configurations with the &hasi; might or might not include a
   shared disk subsystem. The shared disk subsystem can be connected via
   high-speed Fibre Channel cards, cables, and switches, or it can be
   configured to use iSCSI. If a server fails, another designated server in
   the cluster automatically mounts the shared disk directories that were
   previously mounted on the failed server. This gives network users
   continuous access to the directories on the shared disk subsystem.
  </para>

  <important>
   <title>Shared Disk Subsystem with cLVM</title>
   <para>
    When using a shared disk subsystem with cLVM, that subsystem must be
    connected to all servers in the cluster from which it needs to be
    accessed.
   </para>
  </important>

  <para>
   Typical resources might include data, applications, and services. The
   following figure shows how a typical Fibre Channel cluster configuration
   might look.
  </para>

  <figure>
   <title>Typical Fibre Channel Cluster Configuration</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example3.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example3.png" width="100%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Although Fibre Channel provides the best performance, you can also
   configure your cluster to use iSCSI. iSCSI is an alternative to Fibre
   Channel that can be used to create a low-cost Storage Area Network (SAN).
   The following figure shows how a typical iSCSI cluster configuration
   might look.
  </para>

  <figure>
   <title>Typical iSCSI Cluster Configuration</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example4.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example4.png" width="100%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Although most clusters include a shared disk subsystem, it is also
   possible to create a cluster without a share disk subsystem. The
   following figure shows how a cluster without a shared disk subsystem
   might look.
  </para>

  <figure>
   <title>Typical Cluster Configuration Without Shared Storage</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_cluster_example5.png" width="100%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_cluster_example5.png" width="100%" format="png"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>

 <sect1 id="sec.ha.architecture">
  <title>Architecture</title>

  <para>
   This section provides a brief overview of the &hasi; architecture. It
   identifies and provides information on the architectural components, and
   describes how those components interoperate.
  </para>

  <sect2 id="sec.ha.architecture.layers">
   <title>Architecture Layers</title>
   <para>
    The &hasi; has a layered architecture.
    <xref linkend="fig.ha.architecture"
     xrefstyle="FigureXRef"/>
    illustrates the different layers and their associated components.
   </para>
   <figure id="fig.ha.architecture">
    <title>Architecture</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ha_cluster_components_arch.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ha_cluster_components_arch.png" width="100%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3 id="sec.ha.architecture.layers.messaging">
    <title>Messaging and Infrastructure Layer</title>
    <para>
     The primary or first layer is the messaging/infrastructure layer, also
     known as the &corosync;/&ais; layer. This layer contains components that send out
     the messages containing <quote>I'm alive</quote> signals, as well as
     other information. The program of the &hasi; resides in the
     messaging/infrastructure layer.
    </para>
   </sect3>
   <sect3 id="sec.ha.architecture.layers.allocation">
    <title>Resource Allocation Layer</title>
    <para>
     The next layer is the resource allocation layer. This layer is the most
     complex, and consists of the following components:
    </para>
    <variablelist>
     <varlistentry id="vle.crm">
      <term>Cluster Resource Manager (CRM)</term>
      <listitem>
       <para>
        Every action taken in the resource allocation layer passes through
        the Cluster Resource Manager. If other components of the resource
        allocation layer (or components which are in a higher layer) need to
        communicate, they do so through the local CRM.
       </para>
       <para>
        On every node, the CRM maintains the
        <xref linkend="vle.cib" xrefstyle="select:title"
        />,
        containing definitions of all cluster options, nodes, resources
        their relationship and current status. One CRM in the cluster is
        elected as the Designated Coordinator (DC), meaning that it has the
        master CIB. All other CIBs in the cluster are a replicas of the
        master CIB. Normal read and write operations on the CIB are
        serialized through the master CIB. The DC is the only entity in the
        cluster that can decide that a cluster-wide change needs to be
        performed, such as fencing a node or moving resources around.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="vle.cib">
      <term>Cluster Information Base (CIB)</term>
      <listitem>
       <para>
        The Cluster Information Base is an in-memory XML representation of
        the entire cluster configuration and current status. It contains
        definitions of all cluster options, nodes, resources, constraints
        and the relationship to each other. The CIB also synchronizes
        updates to all cluster nodes. There is one master CIB in the
        cluster, maintained by the DC. All other nodes contain a CIB
        replica.
       </para>
<!--taroth 090211: handed over the following snippet to toms for possible use in
        the CLI chapters as it rather belongs there-->
<!--<para>If an administrator wants to manipulate the cluster's behavior, he
        can use either the <literal>cibadmin</literal> command line tool or
        the GUI tool.
       </para>
       
       <note>
        <title>Usage of GUI Tool and <command>cibadmin</command></title>
        <para>
         The GUI tool can be used from any machine with a connection to the
         cluster. The <command>cibadmin</command> command must be used on a
         cluster node, and is not restricted to only the DC node.
        </para>
       </note>-->
      </listitem>
     </varlistentry>
     <varlistentry id="vle.pe.and.te">
      <term>Policy Engine (PE)</term>
      <listitem>
       <para>
        Whenever the Designated Coordinator needs to make a cluster-wide
        change (react to a new CIB), the Policy Engine calculates the next
        state of the cluster based on the current state and configuration.
        The PE also produces a transition graph containing a list of
        (resource) actions and dependencies to achieve the next cluster
        state.
        The PE runs on every node to speed up DC failover.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry id="vle.lrm">
      <term>Local Resource Manager (LRM)</term>
      <listitem>
       <para>
        The LRM calls the local Resource Agents (see
        <xref
         linkend="sec.ha.architecture.layers.resources"/>) on
        behalf of the CRM. It can thus perform start / stop / monitor
        operations and report the result to the CRM. It also hides the
        difference between the supported script standards for Resource
        Agents (OCF, LSB, Heartbeat Version 1). The LRM is the authoritative
        source for all resource-related information on its local node.
<!--In &productname;, the LRM is
        implemented as daemon, interacting directly with resource agents.
        The daemon is not cluster-aware and presents a common interface to
        the supported resource types.-->
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 id="sec.ha.architecture.layers.resources">
    <title>Resource Layer</title>
    <para>
     The highest layer is the Resource Layer. The Resource Layer includes
     one or more Resource Agents (RA). Resource Agents are programs (usually
     shell scripts) that have been written to start, stop, and monitor a
     certain kind of service (a resource). Resource Agents are called only
     by the LRM. Third parties can include their own agents in a defined
     location in the file system and thus provide out-of-the-box cluster
     integration for their own software.
    </para>
   </sect3>
  </sect2>

  <sect2 id="sec.ha.architecture.processflow">
   <title>Process Flow</title>
   <para>
    &productname; uses Pacemaker as CRM. The CRM is implemented as daemon
    (<literal>crmd</literal>) that has an instance on each cluster node.
    Pacemaker centralizes all cluster decision-making by electing one of the
    crmd instances to act as a master. Should the elected crmd process (or
    the node it is on) fail, a new one is established.
   </para>
   <para>
    A CIB, reflecting the cluster’s configuration and current state of all
    resources in the cluster is kept on each node. The contents of the CIB
    are automatically kept in sync across the entire cluster.
   </para>
   <para>
    Many actions performed in the cluster will cause a cluster-wide change.
    These actions can include things like adding or removing a cluster
    resource or changing resource constraints. It is important to understand
    what happens in the cluster when you perform such an action.
   </para>
   <para>
    For example, suppose you want to add a cluster IP address resource. To
    do this, you can use one of the command line tools or the GUI to modify
    the CIB. It is not required to perform the actions on the DC, you can
    use either tool on any node in the cluster and they will be relayed to
    the DC. The DC will then replicate the CIB change to all cluster nodes.
   </para>
   <para>
    Based on the information in the CIB, the PE then computes the ideal
    state of the cluster and how it should be achieved and feeds a list of
    instructions to the DC. The DC sends commands via the
    messaging/infrastructure layer which are received by the crmd peers on
    other nodes. Each crmd uses it LRM (implemented as lrmd) to perform
    resource modifications. The lrmd is non-cluster aware and interacts
    directly with resource agents (scripts).
   </para>
   <para>
    The peer nodes all report the results of their operations back to the
    DC. Once the DC concludes that all necessary operations are successfully
    performed in the cluster, the cluster will go back to the idle state and
    wait for further events. If any operation was not carried out as
    planned, the PE is invoked again with the new information recorded in
    the CIB.
   </para>
   <para>
    In some cases, it may be necessary to power off nodes in order to
    protect shared data or complete resource recovery. For this Pacemaker
    comes with a fencing subsystem, stonithd. &stonith; is an acronym for
    <quote>Shoot The Other Node In The Head</quote> and is usually
    implemented with a remote power switch. In Pacemaker, &stonith; devices
    are modeled as resources (and configured in the CIB) to enable them to
    be easily monitored for failure. However, stonithd takes care of
    understanding the &stonith; topology such that its clients simply
    request a node be fenced and it does the rest.
   </para>
  </sect2>
 </sect1>
 <sect1 id="sec.ha.news">
  <title>What's New?</title>
  <remark>taroth 2010-01-25: keep this section for SP1? mention changes from SLEHA
   11 to SLEHA 11 SP1 in a separate section? </remark>
  <para>
   With &sls; 11, the cluster stack has changed from &hb; to &ais;. &ais;
   implements an industry standard API, the Application Interface
   Specification (AIS), published by the Service Availability Forum. The
   cluster resource manager from &sls; 10 has been retained but has been
   significantly enhanced, ported to &ais; and is now known as Pacemaker.
  </para>

  <para>
   For more details what changed in the &ha; components from &slsreg; 10 SP2
   to &productname; &productnumber;, refer to the following sections.
  </para>

  <sect2 id="sec.ha.news.new">
   <title>New Features and Functions Added</title>
   <variablelist>
    <varlistentry id="vle.ha.news.fail">
     <term>Migration Threshold and Failure Timeouts</term>
     <listitem>
      <para>
       The &hasi; now comes with the concept of a migration threshold and
       failure timeout. You can define a number of failures for resources,
       after which they will migrate to a new node. By default, the node
       will no longer be allowed to run the failed resource until the
       administrator manually resets the resource’s failcount. However it
       is also possible to expire them by setting the resource’s
       <literal>failure-timeout</literal> option.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry id="vle.ha.news.defaults">
     <term>Resource and Operation Defaults</term>
     <listitem>
      <para>
       You can now set global defaults for resource options and operations.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Support for Offline Configuration Changes</term>
     <listitem>
      <para>
       Often it is desirable to preview the effects of a series of changes
       before updating the conﬁguration atomically. You can now create a
       <quote>shadow</quote> copy of the conﬁguration that can be edited
       with the command line interface, before committing it and thus
       changing the active cluster configuration atomically.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Reusing Rules, Options and Sets of Operations</term>
     <listitem>
      <para>
       Rules, instance_attributes, meta_attributes and sets of operations
       can be deﬁned once and referenced in multiple places.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Using XPath Expressions for Certain Operations in the CIB</term>
     <listitem>
      <para>
       The CIB now accepts XPath-based <literal>create</literal>,
       <literal>modify</literal>, <literal>delete</literal> operations. For
       more information, refer to the <command>cibadmin</command> help text.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Multi-dimensional Collocation and Ordering Constraints</term>
     <listitem>
      <para>
       For creating a set of collocated resources, previously you could
       either deﬁne a resource group (which could not always accurately
       express the design) or you could deﬁne each relationship as an
       individual constraint&mdash;causing a constraint explosion as the
       number of resources and combinations grew. Now you can also use an
       alternate form of collocation constraints by defining
       <literal>resource_sets</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Connection to the CIB From Non-cluster Machines</term>
     <listitem>
      <para>
       Provided Pacemaker is installed on a machine, it is possible to
       connect to the cluster even if the machine itself is not a part of
       it.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Triggering Recurring Actions at Known Times</term>
     <listitem>
      <para>
       By default, recurring actions are scheduled relative to when the
       resource started, but this is not always desirable. To specify a
       date/time that the operation should be relative to, set the
       operation’s interval-origin. The cluster uses this point to
       calculate the correct start-delay such that the operation will occur
       at origin + (interval * N).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 id="sec.ha.news.changed">
   <title>Changed Features and Functions</title>
   <variablelist>
    <varlistentry id="vle.ha.news.options">
     <term>Naming Conventions for Resource and Custer Options</term>
     <listitem>
      <para>
       All resource and cluster options now use dashes (-) instead of
       underscores (_). For example, the <literal>master_max</literal> meta
       option has been renamed to <literal>master-max</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Renaming of <literal>master_slave</literal> Resource </term>
     <listitem>
      <para>
       The <literal>master_slave</literal> resource has been renamed to
       <literal>master</literal>. Master resources are a special type of
       clone that can operate in one of two modes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Container Tag for Attributes</term>
     <listitem>
      <para>
       The <literal>attributes</literal> container tag has been removed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Operation Field for Prerequisites</term>
     <listitem>
      <para>
       The <literal>pre-req</literal> operation field has been renamed
       <literal>requires</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Interval for Operations</term>
     <listitem>
      <para>
       All operations must have an interval. For start/stop actions the
       interval must be set to <literal>0</literal> (zero).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Attributes for Collocation and Ordering Constraints</term>
     <listitem>
      <para>
       The attributes of collocation and ordering constraints were renamed
       for clarity.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Cluster Options for Migration Due to Failure</term>
     <listitem>
      <para>
       The <literal>resource-failure-stickiness</literal> cluster option has
       been replaced by the <literal>migration-threshold</literal> cluster
       option. See also <xref
        linkend="vle.ha.news.fail"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Arguments for Command Line Tools</term>
     <listitem>
      <para>
       The arguments for command-line tools have been made consistent. See
       also <xref
        linkend="vle.ha.news.options"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Validating and Parsing XML</term>
     <listitem>
      <para>
       The cluster configuration is written in XML. Instead of a Document
       Type Definition (DTD), now a more powerful RELAX&nbsp;NG schema is
       used to define the pattern for the structure and content.
       <literal>libxml2</literal> is used as parser.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>id</literal> Fields</term>
     <listitem>
      <para>
       <literal>id</literal> ﬁelds are now XML IDs which have the
       following limitations:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         IDs cannot contain colons.
        </para>
       </listitem>
       <listitem>
        <para>
         IDs cannot begin with a number.
        </para>
       </listitem>
       <listitem>
        <para>
         IDs must be globally unique (not just unique for that tag).
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>References to Other Objects</term>
     <listitem>
      <para>
       Some ﬁelds (such as those in constraints that refer to resources)
       are IDREFs. This means that they must reference existing resources or
       objects in order for the conﬁguration to be valid. Removing an
       object which is referenced elsewhere will therefor fail.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Removed Features and Functions</title>
   <variablelist>
    <varlistentry>
     <term>Setting Resource Meta Options</term>
     <listitem>
      <para>
       It is no longer possible to set resource meta-options as top-level
       attributes. Use meta attributes instead.
       <remark>taroth 090218: reference the crm_resource --meta command here
        (request by abeekhof).</remark>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Setting Global Defaults</term>
     <listitem>
      <para>
       Resource and operation defaults are no longer read from
       <literal>crm_conﬁg</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
