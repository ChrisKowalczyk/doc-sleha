<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--
 toms 2016-08-04: TODO

 This Quick Start should link to the "Installation and Setup Quick Start";
 this would cover all installation and setup of a two-node cluster.
 The rest (DRBD, LVM & NFS) would be covered by this guide.

-->
<?provo dirname="nfs_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art_ha_quick_nfs"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
<?suse-quickstart columns="no" version="2"?>
 <title>&nfsquick;</title>
 <subtitle>&productname; &productnumber;</subtitle>
 <info>
  <productnumber>&productnumber;</productnumber>
  <productname>&productname;</productname>
  <abstract>
   <para>
    &abstract-nfsquick;
   </para>
  </abstract>
  <xi:include href="ha_authors.xml"/>
   <dm:docmanager>
    <dm:bugtracker>
      <dm:product>SUSE Linux Enterprise High Availability Extension 12 SP2</dm:product>
      <dm:component>Documentation</dm:component>
    </dm:bugtracker>
  </dm:docmanager>
 </info>
<?suse-quickstart columns="no" version="2"?>
 <sect1 xml:id="sec_ha_quick_nfs_intro">
  <title>Introduction</title>
  <para>
   NFS (the Network File System) is one of the most long-lived and
   ubiquitous networked storage solutions on Linux. The solution described
   in this document is applicable to NFS clients using version 3 or version 4.
  </para>
 </sect1>
 <sect1 xml:id="sec_ha_quick_nfs_usagescenario">
  <title>Usage Scenario</title>
  <para>
   The procedures in this document will lead to a highly available NFS server
   with the following properties:
  </para>

  <itemizedlist>
   <!-- Taken from art_sle_ha_install_quick.xml: -->
   <listitem>
    <para>
     Two nodes: <systemitem class="server">&node1;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.1</systemitem>)
     and <systemitem class="server">&node2;</systemitem> (IP: <systemitem
      class="ipaddress">&subnetI;.2</systemitem>),
     connected to each other via network.
    </para>
   </listitem>
   <listitem>
    <para>
     Two floating, virtual IP addresses (<systemitem class="ipaddress"
      >&nfs-vip-hawk;</systemitem> and <systemitem class="ipaddress"
      >&nfs-vip-exports;</systemitem>) which allows clients to connect to
     the service no matter which physical node it is running on.
     One is used for cluster administration with &hawk2;, the other IP address
     is used exclusively for the NFS exports.
    </para>
   </listitem>
   <listitem>
    <para>A shared storage device, used as SBD fencing mechanism.
     This avoids split brain scenarios.
    </para>
   </listitem>
   <listitem>
    <para>
     Failover of resources from one node to the other if the active host breaks
     down (<emphasis>active/passive</emphasis> setup).
    </para>
   </listitem>
   <listitem>
    <para>
     Local storage on each host. The data is synchronized between the
     hosts using DRBD on top of LVM.
    </para>
   </listitem>
   <listitem>
    <para>
     An exported file system through NFS.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   After installing and setting up of the basic two-node cluster, extending it
   with storage and cluster resources for NFS, this will lead to a highly
   available NFS storage server.
  </para>
 </sect1>

 <sect1 xml:id="sec_ha_quick_nfs_installation">
  <title>Installing a Basic Two-Node Cluster</title>
  <para>
   Before you proceed any further, you need to install and set up a basic
   two-node cluster first. This task is described in our
   <citetitle>&instquick;</citetitle>. The &instquick; uses
   the <package>ha-cluster-bootstrap</package> package to help you with
   setting up a cluster with minimal effort.
  </para>
  <para>
   Proceed with the &instquick; and come back.
  </para>
  <para>After the basic two-node cluster is set up, enable and start the
   NFS service on both nodes:</para>
  <screen>&prompt.root;<command>systemctl</command> enable nfs-server
&prompt.root;<command>systemctl</command> start nfs-server</screen>
 </sect1>

 <sect1 xml:id="sec_ha_quick_nfs_lvm">
   <title>Creating a LVM Device</title>
   <para>LVM (<emphasis>Logical Volume Manager</emphasis>) enables
    flexible distribution of hard disk space over several file
    systems. It was developed because sometimes the need to change the
    segmenting of hard disk space arises just after the initial
    partitioning has been done.
   </para>
   <para>To prepare your disks for using LVM, do the following:</para>
   <procedure>
    <step>
     <para>
      Create an LVM volume group and replace <filename>/dev/sdb1</filename>
      with your corresponding device for LVM:</para>
     <screen>&prompt.root;<command>pvcreate</command> /dev/sdb1</screen>
    </step>
    <step>
     <para>Create an LVM Volume Group <systemitem>nfs</systemitem>
            that includes this physical volume: </para>
     <screen>&prompt.root;<command>vgcreate</command> nfs /dev/sdb1</screen>
    </step>
    <step>
      <para>
       Create one or more logical volumes in the volume group
       <systemitem>nfs</systemitem>. This example assumes 20 Gigabyte, named
       <systemitem>work</systemitem>:
      </para>
      <screen>&prompt.root;<command>lvcreate</command> -n work -L 20G nfs</screen>
     </step>
     <step>
      <para>
       Activate the volume group<!-- and create file systems on the new logical
       volumes. This example assumes <literal>ext3</literal> as the file
       system type-->: </para>
<screen>&prompt.root;<command>vgchange</command> -ay nfs</screen>
     </step>
   </procedure>
   <para>After you have successfully executed the above steps, your system
    will make the device <filename>/dev/<replaceable
     >VOLGROUP</replaceable>/<replaceable
      >LOGICAL_VOLUME</replaceable></filename> visible.
    In this case it will be <filename>/dev/nfs/work</filename>.
   </para>
  </sect1>

 <sect1 xml:id="sec_ha_quick_nfs_drbd_device">
   <title>Creating a DRBD Device</title>
   <para>
    This section describes how to set up a DRBD device on top of LVM. Using
    LVM as a backend of DRBD has some benefits to configure it this way:
    configuring, adding, and resizing is easier than using LVM on top of DRBD.
   </para>
  <para>
    As the LVM volume group is named <literal>nfs</literal> hence the
    DRBD resource uses the same name.
   </para>

   <sect2 xml:id="sec.ha_quick_nfs_drbd_config">
    <title>Creating DRBD Configuration</title>
    <para>
     It is highly recommended to follow these advise:
    </para>

    <itemizedlist>
     <listitem>
      <para>Use the directory <filename>/etc/drbd.d/</filename> for your
      configuration.</para>
     </listitem>
     <listitem>
      <para>Name the file to its purpose of the resource.</para>
     </listitem>
     <listitem>
      <para>Put your resource configuration in a file with a <filename
       class="extension">.res</filename> extension.</para>
     </listitem>
    </itemizedlist>

    <para>
     This example uses the <filename>/etc/drbd.d/nfs.res</filename> file.
     Proceed as follows:
    </para>
    <procedure>
     <title>Creating DRBD Configuration</title>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/nfs.res</filename> with the
        following contents:
      </para>
      <remark>toms 2016-07-25: TODO bsc#981560</remark>
<screen>resource nfs {
   device /dev/drbd0; <co xml:id="co.ha_quick_nfs_drbd.device"/>
   disk   /dev/nfs/work; <co xml:id="co.ha_quick_nfs_drbd.disk"/>
   meta-disk internal; <co xml:id="co.ha_quick_nfs_drbd.metadisk"/>

   net {
      protocol	C; <co xml:id="co.ha_quick_nfs_drbd.protocol"/>
   }

   connection-mesh {
      host	&node1; &node2;;
   }
   on &node1; { <co xml:id="co.ha_quick_nfs_drbd.on"/>
      address   &subnetI;.1:&drbd.port;;
      node-id   0;
   }
   on &node2; { <xref linkend="co.ha_quick_nfs_drbd.on"/>
      address   &subnetI;.2:&drbd.port;;
      node-id   1;
   }
}</screen>
      <calloutlist>
       <callout arearefs="co.ha_quick_nfs_drbd.device">
        <para>The DRBD device that applications are supposed to access.</para>
       </callout>
       <callout arearefs="co.ha_quick_nfs_drbd.disk">
        <para>The lower-level block device used by DRBD to store the actual
         data. This is the LVM device that was created in <xref
          linkend="sec_ha_quick_nfs_lvm"/>.
        </para>
       </callout>
       <callout arearefs="co.ha_quick_nfs_drbd.metadisk">
        <para>Where the meta data format is stored. With
         <literal>internal</literal>, the meta data is stored together with
         the user data on the same device.
        </para>
       </callout>
       <callout arearefs="co.ha_quick_nfs_drbd.protocol">
        <para>The specified protocol to be used for this connection. For protocol
         <literal>C</literal>, a write is considered to be complete as soon as
         it has reached all disks, be it local or remote.
        </para>
       </callout>
       <callout arearefs="co.ha_quick_nfs_drbd.on">
        <para>Contains the IP address and a unique identifier for each node.</para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Open <filename>/etc/csync2/csync2.cfg</filename> and check whether the
       following two lines exist:
      </para>
<screen>include /etc/drbd.conf;
include /etc/drbd.d;</screen>
      <para>
       If not, add them to the file.
      </para>
     </step>
     <step>
      <para>
       Copy the file to the other nodes:
      </para>
<screen>&prompt.root;<command>csync2</command> -xv</screen>
      <para>
       For information about &csync;, refer to
       <xref linkend="sec.ha.installation.setup.csync2"/>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 xml:id="sec_ha_quick_nfs_drbd_activate">
    <title>Activating the DRBD Device</title>
    <para>
     After you have prepared your DRBD configuration, proceed as follows:
    </para>

    <!--
    Taken some steps from https://github.com/SUSE/doc-sleha/commit/5bb10f7fc6
    -->
    <procedure>
     <step>
      <para> If you use a firewall in your cluster, open port
              &drbd.port; in your firewall configuration. </para>
     </step>
     <step>
      <para>The first time you are doing this, execute the following
        commands on <emphasis>both</emphasis> nodes (in our example, &node1;
        and &node2;):
      </para>
<screen>&prompt.root;<command>drbdadm</command> create-md nfs
&prompt.root;<command>drbdadm</command> up nfs</screen>
      <para> This initializes the metadata storage and creates the
              <filename>/dev/drbd0</filename> device.
      </para>
     </step>
     <step>
      <para>
       If the DRBD devices on all nodes have the same data, you can skip
       the initial resychronization. Use the following command:
      </para>
      <screen>&prompt.root;<command>drbdadm</command
              > new-current-uuid --clear-bitmap nfs/0</screen>
     </step>
     <step>
       <para>Make &node1; primary:</para>
       <screen>&prompt.root;<command>drbdadm</command> primary --force nfs</screen>
     </step>
      <step>
       <para>Check the DRBD status:</para>
       <screen>&prompt.root;<command>drbdadm</command> status nfs</screen>
       <para>This returns the following message:</para>
          <screen>nfs role:Primary
  disk:UpToDate
  kemter-3 role:Secondary
    peer-disk:UpToDate</screen>
     </step>
    </procedure>
    <para>
     After the sychronization is finished, you can access the DRBD resource
     on the block device <filename>/dev/drbd0</filename>. Use this device
     for creating your file system.
     Find more information about DRBD in <xref linkend="cha.ha.drbd"/>.
     </para>
   </sect2>

   <sect2 xml:id="sec_ha_quick_nfs_drbd_createfs">
    <title>Creating the File System</title>
    <para>After you have finished <xref linkend="sec_ha_quick_nfs_drbd_activate"/>
    you should see a DRBD device on <filename>/dev/drbd0</filename>.
    </para>
    <screen>&prompt.root;<command>mkfs.ext3</command> /dev/drbd0</screen>
   </sect2>
  </sect1>

  <!--
  <sect1 xml:id="sec_ha_quick_nfs_drbd">
   <title></title>
   <para></para>
   <important>
     <title>Automatic Synchronization</title>
     <para>
      Execute all of the following steps only on the node where your
      resource is currently in the primary role. It is
      <emphasis>not</emphasis> necessary to repeat the commands on the DRBD
      peer node as the changes are automatically synchronized.
     </para>
    </important>

    <procedure>
     <step>
      <para>x</para>
     </step>
    </procedure>
  </sect1>
-->

 <sect1 xml:id="sec_ha_quick_nfs_initial_pacemaker">
  <title>Adjusting Pacemaker's Configuration</title>
  &failback-nodes;
  <para>
    To adjust the option, open the &crmshell; as &rootuser; (or any
    non-&rootuser; user that is part of the
    <systemitem class="groupname">haclient</systemitem> group) and run the
    following commands:
   </para>
   <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>rsc_defaults</command> resource-stickiness="200"
&prompt.crm.conf;<command>commit</command></screen>

<!--
   <formalpara>
   <title>With &hawk2;</title>
   <para>
   To adjust the option in &hawk2;, click <guimenu>Cluster Configuration</guimenu>.
   In the text field <guimenu>resource-stickiness</guimenu> enter
   <literal>200</literal>. Confirm with <guimenu>Apply</guimenu>.
  </para>
  </formalpara>
-->
   <para>
    For more information about global cluster options, refer to
    <xref linkend="sec.ha.config.basics.global"/>.
   </para>
 </sect1>

 <sect1 xml:id="sec_ha_quick_nfs_resources">
  <title>Creating Cluster Resources<!-- for an HA NFS Server--></title>
  <para>The following sections cover in detail how to configure
    the required resources for a highly available NFS cluster.
    The configuration steps use the &crmshell;.
    The following list shows the needed cluster resources:
  </para>
  <variablelist xml:id="vl.ha.quick.nfs_overview-cluster-res">
   <title>Overview of Cluster Resources</title>
   <varlistentry>
    <term>DRBD Primitive and Multi-state Resource
     <!--<xref linkend="sec_ha_quick_nfs_resources_drbd" xrefstyle="select:title"/>-->
    </term>
    <listitem>
     <para>
      These resources are used to replicate data. The master/slave resource
      is switched from and to the Primary and Secondary roles as deemed
      necessary by the cluster resource manager.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS Kernel Server Resource
     <!--<xref linkend="sec_ha_quick_nfs_resources_nfsserver" xrefstyle="select:title"/>-->
    </term>
    <listitem>
     <para>
      With this resource, Pacemaker ensures that the NFS server daemons are
      always available.
     </para>
    </listitem>
   </varlistentry>
   <!--<varlistentry>
    <term>LVM and File System Resources
     <!-\-<xref linkend="sec_ha_quick_nfs_resources_lvm" xrefstyle="select:title"/>-\->
    </term>
    <listitem>
     <para>
      The LVM Volume Group is made available on whichever node currently
      holds the DRBD resource in the primary role. Apart from that, you need
      resources for one or more file systems residing on any of the Logical
      Volumes in the Volume Group. They are mounted by the cluster manager
      wherever the Volume Group is active.
     </para>
    </listitem>
   </varlistentry>-->
   <varlistentry>
    <term>NFSv4 Virtual File System Root
     <!--<xref linkend="sec_ha_quick_nfs_resources_nfsexport_nfsv4root" xrefstyle="select:title"/>-->
    </term>
    <listitem>
     <para>
      A virtual NFS root export (only needed for NFSv4 clients).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Non-root NFS Exports
     <!--<xref linkend="sec_ha_quick_nfs_resources_nfsexport_nonroot" xrefstyle="select:title"/>-->
    </term>
    <listitem>
     <para>
      One or more NFS exports, typically corresponding to the file system.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <itemizedlist>
    <title>Example NFS Scenario</title>
    <listitem>
        <para>The following configuration examples assume that
         <systemitem class="ipaddress">&nfs-vip-exports;</systemitem> is the virtual
         IP address to use for an NFS server which serves clients in the
         <systemitem class="ipaddress">&subnetII;.x/32</systemitem> subnet.</para>
    </listitem>
    <listitem>
        <para>The service is to host an NFSv4 virtual file system root
          hosted from <literal>/srv/nfs</literal>, with exports data
          served from <literal>/srv/nfs/work</literal>. </para>
    </listitem>
    <listitem>
        <para>Into this export directory, the cluster will mount
            <literal>ext3</literal> file systems from the DRBD device
         <filename>/dev/drbd0</filename>.
         This DRBD device sits on top of a LVM logical volume with the name
         <literal>nfs</literal>.
        </para>
    </listitem>
  </itemizedlist>

  <sect2 xml:id="sec_ha_quick_nfs_resources_drbd">
   <title>DRBD Primitive and Multi-state Resource</title>
   <para>
    To configure these resources, issue the following commands from the
    <command>crm</command> shell:
   </para>
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> drbd_nfs \
  ocf:linbit:drbd \
    params drbd_resource="nfs" \
  op monitor interval="15" role="Master" \
  op monitor interval="30" role="Slave"
&prompt.crm.conf;<command>ms</command> ms-drbd_nfs drbd_nfs \
  meta master-max="1" master-node-max="1" clone-max="2" \
  clone-node-max="1" notify="true"
&prompt.crm.conf;<command>commit</command></screen>
   <para>
    This will create a Pacemaker multi-state resource corresponding to the
    DRBD resource <literal>nfs</literal>. Pacemaker should now activate your
    DRBD resource on both nodes, and promote it to the Master role on one of
    them.
   </para>
   <para>
    Check this with the <command>crm status</command> command, or run
    <command>drbdadm status</command>.
   </para>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_resources_lvm">
   <title>LVM and File System Resources</title>
   <orderedlist>
    <listitem>
     <para>
      Configure LVM and the file system type resources as follows (but
      <emphasis>do not</emphasis> commit this configuration yet):
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> fs_work \
  ocf:heartbeat:Filesystem \
  params device=/dev/drbd/by-disk/nfs/work \
    directory=/srv/nfs/work \
    fstype=ext3 \
  op monitor interval="10s"</screen>
    </listitem>
    <listitem>
     <para>
      Combine these resources into a Pacemaker resource
      <emphasis>group</emphasis>:
     </para>
<screen>&prompt.crm.conf;<command>group</command> g-nfs fs_work</screen>
    </listitem>
    <listitem>
     <para>
      Add the following constraints to make sure that the group is started
      on the same node where the DRBD multi-state resource is in the Master
      role:
     </para>
<screen>&prompt.crm.conf;<command>order</command> o-drbd_before_nfs inf: \
  ms-drbd_nfs:promote g-nfs:start
&prompt.crm.conf;<command>colocation</command> c-nfs_on_drbd inf: \
  g-nfs ms-drbd_nfs:Master</screen>
    </listitem>
    <listitem>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </listitem>
   </orderedlist>
   <para>
    After these changes have been committed, Pacemaker does the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      It activates all logical volumes of the <literal>nfs</literal> LVM
      volume group on the same node where DRBD is in the primary role.
      Confirm this with <command>vgdisplay</command> or
      <command>lvs</command>.
     </para>
    </listitem>
    <listitem>
     <para>
      It mounts the logical volume to
      <filename>/srv/nfs/work</filename> on the same node. Confirm
      this with <command>mount</command> (or by looking at
      <filename>/proc/mounts</filename>).
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_resources_nfsexport">
   <title>NFS Export Resources</title>
   <para>
    When your DRBD, LVM, and file system resources are working properly,
    continue with the resources managing your NFS exports. To create highly
    available NFS export resources, use the <literal>exportfs</literal>
    resource type.
   </para>
   <sect3 xml:id="sec_ha_quick_nfs_resources_nfsexport_nfsv4root">
    <title>NFSv4 Virtual File System Root</title>
    <para>
     If clients exclusively use NFSv3 to connect to the server, you do not
     need this resource. In this case, continue with
     <xref linkend="sec_ha_quick_nfs_resources_nfsexport_nonroot"/>.
    </para>
    <orderedlist>
     <listitem>
      <para>
       This is the root of the virtual NFSv4 file system.
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> exportfs_root \
  ocf:heartbeat:exportfs \
  params directory="/srv/nfs" \
    options="rw,crossmnt" \
    clientspec="&nfs-clientspec;" \<!-- 10.9.9.0/24 -->
  op monitor interval="30s"
&prompt.crm.conf;<command>clone</command> cl-exportfs_root exportfs_root</screen>
      <remark>toms 2016-09-01: What clientspec should be used here?</remark>
      <para>
       This resource does not hold any actual NFS-exported data, merely the
       empty directory (<filename>/srv/nfs</filename>) that the other NFS
       exports are mounted into. Since there is no shared data involved
       here, we can safely <emphasis>clone</emphasis> this resource.
      </para>
     </listitem>
     <listitem>
      <para>
       Since any data should be exported only on nodes where this clone has
       been properly started, add the following constraints to the
       configuration:
      </para>
<screen>&prompt.crm.conf;<command>order</command> o-root_before_nfs Mandatory: \
  cl-exportfs_root g-nfs:start
&prompt.crm.conf;<command>colocation</command> c-nfs_on_root inf: \
  g-nfs cl-exportfs_root
&prompt.crm.conf;<command>commit</command></screen>
      <para>
       After this, Pacemaker should start the NFSv4 virtual file system root
       on both nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       Check the output of the <command>exportfs -v</command> command to
       verify this.
      </para>
     </listitem>
    </orderedlist>
   </sect3>
   <sect3 xml:id="sec_ha_quick_nfs_resources_nfsexport_nonroot">
    <title>Non-root NFS Exports</title>
    <para>
     To export our <filename>/srv/nfs/work</filename> directory to clients,
     use the following primitive:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Create NFS exports with the following commands:
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> exportfs_work \
  ocf:heartbeat:exportfs \
    params fsid=1 \
      directory="/srv/nfs/work" \
      options="rw,mountpoint" \
      clientspec="&nfs-clientspec;" \
      wait_for_leasetime_on_stop=true \
  op monitor interval="30s"</screen>
     </listitem>
     <listitem>
      <para>
       After you have created these resources, append them to the existing
       <literal>g-nfs</literal> resource group:
      </para>
<screen>&prompt.crm.conf;<command>modgroup</command> g-nfs add exportfs_work</screen>
     </listitem>
     <listitem>
      <para>
       Commit this configuration:
      </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
      <para>
       Pacemaker will export the NFS virtual file system root and the two
       other exports.
      </para>
     </listitem>
     <listitem>
      <para>
       Confirm that the NFS exports are set up properly:
      </para>
<screen>&prompt.root;<command>exportfs</command> -v</screen>
     </listitem>
    </orderedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_resources_vip">
   <title>Virtual IP Address for NFS Exports</title>
   <para>
    The initial installation creates an administrative virtual IP address for
    &hawk2;. Although you could use this IP address for your NFS exports too,
    create another one exclusively for NFS exports. This makes it easier to
    apply security restrictions later. Use the following commands in the
    &crmshell;:
   </para>
   <screen>&prompt.crm.conf;<command>primitive</command> vip_nfs IPaddr2 \
   params ip=&nfs-vip-exports; cidr_netmask=24 \
   op monitor interval=10 timeout=20
&prompt.crm.conf;<command>modgroup</command> g-nfs add vip_nfs</screen>
  </sect2>
 </sect1>

 <!-- toms 2015-10-23: This is an attempt to descript how to set up
      a HA NFS cluster with cluster scripts

      toms 2016-08-23: Still disabled for the time being as not really finished
   -->
 <!--<xi:include href="nfs_quick_clusterscript.xml"/>-->

 <sect1 xml:id="sec_ha_quick_nfs_use">
  <title>Using the NFS Service</title>
  <para>
   This section outlines how to use the highly available NFS service from an
   NFS client. It covers NFS clients using NFS versions 3 and 4.
  </para>
  <para>
   To connect to the NFS service, make sure to use the <emphasis>virtual IP
   address</emphasis> to connect to the cluster, rather than a physical IP
   configured on one of the cluster nodes' network interfaces. NFS version 3
   requires that you specify the <emphasis>full</emphasis> path of the NFS
   export on the server.
  </para>

  <para>
   In its simplest form, the command to mount the NFS export with NFSv3
   looks like this:
  </para>

<screen>&prompt.root;<command>mount</command> -o vers=4 &nfs-vip-exports;:/work /home/work</screen>

  <para>
   For selecting a specific transport protocol (<option>proto</option>) and
   maximum read and write request sizes (<option>rsize</option> and
   <option>wsize</option>), use:
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs -o proto=udp,rsize=32768,wsize=32768 \
    &nfs-vip-exports;:/work /home/work</screen>

  <para>
   To connect to a highly available NFS service, NFSv4 clients must use the
   floating cluster IP address <systemitem class="ipaddress"
    >&nfs-vip-exports;</systemitem> (as with NFSv3), rather than any of the
   physical cluster nodes' addresses. NFS version 4 requires that you
   specify the NFS export path <emphasis>relative</emphasis> to the root of
   the virtual file system. Thus, to connect to the
   <literal>work</literal> export, you would use the following
   <command>mount</command> command (note the <literal>nfs4</literal> file
   system type):
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs4 &nfs-vip-exports;:/work /home/work</screen>

  <para>
   For further NFSv3 and NFSv4 mount options, consult the
   <command>nfs</command> man page.
  </para>
 </sect1>
 <xi:include href="common_gfdl1.2_i.xml"/>
</article>
