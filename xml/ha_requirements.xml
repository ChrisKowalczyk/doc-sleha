<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter version="5.0" xml:id="cha.ha.requirements"
  xmlns="http://docbook.org/ns/docbook" 
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:dm="urn:x-suse:ns:docmanager" 
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <?dbfo-need height="10em"?>
  <title>System Requirements and Recommendations</title>
 <info>
      <dm:docmanager>
        <dm:bugtracker>
          <dm:assignee>thomas.schraitle@suse.com</dm:assignee>
        </dm:bugtracker>
        <dm:translation>yes</dm:translation>
      </dm:docmanager>
      <abstract>
        <para>
    The following section informs you about system requirements, and some
    prerequisites for &productnamereg;. It also includes recommendations
    for cluster setup.
   </para>
      </abstract>
    </info>
    <sect1 xml:id="sec.ha.requirements.hw">
  <title>Hardware Requirements</title>

  <para>
   The following list specifies hardware requirements for a cluster based on
   &productnamereg;. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </para>

  <itemizedlist>
   <listitem>
    <formalpara>
     <title>1 to 32 Linux servers with software as specified in
      <xref linkend="sec.ha.requirements.sw"/>.</title>
      <para>The servers do not require identical hardware (memory, disk space,
       etc.), but they must have the same architecture. Cross-platform clusters
       are not supported. Using <literal>pacemaker_remote</literal>, the cluster
       can be extended to include additional Linux servers beyond the 32-node limit.
      </para>
    </formalpara>
   </listitem>
   <listitem>
    &sys-req-hw-comm-channels;
    <para>For details, refer to <xref linkend="cha.ha.netbonding"/> and
     <xref linkend="pro.ha.installation.setup.channel2"/>, respectively.
     <!--bmwiedemann 2015-01-12: are there alternatives to Ethernet? WLAN?
     taroth 2015-01-12: checked this with the developers, the answer was:
     cable connections are more robust-->
    </para>
   </listitem>
   <listitem>
    &sys-req-hw-stonith;
    <important>
     <title>No Support Without &stonith;</title>
     <para>
      A cluster without &stonith; is not supported.
     </para>
    </important>
    <para>
     For more information, refer to <xref linkend="cha.ha.fencing"/>.
     </para>
   </listitem>
  </itemizedlist>
 </sect1>
<?dbfo-need height="10em"?>
 <sect1 xml:id="sec.ha.requirements.sw">
  <title>Software Requirements</title>

  <para>
   On all nodes that will be part of the cluster the following software
   must be installed.
  </para>

<!--taroth 2011-11-02: for the records, we do *not* support mixed clusters
       (SP1/SP2), exception: during rolling upgrade (info by lmb@ha-devel)-->

  <itemizedlist>
   <listitem>
    &sys-req-sw-sles;
   </listitem>
   <listitem>
    &sys-req-sw-sleha;
   </listitem>
   <listitem>
    <para>
     (Optional) For &geo; clusters: &hageo; &productnumber;
     (with all available online updates)
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec.ha.requirements.disk">
  <title>Storage Requirements</title>

  <para>
   To make data highly available, a shared disk system (Storage Area
   Network, or SAN) is recommended for your cluster. If a shared disk
   subsystem is used, ensure the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The shared disk system is properly set up and functional according to
     the manufacturer&rsquo;s instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
     Hardware-based RAID is recommended. Host-based software RAID is not
     supported for all configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </para>
   </listitem>
   <listitem>
    <para>
     When using DRBD* to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the device provided
     by DRBD&mdash;never the backing device. Use the same (bonded) NICs
     that the rest of the cluster uses to leverage the redundancy provided
     there.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec.ha.requirements.other">
  <title>Other Requirements and Recommendations</title>

  <para>
   For a supported and useful &ha; setup, consider the following
   recommendations:
  </para>

  <variablelist>
   <varlistentry xml:id="vle.ha.req.nodes">
    <term>Number of Cluster Nodes</term>
    <listitem>
     <para>
      <remark>taroth 2016-06-26: the following is taken from the Cloud docs - DEVs,
      if this is also true for SLE HA?</remark>
      <remark>krig 2014-08-20: Technically, a cluster can consist of a single node. That seems a
       bit pointless, though. The only reason to have a single-node cluster
       that I can think of would be if the intention is to add more nodes
       later, and you'd want to configure things the "right" way from the
       start. I don't think the recommendation for an odd number of nodes is
       entirely correct since STONITH is required anyway. It's possible to
       run a cluster without quorum as long as STONITH is
       configured. However, this should probably be run by someone with more
       experience than me.</remark>
      Each cluster must consist of at least two cluster nodes.
     </para>
     <important>
      <title>Odd Number of Cluster Nodes</title>
      <para>
       It is strongly recommended to use an <emphasis>odd</emphasis> number
       of cluster nodes with a <emphasis>minimum</emphasis> of three nodes.
      </para>
      <para>
       A cluster needs
       <xref linkend="gloss.quorum" xrefstyle="select:label nopage"/> to
       keep services running. Therefore a three-node cluster can tolerate
       only failure of one node at a time, whereas a five-node cluster can
       tolerate failures of two nodes etc.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.ha.req.stonith">
    <term>&stonith;</term>
    <listitem>
     <important>
      <title>No Support Without &stonith;</title>
      <para>
       A cluster without &stonith; is not supported.
      </para>
     </important>
     <para>
      For a supported &ha; setup, ensure the following:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Each node in the &ha; cluster must have at least one &stonith;
        device (usually a piece of hardware). We strongly recommend multiple
        &stonith; devices per node, unless SBD is used. SBD provides a
        way to enable &stonith; and fencing in clusters without external
        power switches, but it requires shared storage.
       </para>
      </listitem>
      <listitem>
       <para>
        The global cluster options <systemitem>stonith-enabled</systemitem>
        and <systemitem>startup-fencing</systemitem> must be set to
        <literal>true</literal>. As soon as you change them, you will lose
        support.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.ha.req.communication">
    <term>Redundant Communication Paths</term>
    <listitem>
     <para>
      For a supported &ha; setup, it is required to set up cluster
      communication via two or more redundant paths. This can be done via:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <xref linkend="cha.ha.netbonding" xrefstyle="select:title"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        A second communication channel in &corosync;. For details, see
        <xref linkend="pro.ha.installation.setup.channel2"/>.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If possible, choose network device bonding.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Time Synchronization</term>
    &sys-req-other-ntp;
   </varlistentry>
   <varlistentry>
    <term>NIC Names</term>
    <listitem>
     <para>
      Must be identical on all nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Host Name and IP Address</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        Use static IP addresses.
       </para>
      </listitem>
      <listitem>
       &sys-req-other-etc-hosts;
       <para>
        For details on how Pacemaker gets the node names, see also
        <link xlink:href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-node-name.html"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.ha.req.storage">
    <term>Storage Requirements</term>
    <listitem>
<!-- taroth 2014-04-14: https://bugzilla.novell.com/show_bug.cgi?id=873373 -->
     <para>
      Some services may require shared storage. For requirements, see
      <xref linkend="sec.ha.requirements.disk"/>. You can also use an
      external NFS share or DRBD. If using an external NFS share, it must be
      reliably accessible from all cluster nodes via redundant communication
      paths. See <xref linkend="vle.ha.req.communication"/>.
     </para>
     <para>
      When using SBD as &stonith; device, additional requirements apply
      for the shared storage. For details, see
      <link xlink:href="http://linux-ha.org/wiki/SBD_Fencing"/>, section
      <citetitle>Requirements</citetitle>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle.ha.req.ssh">
    <term>SSH</term>
    <listitem>
     &sys-req-other-ssh;
     <note>
      <title>Regulatory Requirements</title>
      <para>
       If passwordless SSH access does not comply with regulatory
       requirements, you can use the work-around described in
       <xref linkend="app.crmreport.nonroot"/> for running 
       <command>crm report</command>.
      </para>
      <para>
       For the <guimenu>History Explorer</guimenu> there is currently no
       alternative for passwordless login.
      </para>
     </note>
    </listitem>
   </varlistentry>

  </variablelist>
 </sect1>
</chapter>