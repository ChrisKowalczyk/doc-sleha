<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ha.requirements">
 <title>System Requirements</title>
 <abstract>
  <para>
   The following section informs you about the system requirements and some
   prerequisites for &productnamereg;, including hardware, software, shared
   disk system, and other requirements.
  </para>
 </abstract>
 <sect1 id="sec.ha.requirements.hw">
  <title>Hardware Requirements</title>

  <para>
   The following list specifies hardware requirements for a cluster based on
   &productnamereg;. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     1 to 32 Linux servers with software as specified in
     <xref linkend="sec.ha.requirements.sw"/>. The servers do not require
     identical hardware (memory, disk space, etc.).
    </para>
   </listitem>
   <listitem>
    <para>
     At least two TCP/IP communication media. Cluster nodes use multicast or
     unicast for communication so the network equipment must support the
     communication means you want to use. The communication media should
     support a data rate of 100 Mbit/s or higher. Preferably, the Ethernet
     channels should be bonded.
    </para>
   </listitem>
   <listitem>
    <para>
     Optional: A shared disk subsystem connected to all servers in the
     cluster from where it needs to be accessed.
    </para>
   </listitem>
   <listitem>
    <para>
     A &stonith; mechanism. &stonith; is an acronym for <quote>Shoot the
     other node in the head</quote>. A &stonith; device is a power switch
     which the cluster uses to reset nodes that are thought to be dead or
     behaving in a strange manner. Resetting non-heartbeating nodes is the
     only reliable way to ensure that no data corruption is performed by
     nodes that hang and only appear to be dead.
    </para>
    <important>
     <title>No Support Without &stonith;</title>
     <para>
      A cluster without &stonith; is not supported.
     </para>
    </important>
    <para>
     For more information, refer to <xref linkend="cha.ha.fencing"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
<?dbfo-need height="10em"?>
 <sect1 id="sec.ha.requirements.sw">
  <title>Software Requirements</title>

  <para>
   Ensure that the following software requirements are met:
  </para>

  <remark>taroth 2011-10-10: DEVs, we still do not support mixed clusters? or do
   we in the meantime because of the new maintenance model?</remark>

  <itemizedlist>
   <listitem>
    <para>
     &slsreg; &productnumber; with all available online updates installed on
     all nodes that will be part of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; &productnumber; including all available online updates
     installed on all nodes that will be part of the cluster.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.requirements.disk">
  <title>Shared Disk System Requirements</title>

  <para>
   A shared disk system (Storage Area Network, or SAN) is recommended for
   your cluster if you want data to be highly available. If a shared disk
   subsystem is used, ensure the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The shared disk system is properly set up and functional according to
     the manufacturer&rsquo;s instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
     Hardware-based RAID is recommended. Host-based software RAID is not
     supported for all configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </para>
   </listitem>
   <listitem>
    <para>
     When using DRBD to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the replicated
     <remark>taroth
     2011-04-28: comment by ulrich windl: replicating device? DEVs, which is
     correct?</remark>
     device. Use the same (bonded) NICs that the rest of the cluster uses to
     leverage the redundancy provided there.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.requirements.other">
  <title>Other Requirements</title>

  <variablelist>
   <varlistentry>
    <term>Time Synchronization</term>
    <listitem>
     <para>
      Cluster nodes should synchronize to an NTP server outside the cluster.
      For more information, see the <citetitle>&sls;</citetitle> &admin;,
      available at
      <ulink
       url="http://www.suse.com/documentation/sles11"/>. Refer
      to the chapter <citetitle>Time Synchronization with NTP</citetitle>.
     </para>
     <para>
      If nodes are not synchronized, log files or cluster reports are very
      hard to analyze.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NIC Names</term>
    <listitem>
     <para>
      Must be identical on all nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Hostname and IP Address</term>
    <listitem>
     <para>
      Configure hostname resolution by editing the
      <filename>/etc/hosts</filename> file on <emphasis>each</emphasis>
      server in the cluster. To ensure that cluster communication is not
      slowed down or tampered with by any DNS:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Use static IP addresses.
       </para>
      </listitem>
      <listitem>
       <para>
        List all cluster nodes in this file with their fully qualified
        hostname and short hostname. It is essential that members of the
        cluster are able to find each other by name. If the names are not
        available, internal cluster communication will fail.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      For more information, see the <citetitle>&sls;</citetitle> &admin;,
      available at <ulink url="http://www.suse.com/documentation"
      />.
      Refer to chapter <citetitle>Basic Networking &gt; Configuring Hostname
      and DNS</citetitle>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>
      All cluster nodes need to be able to access one another with ssh.
      <remark>taroth 2011-10-12: DEVs, is this still needed for hb_report?
       and is it only required for hb_report or for anything else, as well?</remark>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual Environments</term>
    <listitem>
     <para>
      <remark>taroth 2011-10-12: kdupke, do we support all of those?</remark>
      The following virtual environments are supported:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        &kvm;
       </para>
      </listitem>
      <listitem>
       <para>
        &vmware;
       </para>
      </listitem>
      <listitem>
       <para>
        &xen;
       </para>
      </listitem>
      <listitem>
       <para>
        Hyper-V
       </para>
      </listitem>
      <listitem>
       <para>
        LXC
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
