<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.ha.requirements">
 <title>System Requirements</title>
 <para>
  The following section informs you about the system requirements and some
  prerequisites for &productnamereg;, including hardware, software, shared
  disk system, and other requirements.
 </para>
 <sect1 id="sec.ha.requirements.hw">
  <title>Hardware Requirements</title>

  <para>
   The following list specifies hardware requirements for a cluster based on
   &productnamereg;. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     1 to 32 Linux servers with software as specified in
     <xref linkend="sec.ha.requirements.sw"/>. The servers do not require
     identical hardware (memory, disk space, etc.).
    </para>
   </listitem>
   <listitem>
    <para>
     At least two TCP/IP communication media. Cluster nodes use multicast or
     unicast for communication so the network equipment must support the
     communication means you want to use. The communication media should
     support a data rate of 100 Mbit/s or higher. Preferably, the Ethernet
     channels should be bonded.
    </para>
   </listitem>
   <listitem>
    <para>
     Optional: A shared disk subsystem connected to all servers in the
     cluster from where it needs to be accessed.
    </para>
   </listitem>
   <listitem>
    <para>
     A &stonith; mechanism. &stonith; is an acronym for <quote>Shoot the
     other node in the head</quote>. A &stonith; device is a power switch
     which the cluster uses to reset nodes that are thought to be dead or
     behaving in a strange manner. Resetting non-heartbeating nodes is the
     only reliable way to ensure that no data corruption is performed by
     nodes that hang and only appear to be dead.
    </para>
    <important>
     <title>No Support Without &stonith;</title>
     <para>
      A cluster without &stonith; is not supported.
     </para>
    </important>
    <para>
     For more information, refer to <xref linkend="cha.ha.fencing"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
<?dbfo-need height="10em"?>
 <sect1 id="sec.ha.requirements.sw">
  <title>Software Requirements</title>

  <para>
   Ensure that the following software requirements are met:
  </para>

  <remark>taroth 2011-10-10: DEVs, we still do not support mixed clusters? or do
   we in the meantime because of the new maintenance model?</remark>

  <itemizedlist>
   <listitem>
    <para>
     &slsreg; &productnumber; with all available online updates installed on
     all nodes that will be part of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; &productnumber; including all available online updates
     installed on all nodes that will be part of the cluster.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.requirements.disk">
  <title>Shared Disk System Requirements</title>

  <para>
   A shared disk system (Storage Area Network, or SAN) is recommended for
   your cluster if you want data to be highly available. If a shared disk
   subsystem is used, ensure the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The shared disk system is properly set up and functional according to
     the manufacturer&rsquo;s instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
     Hardware-based RAID is recommended. Host-based software RAID is not
     supported for all configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </para>
   </listitem>
   <listitem>
    <para>
     When using DRBD to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the replicated
     <remark>taroth
     2011-04-28: comment by ulrich windl: replicating device? DEVs, which is
     correct?</remark>
     device. Use the same (bonded) NICs that the rest of the cluster uses to
     leverage the redundancy provided there.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.ha.requirements.other">
  <title>Other Requirements</title>

  <variablelist>
   <varlistentry>
    <term>Time Synchronization</term>
    <listitem>
     <para>
      Cluster nodes should synchronize to an NTP server outside the cluster.
      For more information, see the <citetitle>&sls;</citetitle> &admin;,
      available at
      <ulink
       url="http://www.suse.com/documentation/sles11"/>. Refer
      to the chapter <citetitle>Time Synchronization with NTP</citetitle>.
     </para>
     <para>
      If nodes are not synchronized, log files or cluster reports are very
      hard to analyze.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NIC Names</term>
    <listitem>
     <para>
      Must be identical on all nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Hostname and IP Address</term>
    <listitem>
     <para>
      Configure hostname resolution by editing the
      <filename>/etc/hosts</filename> file on <emphasis>each</emphasis>
      server in the cluster. To ensure that cluster communication is not
      slowed down or tampered with by any DNS:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Use static IP addresses.
       </para>
      </listitem>
      <listitem>
       <para>
        List all cluster nodes in this file with their full qualified
        hostname and short hostname. It is essential that members of the
        cluster are able to find each other by name. If the names are not
        available, internal cluster communication will fail.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      For more information, see the <citetitle>&sls;</citetitle> &admin;,
      available at <ulink url="http://www.suse.com/documentation"
      />.
      Refer to chapter <citetitle>Basic Networking &gt; Configuring Hostname
      and DNS</citetitle>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>All cluster nodes need to be able to access one another with
      ssh.<remark>taroth 2011-10-12: DEVs, is this still needed for hb_report?
       and is it only required for hb_report or for anything else, as well?</remark></para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual Environments</term>
    <listitem>
     <para>
      <remark>taroth 2011-10-12: kdupke, do we support all of those?</remark>
      The following virtual environments are supported:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        &kvm;
       </para>
      </listitem>
      <listitem>
       <para>
        &vmware;
       </para>
      </listitem>
      <listitem>
       <para>
        &xen;
       </para>
      </listitem>
      <listitem>
       <para>
        Hyper-V
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
<!--taroth 2011-10-10: stopped here, the following probably needs an update
  because of the new bootstrap scenario-->
 <sect1 id="sec.ha.start.oview">
  <title>Overview: Installing and Setting Up a Cluster</title>

  <para>
   <remark condition="clarity">taroth 2011-09-16: a comment for the
   proofreaders: bwiedemann: are done - sagt man das mit Plural?</remark>
   After the preparations are done, the following basic steps are necessary
   to install and set up a cluster with &productnamereg;:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Installation of &slsreg; and &productnamereg; as add-on on top of
     &sls;. For detailed information, see
     <xref
      linkend="sec.ha.installation.inst"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec.ha.installation.setup" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="sec.ha.installation.start" xrefstyle="select:title"/>
    </para>
   </listitem>
   <listitem>
    <para>
     Configuring global cluster options and adding cluster resources.
    </para>
    <para>
     Both can be done either with a graphical user interface (GUI) or with
     command line tools. For detailed information, see
     <xref
      linkend="cha.ha.configuration.gui"/> or
     <xref
      linkend="cha.ha.manual_config"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     To protect your data from possible corruption by means of fencing and
     &stonith;, make sure to configure &stonith; devices as resources. For
     detailed information, see <xref linkend="cha.ha.fencing"/>.
    </para>
   </listitem>
  </orderedlist>

  <para>
   Depending on your requirements, you may also want to configure the
   following file system and storage-related components for your cluster:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Create file systems on a shared disk (Storage Area Network, SAN). If
     necessary, configure those file systems as cluster resources.
    </para>
   </listitem>
   <listitem>
    <para>
     If you need cluster-aware file systems, use OCFS2.
    </para>
   </listitem>
   <listitem>
    <para>
     To allow the cluster to manage shared storage with Logical Volume
     Manager, use cLVM, which is a set of clustering extensions to LVM.
    </para>
   </listitem>
   <listitem>
    <para>
     To protect your data integrity, implement storage protection by using
     fencing mechanisms and by ensuring exclusive storage access.
    </para>
   </listitem>
   <listitem>
    <para>
     If needed, make use of data replication with DRBD.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   For detailed information, see <xref linkend="part.storage"/>.
  </para>
 </sect1>
</chapter>
