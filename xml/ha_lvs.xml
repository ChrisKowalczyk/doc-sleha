<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!--taroth 2010-02-10: todo -https://fate.novell.com/307409-->
<chapter id="cha.ha.lvs">
 <title>Load Balancing with &lvs;</title>
<!--taroth 090519: see http://www.linuxvirtualserver.org/-->
 <para>
  The goal of &lvs; (LVS) is to provide a basic framework that directs
  network connections to multiple servers that share their workload. &lvs;
  is a cluster of servers (one or more load balancers and several real
  servers for running services) which appears to be one large, fast server
  to an outside client. This apparent single server is called a
  <emphasis>virtual server</emphasis>. The &lvs; as an advanced load
  balancing solution can be used to build highly scalable and highly
  available network services, such as Web, cache, mail, FTP, media and VoIP
  services.
 </para>
 <para>
  The real servers and the load balancers may be interconnected by either
  high-speed LAN or by geographically dispersed WAN. The load balancers can
  dispatch requests to the different servers and make parallel services of
  the cluster appear as a virtual service on a single IP address, and
  request dispatching can use IP load balancing technologies or
  application-level load balancing technologies. Scalability of the system
  is achieved by transparently adding or removing nodes in the cluster. High
  availability is provided by detecting node or daemon failures and
  reconfiguring the system appropriately.
 </para>
 <sect1 id="sec.ha.lvs.overview">
  <title>Conceptual Overview</title>
  <!--Concepts
  LVS hides real servers behind a virtual IP and load balances the
  incoming request across all cluster nodes based on a scheduling
  algorithm. It implements transport-layer load balancing inside the Linux W
  kernel, also called Layer-4 sw itching.
  T
  a
  There are 3 types of LVS load balancing:
  O
  Netw ork Address Translation (NAT)
  Incoming requests arrive at the virtual IP and are forw arded to the
  real servers by changing the destination IP address. The real
  servers send the response to the load balancer w hich in turn
  changes the destination IP address and forw ards the response
  back to the client.
  As all traffic goes through the load balancer, it usually becomes a
  bottleneck for the cluster.
  IP Tunneling
  LVS sends requests to real servers through an IP tunnel
  (redirecting to a different IP address) and the real servers reply
  directly to the client using their ow n routing tables. Cluster
  members can be in different subnets.
  Direct routing
  Packets from end users are forw arded directly to the real server.
  The IP packet is not modified as the real servers are configured to
  accept traffic for the shared cluster virtual IP address by using a
  virtual non-ARP alias interface. The response from the real server
  is send directly to the client. The real servers and load-balancer
  (LVS) have to be in the same physical netw ork segment. (layer 2)-->
  <para>
   LVS consists of two main components:
  </para>

  <variablelist>
   <varlistentry>
    <term>Kernel Code: ip_vs (or IPVS)</term>
    <listitem>
     <para>
      The node that runs a Linux kernel patched to include the IPVS code is
      called <emphasis>director</emphasis>. The IPVS code running on the
      director is the essential feature of LVS.
     </para>
     <para>
      Clients connect to the director that forwards packets to the real
      servers. The director is a layer 4 router with a modified set of
      routing rules (for example, connections do not originate or terminate
      on the director, it does not send acknowledgments) that make the LVS
      work. The director, together with the real servers, are the virtual
      server which appear as one machine to the client(s).
     </para>
     <para>
      There are different forwarding methods that determine how the director
      sends packets from the client to the real servers. Deciding which real
      server to use for a new connection requested by a client is
      implemented using different algorithms, which are available as modules
      and can be adapted to specific needs. Upon receiving a connect request
      from a client, the director assigns a real server to the client based
      on a <emphasis>schedule</emphasis>. The scheduler is the part of the
      IPVS kernel code which decides which real server will get the next new
      connection.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>User Space Controller: ipvsadm</term>
    <listitem>
     <para>
      <remark>taroth 2010-03-10: todo - find out if the text about ipvsadm should be replaced with
       an introduction to ldirectord </remark> Provided by the <systemitem class="resource"
       >ipvsadm</systemitem> package, ipvsadm is a user interface and lets you manage the &lvs;.
      For example, you can set rules for services handled, handle failover, or set the scheduler
      type. </para>
     <para> You use ipvsadm from the command line (or in rc files) to set up: </para>
     <itemizedlist>
      <listitem>
       <para>
        Services/servers that the director directs (e.g. http goes to all
        real servers, while ftp goes only to one of the real servers)
       </para>
      </listitem>
      <listitem>
       <para>
        Weighting given to each real server (useful if some servers are
        faster than others)
       </para>
      </listitem>
      <listitem>
       <para>
        Scheduling algorithms
       </para>
      </listitem>
     </itemizedlist>
     <para>
      You use can also use ipvsadm for the following tasks:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Adding services
       </para>
      </listitem>
      <listitem>
       <para>
        Shutting down services
       </para>
      </listitem>
      <listitem>
       <para>
        Deleting services
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>By default, the kernel does not have the IPVS module installed. The IPVS kernel module is
   included in the <systemitem class="resource">cluster-network-kmp-default</systemitem> package.
  </para>
 </sect1>
 <sect1 id="sec.ha.lvs.yast">
  <title>Configuring IP Load Balancing with &yast;</title>

  <para>You can configure kernel-based IP load balancing with the &yast; iplb module. It is a
   front-end for <systemitem class="daemon">ldirectord</systemitem>, a user-space daemon for
   managing &lvs; and monitoring the real servers in an LVS cluster of load balanced virtual
   servers.</para>
  <para>To access the IP Load Balancing dialog, start &yast; as &rootuser; and select <menuchoice>
    <guimenu>&ha;</guimenu>
    <guimenu>IP Load Balancing</guimenu>
   </menuchoice>. Alternatively, start the &yast; cluster module as &rootuser; on a command
   line with <command>yast2&nbsp;iplb</command>. The &yast; module writes its configuration
   to <filename>/etc/ha.d/ldirectord.cf</filename>. This configuration file specifies the virtual
   services and their associated real servers and tells <systemitem class="daemon"
    >ldirecord</systemitem> how to configure the server as a LVS redirector. When the daemon is
   initialized, it creates the virtual services for the cluster. By periodically requesting a known
   URL and checking the responses, the <systemitem class="daemon">ldirectord</systemitem> daemon
   monitors the health of the real servers. If a real server fails, it will be removed from the
   available server list at the load balancer. When the service monitor detects that the dead server
   has recovered and is working again, it will add the server back to the available server list. For
   the case that all real servers should be down, a fall-back server can be specified to which to
   redirect a Web service. Typically the fall-back server is localhost, presenting an emergency page
   about the Web service being temporarily unavailable.</para>
  <para>The tabs available in the &yast; module correspond to the sections of the
    <filename>/etc/ha.d/ldirectord.cf</filename> configuration file, one defining global options and
   the other one defining the options for the virtual servers. <remark>taroth 2010-03-16: DEVS, is
    the following correct?</remark>If a certain parameter is both specified in the virtual server
   section and in the global section, the value defined in the virtual server section overrides the
   value defined in the global section.</para>

  <procedure>
   <title>Configuring Global Parameters</title>
   <para>The following procedure describes how to configure the most important parameters in the
    global section. For more details about the individual parameters, either click
     <guimenu>Help</guimenu> or refer to the <systemitem class="daemon">ldirectord</systemitem> man page.</para>

   <step>
    <para>In <guimenu>Check Interval</guimenu>, define the interval in which <systemitem
      class="daemon">ldirectord</systemitem> will connect to each of the real servers to check if
     they are still online.</para>
   </step>
   <step>
    <para>With <guimenu>Check Timeout</guimenu> set the time in which the server should have
     responded after the last check.</para>
   </step>
   <step>
    <para>With <guimenu>Check Count </guimenu> you can define how many times <systemitem
      class="daemon">ldirectord</systemitem> will attempt to request the real servers until the
     check is considered as failed.</para>
   </step>
   <step>
    <para>With <guimenu>Negotiate Timeout</guimenu> define a timeout in seconds for negotiate
     checks. </para>
   </step>
   <step>
    <para>In <guimenu>Fallback</guimenu>, enter the hostname or IP address of the Web server onto
     which to redirect a Web service in case all real servers are down.</para>
   </step>
   <step>
    <para>If you want to use an alternative path for logging, specify a path for the logs with
      <guimenu>Log File</guimenu>. By default, <systemitem class="daemon">ldirectord</systemitem> writes its logs to
      <filename>/var/log/ldirectord.log</filename>.</para>
   </step>
   <step>
    <para>If you want the system to send alerts in case the connection status to any real server
     changes, enter a valid e-mail address in <guimenu>Email Alert</guimenu>.</para>
   </step>
   <step>
    <para>With <guimenu>Email Alert Frequency</guimenu>, define after how many seconds the e-mail
     alert should be repeated if any of the real servers remains inaccessible.</para>
   </step>
   <step>
    <para>In <guimenu>Email Alert Status</guimenu> specify the server states for which email alerts
     should be sent. If you want to define more than one state, use a comma-separated list. </para>
   </step>
   <step>
    <para>With the <guimenu>Quiescent</guimenu> switch, define if to remove failed real servers from
     the kernel's LVS table or not. If set to <guimenu>Yes</guimenu>, failed servers are not removed
     but instead their weight is set to <literal>0</literal> which means that no new connections
     will be accepted. Already established connections will persist until they time out.</para>
   </step>
  </procedure>
  
  <procedure>
   <title>Configuring Virtual Services</title>
   <para>You can configure one or more virtual services by defining a couple of parameters for each.
    The following procedure describes how to configure the most important parameters for a virtual
    service. For more details about the individual parameters, either click <guimenu>Help</guimenu>
    or refer to the <systemitem class="daemon">ldirectord</systemitem> man page.</para>
   <step>
    <para> In the &yast; iplb module, switch to the <guimenu>Virtual Server
      Configuration</guimenu> tab. </para>
   </step>
   <step>
    <para>
     <guimenu>Add</guimenu> a new virtual server or select and <guimenu>Edit</guimenu> an existing
     virtual server.</para>
   </step>
   <step>
    <para>
     <remark>taroth 2010-03-16: DEVs, is the following correct?</remark> In <guimenu>Virtual
      Server</guimenu> enter the shared virtual IP address (or hostname) and port (or service name)
     under which the director(s) and the real servers are accessible as LVS. Instead of IP
     address/hostname and port/service name, you can also use a firewall mark. A firewall mark is a
     way of aggregating an arbitrary collection of <literal>VIP:port</literal> services into one
     virtual service.</para>
   </step>
   <step>
    <para>To specify the <guimenu>Real Servers</guimenu>, you need to enter the IP address (or
     hostnames) of the servers, the ports (or service names) and the forwarding method. The
     forwarding method must be <literal>gate</literal>, <literal>ipip</literal> or
      <literal>masq</literal>. Click the <guimenu>Add</guimenu> button and enter the required
     arguments for each real server.</para>
   </step>
   <step>
    <para>As <guimenu>Check Type</guimenu>, select the type of check to perform for testing if the
     real servers are still alive. For example, to send a request and check if the response contains
     an expected string, select <literal>Negotiate</literal>.</para>
   </step>
   <step>
    <para>If you have set the <guimenu>Check Type</guimenu> to <literal>Negotiate</literal>, you
     also need to define the type of <guimenu>Service</guimenu> to monitor.</para>
   </step>
   <step>
    <para>In <guimenu>Request</guimenu>, enter the URI to the object that is requested on each real
     server during the check intervals.</para>
   </step>
   <step>
    <para>If you want to check if the response from the real servers contains a certain string
      (<quote>I'm alive</quote> message), define a regular expression that needs to be matched.
     Enter the regular expression into <guimenu>Receive</guimenu>. If the response from a real
     server contains this expression, the real server is considered to be alive.</para>
   </step>
   <step>
    <para>Depending on the type of <guimenu>Service</guimenu> you have selected in step FIXME, you
     also need to specify further parameters like <guimenu>Login</guimenu>,
      <guimenu>Password</guimenu>, <guimenu>Database</guimenu>, or <guimenu>Secret</guimenu>. For
     more information, refer to the &yast; help text or to the <systemitem class="daemon">ldirectord</systemitem> man page. </para>
   </step>
   <step>
    <para>Select the <guimenu>Scheduler</guimenu> to be used for load balancing. For information on
     the available schedulers, refer to the <command>ipvsadm(8)</command> man page. </para>
   </step>
   <step>
    <para>Select the <guimenu>Protocol</guimenu> to be used. <remark>taroth 2010-03-16: DEVs, to be
      used for what? for forwarding the requests arriving at the VIP to the real servers? or for
      which purpose?</remark>. If the virtual service is specified as an IP address and port, it
     must be either <literal>tcp</literal> or <literal>udp</literal>. If the virtual service is
     specified as a firewall mark, the protocol must be <literal>fwm</literal>. </para>
   </step>
   <step>
    <para>Define further parameters, if needed. Confirm your configuration with
      <guimenu>OK</guimenu>. &yast; writes the configuration to
      <filename>/etc/ha.d/ldirectord.cf</filename>.</para></step>
  </procedure> 
  <!--

For HA purpose, the user need to add ldirecord into pacemaker as a
primitive resource too. So that ldirectord can fail-over to other servers
on hardware failure.
We also suggest to use csync2 to sync /etc/ha.d/ldirectord.cf across
nodes in the cluster.
-->
 
  
 </sect1>
 
 <sect1>
  <title>&ha;</title>

  <para>
   To construct a highly available &lvs; cluster, you can use several
   built-in features of the software. In general, there are service monitor
   daemons running on the load balancer to check server health periodically.
   <remark>taroth 090623: DEVs, which daemons? should we mention them here?</remark>
   If there is no response for a service access request or ICMP ECHO_REQUEST
   from a server within a specified time, the service monitor will consider
   the server dead and remove it from the available server list at the load
   balancer. Thus, no new requests will be sent to this dead server. When
   the service monitor detects that the dead server has recovered and is
   working again, it will add the server back to the available server list.
   Therefore, the load balancer can automatically mask the failure of
   service daemons or servers.
  </para>

  <para>
   Furthermore, administrators can also use system tools to add new servers
   to increase the system throughput or remove servers for system
   maintenance, without bringing down the whole system service.
  </para>

  <para>
   To prevent the load balancer from becoming a single point of failure for
   the whole system, you need to set up a backup (or several backups) of the
   load balancer.
   <remark>taroth 090623:
    DEVs, the information this is based on seems to be quite old - are heartbeat daemons still
    correct? or do we need to mention other daemons here (&ais;??) is the rest of the following para
    still true?</remark>
   Two heartbeat daemons run on the primary and the backup, respectively.
   They periodically heartbeat the <quote>I'm alive</quote> message to each
   other through serial lines and/or network interfaces. When the heartbeat
   daemon of the backup cannot hear the heartbeat message from the primary
   within the specified time, it will take over the virtual IP address in
   order to provide the load-balancing service.
  </para>

  <para>
   <remark>taroth 090623: DEVs, is this para still correct?</remark>
   When the failed load balancer recovers again, there are two possible
   results: it can become the backup load balancer automatically, or the
   active load balancer releases the VIP address so that the recovered one
   takes over the VIP address and becomes the primary load balancer again.
   The primary load balancer has the state of connections, meaning it knows
   which server the connection is forwarded to. If the backup load balancer
   takes over without that connection information, the clients have to send
   their requests again in order to access service. In order to make load
   balancer failover transparent to client applications, there is a
   connection synchronization in IPVS: the primary IPVS load balancer
   synchronizes connection information to the backup load balancers through
   UDP multicast. When the backup load balancer takes over after the primary
   one fails, the backup load balancer will have the state of most
   connections, so that almost all connections can continue to access the
   service through the backup load balancer.
  </para>
 </sect1>
 <sect1>
  <title>For More Information</title>

  <para>
   To learn more about &lvs;, refer to the project home page available at
   <ulink
    url="http://www.linuxvirtualserver.org/"/>.
  </para>
 </sect1>
<!--taroth 090519: see also 
  http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/index.html-->
<!--    
  * IPVS,ipvs,ip_vs the code that patches the linux kernel on the director.
  * LVS, linux virtual server This is the director + realservers. Together these machines are the virtual server, 
     which appears as one machine to the client(s).
  * director: the node that runs the ipvs code. 
  * realservers: the hosts that have the services. The realservers handle the requests from the clients.
  * client the host or user level process that connects to the VIP on the director
  * forwarding method (currently LVS-NAT, LVS-DR, LVS-Tun). The director is a router with somewhat different rules 
     for forwarding packets than a normal router. The forwarding method determines how the director sends packets 
     from the client to the realservers.
  * scheduling (ipvsadm and schedulers) - the algorithm the director uses to select a realserver 
  to service a new connection request from a client. -->
<!--taroth 090609: from http://www.linuxvirtualserver.org/-->
<!--High Availability As more and more mission-critical applications
  move on the Internet, providing highly available services becomes
  increasingly important. One of the advantages of a clustered system is
  that it has hardware and software redundancy, because the cluster
  system consists of a number of independent nodes, and each node runs a
  copy of operating system and application software. High availability
  can be achieved by detecting node or daemon failures and reconfiguring
  the system appropriately, so that the workload can be taken over by
  the remaining nodes in the cluster. In fact, high availability is a
  big field. An advanced highly available system may have a reliable
  group communication sub-system, membership management, quoram
  sub-systems, concurrent control sub-system and so on. There must be a
  lot of work to do. However, we can use some existing software packages
  to construct highly available LVS cluster systems now..-->
</chapter>
