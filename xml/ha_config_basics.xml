<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
 <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
 <!ENTITY % entities SYSTEM "entity-decl.ent">
 %entities;
]>
<!--taroth -2010-02-01: todo - https://fate.novell.com/307371
                                                     https://fate.novell.com/303384 -->
<chapter id="cha.ha.configuration.basics">
 <title>Configuration and Administration Basics</title>
 <para>
<!--taroth 2010-01-27: todo - consolidate the basics into one
  chapter as propsed by lmb and then have (sub-)chapters explaining how 
  to apply them via the GUI/CLI -->
 </para>
 <!--taroth 201002-10: also mention setting of global quorum and fencing options
  (crm tool/GUI): rgoldwyn: That is a part of cluster CIB configuration
  using the hb_gui-->
 <abstract>
  <para>
   The main purpose of an HA cluster is to manage user services. Typical
   examples of user services are an Apache web server or a database. From
   the user's point of view, the services do something specific when ordered
   to do so. To the cluster, however, they are just resources which may be
   started or stopped&mdash;the nature of the service is irrelevant to the
   cluster.
  </para>
  
  <para>
   As a cluster administrator, you need to create cluster resources for
   every resource or application you run on servers in your cluster. Cluster
   resources can include Web sites, e-mail servers, databases, file systems,
   virtual machines, and any other server-based applications or services you
   want to make available to users at all times.
  </para>
  
  <para>In this chapter, we will introduce some basic concepts you need to
   know when configuring resources and administering your cluster. The
   following chapters show you how to execute the main configuration and
   administration tasks with each of the management tools the &hasi;
   provides.</para>
 </abstract>
 
 <remark>taroth 2010-02-19: todo - for each section/topic add xrefs to the
 following chapters (GUI, HAWK, CLI)</remark>
 
 <sect1>
  <title>Cluster Resources</title>
  
  <para>
   You can create the following types of resources:
  </para>
  
  <variablelist>
   <varlistentry>
    <term>Primitive</term>
    <listitem>
     <para>
      A primitive resource, the most basic type of a resource.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Group</term>
    <listitem>
     <para>
      Groups contain a set of resources that need to be located together,
      start sequentially and stop in the reverse order. For more
      information, refer to
      <xref
       linkend="sec.ha.configuration.group"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Clone</term>
    <listitem>
     <para>
      Clones are resources that can be active on multiple hosts. Any
      resource can be cloned, provided the respective resource agent
      supports it. For more information, refer to
      <xref
       linkend="sec.ha.configuration.clone"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Master</term>
    <listitem>
     <para>
      Masters are a special type of a clone resources, masters can have
      multiple modes. Masters must contain exactly one group or one regular
      resource.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  
  <para>
   You can add or modify the following parameters for primitive resources at
   any time:
  </para>
  
  <variablelist>
   <varlistentry>
    <term>Meta Attributes</term>
    <listitem>
     <para>
      Meta attributes are options you can add to a resource. They tell the
      CRM how to treat a specific resource. For an overview of the available
      meta attributes, their values and defaults, refer to
      <xref linkend="sec.ha.resources.options"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Instance Attributes</term>
    <listitem>
     <para>
      Instance attributes are parameters for certain resource classes that
      determine how they behave and which instance of a service they
      control. For more information, refer to
      <xref
       linkend="sec.ha.resources.instattributs"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Operations</term>
    <listitem>
     <para>
      The monitor operations added for a resource. These instruct the
      cluster to make sure that the resource is still healthy. Monitor
      operations can be added for all classes of resource agents. You can
      also set particular parameters, such as <literal>Timeout</literal> for
      <literal>start</literal> or <literal>stop</literal> operations. For
      more information, refer to
      <xref linkend="sec.ha.configuration.monitor"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  
  <para>FIXME: STONITH resources</para>
  
  
  <para>
   Some cluster resources are dependent on other components or resources,
   and require that each component or resource starts in a specific order
   and runs together on the same server. To simplify this configuration we
   support the concept of groups.
  </para>
  
  <para>
   Groups have the following properties:
  </para>
  
  <variablelist>
   <varlistentry>
    <term>Starting and Stopping Resources</term>
    <listitem>
     <para>
      Resources are started in the order they appear in and stopped in the
      reverse order which they appear in.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Dependency</term>
    <listitem>
     <para>
      If a resource in the group cannot run anywhere, then none of the
      resources located after that resource in the group is allowed to run.
     </para>
    </listitem>
   </varlistentry>
   <!--<varlistentry>
    <term>Instance Attributes</term>
    <listitem>
    <para>
    <remark>taroth 090401: DEVs, do I get it right that
    groups do not have any instance attributes *by default*, but that
    instance attributes *can* be set for groups anyway and are then
    inherited by the children? however, with the GUI, adding instance
    attributes for a group does not seem to be possible.... - ygao: Cluster resources can only be removed when
    they are not referenced by a constraints</remark>
    Groups do not have instance attributes, however any that are set
    here will be inherited by the group’s children. </para>
    </listitem>
    </varlistentry>-->
   <varlistentry>
    <term>Group Contents</term>
    <listitem>
     <para>
      Groups may only contain a collection of primitive cluster resources.
      To refer to the child of a group resource, use the child’s ID
      instead of the group’s.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Constraints</term>
    <listitem>
     <para>
      Although it is possible to reference the group’s children in
      constraints, it is usually preferable to use the group’s name
      instead.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Stickiness</term>
    <listitem>
     <para>
      Stickiness is additive in groups. Every <emphasis>active</emphasis>
      member of the group will contribute its stickiness value to the
      group’s total. So if the default
      <literal>resource-stickiness</literal> is <literal>100</literal> and a
      group has seven members (ﬁve of which are active), then the group as
      a whole will prefer its current location with a score of
      <literal>500</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Resource Monitoring</term>
    <listitem>
     <para>
      To enable resource monitoring for a group, you must configure
      monitoring separately for each resource in the group that you want
      monitored.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  
  <note>
   <title>Empty Groups</title>
   <para>
    Groups must contain at least one resource, otherwise the configuration
    is not valid.
   </para>
  </note>
 </sect1>
 
 <sect1>
  <title>Constraints</title>
  
  <para>
   Having all the resources configured is only part of the job. Even if the
   cluster knows all needed resources, it might still not be able to handle
   them correctly. Resource constraints let you specify which cluster nodes
   resources can run on, what order resources will load, and what other
   resources a specific resource is dependent on.
  </para>
  
  <para>
   There are three different kinds of constraints available:
  </para>
  
  <variablelist>
   <varlistentry>
    <term><guimenu>Resource Location</guimenu>
    </term>
    <listitem>
     <para>
      Locational constraints that define on which nodes a resource may be
      run, may not be run or is preferred to be run.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Resource Collocation</guimenu>
    </term>
    <listitem>
     <para>
      Collocational constraints that tell the cluster which resources may or
      may not run together on a node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><guimenu>Resource Order</guimenu>
    </term>
    <listitem>
     <para>
      Ordering constraints to define the sequence of actions.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  
  <para>
   When defining constraints, you also need to deal with scores. Scores of
   all kinds are integral to how the cluster works. Practically everything
   from migrating a resource to deciding which resource to stop in a
   degraded cluster is achieved by manipulating scores in some way. Scores
   are calculated on a per-resource basis and any node with a negative score
   for a resource cannot run that resource. After calculating the scores for
   a resource, the cluster then chooses the node with the highest score.
   <literal>INFINITY</literal> is currently deﬁned as
   <literal>1,000,000</literal>. Additions or subtractions with it follows
   the following 3 basic rules:
  </para>
  
  <itemizedlist>
   <listitem>
    <para>
     Any value + INFINITY = INFINITY
    </para>
   </listitem>
   <listitem>
    <para>
     Any value - INFINITY = -INFINITY
    </para>
   </listitem>
   <listitem>
    <para>
     INFINITY - INFINITY = -INFINITY
    </para>
   </listitem>
  </itemizedlist>
  
  <para>
   When defining resource constraints, you also specify a score for each
   constraint. The score indicates the value you are assigning to this
   resource constraint. Constraints with higher scores are applied before
   those with lower scores. By creating additional location constraints with
   different scores for a given resource, you can specify an order for the
   nodes that a resource will fail over to.
  </para>
  <para></para>
 </sect1>
 
 <sect1>
  <title>Failover Nodes</title>
  <para>
   A resource will be automatically restarted if it fails. If that cannot be
   achieved on the current node, or it fails N times on the current node, it
   will try to fail over to another node. You can define a number of
   failures for resources (a <literal>migration-threshold</literal>), after
   which they will migrate to a new node. If you have more than two nodes in
   your cluster, the node a particular resource fails over to is chosen by
   the &ha; software.
  </para>
  
  <para>
   If you want to choose which node a resource will fail over to, you must
   do the following:
  </para>
  
  <procedure>
   <step>
    <para>
     Configure a location constraint for that resource as described in
     <xref
      linkend="pro.ha.configuration.constraints.location"/>.
    </para>
   </step>
   <step>
    <para>
     Add the <literal>migration-threshold</literal> meta attribute to that
     resource as described in
     <xref linkend="pro.ha.configuration.parameters"/> and enter a
     <guimenu>Value</guimenu> for the migration-threshold. The value should
     be positive and less that INFINITY.
    </para>
      </step>
   <step>
    <para>
     If you want to automatically expire the failcount for a resource, add
     the <literal>failure-timeout</literal> meta attribute to that resource
     as described in <xref
      linkend="pro.ha.configuration.parameters"/>
     and enter a <guimenu>Value</guimenu> for the failure-timeout.
    </para>
   </step>
   <step>
    <para>
     If you want to specify additional failover nodes with preferences for a
     resource, create additional location constraints.
    </para>
   </step>
  </procedure>
  
  <para>
   For example, let us assume you have configured a location constraint for
   resource <literal>r1</literal> to preferably run on
   <literal>node1</literal>. If it fails there,
   <literal>migration-threshold</literal> is checked and compared to the
   failcount. If failcount >= migration-threshold then the resource is
   migrated to the node with the next best preference.
  </para>
  
  <para>
   By default, once the threshold has been reached, the node will no longer
   be allowed to run the failed resource until the administrator manually
   resets the resource’s failcount (after fixing the failure cause).
  </para>
  
  <para>
   However, it is possible to expire the failcounts by setting the
   resource’s failure-timeout option. So a setting of
   <literal>migration-threshold=2</literal> and
   <literal>failure-timeout=60s</literal> would cause the resource to
   migrate to a new node after two failures and potentially allow it to move
   back (depending on the stickiness and constraint scores) after one
   minute.
  </para>
  
  <para>
   There are two exceptions to the migration threshold concept, occurring
   when a resource either fails to start or fails to stop: Start failures
   set the failcount to INFINITY and thus always cause an immediate
   migration. Stop failures cause fencing (when
   <literal>stonith-enabled</literal> is set to <literal>true</literal>
   <remark>taroth 090402: todo: add when explaining the CRM config view - ygao: for GUI, it could be
    set in the "CRM Config" view </remark>
   which is the default). In case there is no STONITH resource defined (or
   <literal>stonith-enabled</literal> is set to <literal>false</literal>),
   the resource will not migrate at all.
  </para>
  
  <para>
   To clean up the failcount for a resource with the &hbgui;, select
   <guimenu>Management</guimenu> in the left pane, select the respective
   resource in the right pane and click <guimenu>Cleanup Resource</guimenu>
   in the toolbar. This executes the commands <command>crm_resource
    -C</command> and <command>crm_failcount -D</command> for the specified
   resource on the specified node. For more information, see also
   <xref linkend="man.crmresource"/> and <xref linkend="man.crmfailcount"/>.
  </para>
  </sect1>
 
 <sect1>
  <title>Failback Nodes</title>
  
  <para>
   A resource might fail back to its original node when that node is back
   online and in the cluster. If you want to prevent a resource from failing
   back to the node it was running on prior to failover, or if you want to
   specify a different node for the resource to fail back to, you must
   change its resource stickiness value. You can either specify resource
   stickiness when you are creating a resource, or afterwards.
  </para>
  
  <para>
   Consider the following when specifying a resource stickiness value:
  </para>
  
  <variablelist>
   <varlistentry>
    <term>Value is <literal>0</literal>:</term>
    <listitem>
     <para>
      <remark>taroth 090402: todo - explain better for next review - ygao: It defaults to the
       value of "default-resource-stickiness" which could be set in "CRM Config". And yes, that
       property defaults to be 0. </remark>
      This is the default. The resource will be placed optimally in the
      system. This may mean that it is moved when a <quote>better</quote> or
      less loaded node becomes available. This option is almost equivalent
      to automatic failback, except that the resource may be moved to a node
      that is not the one it was previously active on.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Value is greater than <literal>0</literal>:</term>
    <listitem>
     <para>
      The resource will prefer to remain in its current location, but may be
      moved if a more suitable node is available. Higher values indicate a
      stronger preference for a resource to stay where it is.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Value is less than <literal>0</literal>:</term>
    <listitem>
     <para>
      The resource prefers to move away from its current location. Higher
      absolute values indicate a stronger preference for a resource to be
      moved.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Value is <literal>INFINITY</literal>:</term>
    <listitem>
     <para>
      The resource will always remain in its current location unless forced
      off because the node is no longer eligible to run the resource (node
      shutdown, node standby, reaching the
      <literal>migration-threshold</literal>, or configuration change). This
      option is almost equivalent to completely disabling automatic failback
      <!--ygao 090402: Really cannot understand this either technically or logically:)
       How about deleting it?:, except that the resource may be moved to other nodes
       than the one it was previously active on-->
      .
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Value is <literal>-INFINITY</literal>:</term>
    <listitem>
     <para>
      The resource will always move away from its current location.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 
 <sect1>
   <title>Clone Resources</title>
  <para></para>
  
  <para>
   You may want certain resources to run simultaneously on multiple nodes in
   your cluster. To do this you must configure a resource as a clone.
   Examples of resources that might be configured as clones include
   &stonith; and cluster file systems like OCFS2. You can clone any resource
   provided it is supported by the resource’s Resource Agent. Clone
   resources may even be configured differently depending on which nodes
   they are hosted.
  </para>
  
  <para>
   There are three types of resource clones:
  </para>
  
  <variablelist>
   <varlistentry>
    <term>Anonymous Clones</term>
    <listitem>
     <para>
      These are the simplest type of clones. They behave identically
      anywhere they are running. Because of this, there can only be one
      instance of an anonymous clone active per machine.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Globally Unique Clones</term>
    <listitem>
     <para>
      These resources are distinct entities. An instance of the clone
      running on one node is not equivalent to another instance on another
      node; nor would any two instances on the same node be equivalent.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Stateful Clones</term>
    <listitem>
     <para>
      Active instances of these resources are divided into two states,
      active and passive. These are also sometimes referred to as primary
      and secondary, or master and slave. Stateful clones can be either
      anonymous or globally unique.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 
 <sect1>
  <title>Management: Starting, Removing, Cleaning Up, Monitoring, and
   Migrating Resources</title>
  <para/>

  <sect2>
   <title>Starting</title>
   <para>FIXME</para>
  </sect2>

  <sect2>
   <title>Removing</title>
   <para>FIXME</para>
  </sect2>
  
  <sect2>
   <title>Cleaning Up</title>
   <para>FIXME</para>
  </sect2>
  
  
  <sect2>
   <title>Monitoring</title>
   <para> Although the &hasi; can detect a node failure, it also has
    the ability to detect when an individual resource on a node has
    failed. If you want to ensure that a resource is running, you must
    configure resource monitoring for it. Resource monitoring consists of
    specifying a timeout and/or start delay value, and an interval. The
    interval tells the CRM how often it should check the resource status. </para>
   <para> If the resource monitor detects a failure, the following takes
    place: </para>

   <itemizedlist>
    <listitem>
     <para> Log file messages are generated, according to the
      configuration specified in the <literal>logging</literal> section of
       <filename>/etc/corosync/corosync.conf</filename> (by default,
      written to syslog, usually <filename>/var/log/messages</filename>).
     </para>
    </listitem>
    <listitem>
     <para> The failure is reflected in the &hbgui;, the
      <command>crm_mon</command> tools, and in the CIB status section. To
      view them in the &hbgui;, click <guimenu>Management</guimenu> in
      the left pane, then in the right pane, select the resource whose
      details you want to see. </para>
    </listitem>
    <listitem>
     <para> The cluster initiates noticeable recovery actions which may
      include stopping the resource to repair the failed state and
      restarting the resource locally or on another node. The resource
      also may not be restarted at all, depending on the configuration and
      state of the cluster. </para>
    </listitem>
   </itemizedlist>
  </sect2>
 
 
  <sect2>
   <title>Migrating</title>
   <para>FIXME</para>
  </sect2>
 </sect1>
 
 
</chapter>
